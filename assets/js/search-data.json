{
  
    
        "post0": {
            "title": "Global Vaccinations",
            "content": "Today we will make the faceted geospatial charts from the article Tracking Coronavirus Vaccinations Around the World and find out about a bug relating to faceting geospatial charts and how to go about achieving the plot till a proper fix is made. . . #hide_output import pandas as pd import altair as alt alt.renderers.set_embed_options(actions=False) . owd_vaccine_uri = &quot;https://github.com/owid/covid-19-data/blob/master/public/data/vaccinations/locations.csv&quot; vaccine_uri_raw_data = &quot;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/locations.csv&quot; . raw_data = pd.read_csv(vaccine_uri_raw_data) raw_data.tail() . location iso_code vaccines last_observation_date source_name source_website . 97 Turks and Caicos Islands | TCA | Pfizer/BioNTech | 2021-02-08 | Ministry of Health | https://www.facebook.com/tcihealthpromotions/p... | . 98 United Arab Emirates | ARE | Oxford/AstraZeneca, Pfizer/BioNTech, Sinopharm... | 2021-02-22 | National Emergency Crisis and Disaster Managem... | http://covid19.ncema.gov.ae/en | . 99 United Kingdom | GBR | Oxford/AstraZeneca, Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 100 United States | USA | Moderna, Pfizer/BioNTech | 2021-02-22 | Centers for Disease Control and Prevention | https://covid.cdc.gov/covid-data-tracker/#vacc... | . 101 Wales | NaN | Oxford/AstraZeneca, Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . Let&#39;s study what unique vaccines are there - . from itertools import chain set(chain(*raw_data.vaccines.apply(lambda x: map(str.lstrip, x.split(&#39;,&#39;))).reset_index(drop=True))) . {&#39;Covaxin&#39;, &#39;Johnson&amp;Johnson&#39;, &#39;Moderna&#39;, &#39;Oxford/AstraZeneca&#39;, &#39;Pfizer/BioNTech&#39;, &#39;Sinopharm/Beijing&#39;, &#39;Sinopharm/Wuhan&#39;, &#39;Sinovac&#39;, &#39;Sputnik V&#39;} . Now we will split vaccines into a list and explode it so that the dataframe is in the correct format for our visualization purposes - . vaccine_location_data = raw_data.copy() vaccine_location_data[&#39;vaccines&#39;] = vaccine_location_data.vaccines.apply(lambda x: list(map(str.lstrip, x.split(&#39;,&#39;)))) vaccine_location_data = vaccine_location_data.explode(&#39;vaccines&#39;).reset_index(drop=True) vaccine_location_data.tail() . location iso_code vaccines last_observation_date source_name source_website . 173 United Kingdom | GBR | Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 174 United States | USA | Moderna | 2021-02-22 | Centers for Disease Control and Prevention | https://covid.cdc.gov/covid-data-tracker/#vacc... | . 175 United States | USA | Pfizer/BioNTech | 2021-02-22 | Centers for Disease Control and Prevention | https://covid.cdc.gov/covid-data-tracker/#vacc... | . 176 Wales | NaN | Oxford/AstraZeneca | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 177 Wales | NaN | Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . To check we can get the list of unique vaccines and crosscheck it with what we had earlier - . vaccine_location_data.vaccines.unique() . array([&#39;Pfizer/BioNTech&#39;, &#39;Sputnik V&#39;, &#39;Oxford/AstraZeneca&#39;, &#39;Moderna&#39;, &#39;Sinopharm/Beijing&#39;, &#39;Sinovac&#39;, &#39;Sinopharm/Wuhan&#39;, &#39;Covaxin&#39;, &#39;Johnson&amp;Johnson&#39;], dtype=object) . We will also have to deal with some missing values in iso_code so that we can merge it with the geodataframe. . Let&#39;s get the map data / shapefiles - . import geopandas as gpd uri_50m = &quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip&quot; uri_110m = &quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip&quot; countries_raw = gpd.read_file(uri_50m) . countries_map = countries_raw.copy() countries_map.head() . featurecla scalerank LABELRANK SOVEREIGNT SOV_A3 ADM0_DIF LEVEL TYPE ADMIN ADM0_A3 ... NAME_KO NAME_NL NAME_PL NAME_PT NAME_RU NAME_SV NAME_TR NAME_VI NAME_ZH geometry . 0 Admin-0 country | 1 | 3 | Zimbabwe | ZWE | 0 | 2 | Sovereign country | Zimbabwe | ZWE | ... | 짐바브웨 | Zimbabwe | Zimbabwe | Zimbábue | Зимбабве | Zimbabwe | Zimbabve | Zimbabwe | 辛巴威 | POLYGON ((31.28789 -22.40205, 31.19727 -22.344... | . 1 Admin-0 country | 1 | 3 | Zambia | ZMB | 0 | 2 | Sovereign country | Zambia | ZMB | ... | 잠비아 | Zambia | Zambia | Zâmbia | Замбия | Zambia | Zambiya | Zambia | 赞比亚 | POLYGON ((30.39609 -15.64307, 30.25068 -15.643... | . 2 Admin-0 country | 1 | 3 | Yemen | YEM | 0 | 2 | Sovereign country | Yemen | YEM | ... | 예멘 | Jemen | Jemen | Iémen | Йемен | Jemen | Yemen | Yemen | 也门 | MULTIPOLYGON (((53.08564 16.64839, 52.58145 16... | . 3 Admin-0 country | 3 | 2 | Vietnam | VNM | 0 | 2 | Sovereign country | Vietnam | VNM | ... | 베트남 | Vietnam | Wietnam | Vietname | Вьетнам | Vietnam | Vietnam | Việt Nam | 越南 | MULTIPOLYGON (((104.06396 10.39082, 104.08301 ... | . 4 Admin-0 country | 5 | 3 | Venezuela | VEN | 0 | 2 | Sovereign country | Venezuela | VEN | ... | 베네수엘라 | Venezuela | Wenezuela | Venezuela | Венесуэла | Venezuela | Venezuela | Venezuela | 委內瑞拉 | MULTIPOLYGON (((-60.82119 9.13838, -60.94141 9... | . 5 rows × 95 columns . We have a lot of columns here that we do not need. We will drop all that we do not need. . countries_map.columns . Index([&#39;featurecla&#39;, &#39;scalerank&#39;, &#39;LABELRANK&#39;, &#39;SOVEREIGNT&#39;, &#39;SOV_A3&#39;, &#39;ADM0_DIF&#39;, &#39;LEVEL&#39;, &#39;TYPE&#39;, &#39;ADMIN&#39;, &#39;ADM0_A3&#39;, &#39;GEOU_DIF&#39;, &#39;GEOUNIT&#39;, &#39;GU_A3&#39;, &#39;SU_DIF&#39;, &#39;SUBUNIT&#39;, &#39;SU_A3&#39;, &#39;BRK_DIFF&#39;, &#39;NAME&#39;, &#39;NAME_LONG&#39;, &#39;BRK_A3&#39;, &#39;BRK_NAME&#39;, &#39;BRK_GROUP&#39;, &#39;ABBREV&#39;, &#39;POSTAL&#39;, &#39;FORMAL_EN&#39;, &#39;FORMAL_FR&#39;, &#39;NAME_CIAWF&#39;, &#39;NOTE_ADM0&#39;, &#39;NOTE_BRK&#39;, &#39;NAME_SORT&#39;, &#39;NAME_ALT&#39;, &#39;MAPCOLOR7&#39;, &#39;MAPCOLOR8&#39;, &#39;MAPCOLOR9&#39;, &#39;MAPCOLOR13&#39;, &#39;POP_EST&#39;, &#39;POP_RANK&#39;, &#39;GDP_MD_EST&#39;, &#39;POP_YEAR&#39;, &#39;LASTCENSUS&#39;, &#39;GDP_YEAR&#39;, &#39;ECONOMY&#39;, &#39;INCOME_GRP&#39;, &#39;WIKIPEDIA&#39;, &#39;FIPS_10_&#39;, &#39;ISO_A2&#39;, &#39;ISO_A3&#39;, &#39;ISO_A3_EH&#39;, &#39;ISO_N3&#39;, &#39;UN_A3&#39;, &#39;WB_A2&#39;, &#39;WB_A3&#39;, &#39;WOE_ID&#39;, &#39;WOE_ID_EH&#39;, &#39;WOE_NOTE&#39;, &#39;ADM0_A3_IS&#39;, &#39;ADM0_A3_US&#39;, &#39;ADM0_A3_UN&#39;, &#39;ADM0_A3_WB&#39;, &#39;CONTINENT&#39;, &#39;REGION_UN&#39;, &#39;SUBREGION&#39;, &#39;REGION_WB&#39;, &#39;NAME_LEN&#39;, &#39;LONG_LEN&#39;, &#39;ABBREV_LEN&#39;, &#39;TINY&#39;, &#39;HOMEPART&#39;, &#39;MIN_ZOOM&#39;, &#39;MIN_LABEL&#39;, &#39;MAX_LABEL&#39;, &#39;NE_ID&#39;, &#39;WIKIDATAID&#39;, &#39;NAME_AR&#39;, &#39;NAME_BN&#39;, &#39;NAME_DE&#39;, &#39;NAME_EN&#39;, &#39;NAME_ES&#39;, &#39;NAME_FR&#39;, &#39;NAME_EL&#39;, &#39;NAME_HI&#39;, &#39;NAME_HU&#39;, &#39;NAME_ID&#39;, &#39;NAME_IT&#39;, &#39;NAME_JA&#39;, &#39;NAME_KO&#39;, &#39;NAME_NL&#39;, &#39;NAME_PL&#39;, &#39;NAME_PT&#39;, &#39;NAME_RU&#39;, &#39;NAME_SV&#39;, &#39;NAME_TR&#39;, &#39;NAME_VI&#39;, &#39;NAME_ZH&#39;, &#39;geometry&#39;], dtype=&#39;object&#39;) . #countries_map = countries_map.drop(columns=list(countries_map.columns[:4]) + list(countries_map.columns[5:-1])) countries_map = countries_map.drop(columns=list(countries_map.columns[:17]) + list(countries_map.columns[18:46]) + list(countries_map.columns[47:-1])) . #countries_map = countries_map.rename(columns={&#39;SOV_A3&#39;: &#39;iso_code&#39;}) countries_map = countries_map.rename(columns={&#39;ISO_A3&#39;: &#39;iso_code&#39;}) countries_map.head() . NAME iso_code geometry . 0 Zimbabwe | ZWE | POLYGON ((31.28789 -22.40205, 31.19727 -22.344... | . 1 Zambia | ZMB | POLYGON ((30.39609 -15.64307, 30.25068 -15.643... | . 2 Yemen | YEM | MULTIPOLYGON (((53.08564 16.64839, 52.58145 16... | . 3 Vietnam | VNM | MULTIPOLYGON (((104.06396 10.39082, 104.08301 ... | . 4 Venezuela | VEN | MULTIPOLYGON (((-60.82119 9.13838, -60.94141 9... | . Let&#39;s find out the mismatches of iso_code for the countries that are there in our vaccine dataset and our geodataframe - . vaccine_location_data[~vaccine_location_data.iso_code.isin(countries_map.iso_code)]#[&#39;location&#39;].unique() . location iso_code vaccines last_observation_date source_name source_website . 47 England | NaN | Oxford/AstraZeneca | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 48 England | NaN | Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 57 France | FRA | Moderna | 2021-02-21 | Public Health France | https://www.data.gouv.fr/fr/datasets/donnees-r... | . 58 France | FRA | Oxford/AstraZeneca | 2021-02-21 | Public Health France | https://www.data.gouv.fr/fr/datasets/donnees-r... | . 59 France | FRA | Pfizer/BioNTech | 2021-02-21 | Public Health France | https://www.data.gouv.fr/fr/datasets/donnees-r... | . 63 Gibraltar | GIB | Pfizer/BioNTech | 2021-02-21 | Government of Gibraltar | https://twitter.com/GibraltarGov/status/136389... | . 119 Northern Cyprus | OWID_NCY | Pfizer/BioNTech | 2021-01-22 | Ministry of Health | https://cyprus-mail.com/2021/01/22/coronavirus... | . 120 Northern Cyprus | OWID_NCY | Sinovac | 2021-01-22 | Ministry of Health | https://cyprus-mail.com/2021/01/22/coronavirus... | . 121 Northern Ireland | NaN | Oxford/AstraZeneca | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 122 Northern Ireland | NaN | Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 123 Norway | NOR | Moderna | 2021-02-21 | Norwegian Institute of Public Health | https://www.fhi.no/sv/vaksine/koronavaksinasjo... | . 124 Norway | NOR | Oxford/AstraZeneca | 2021-02-21 | Norwegian Institute of Public Health | https://www.fhi.no/sv/vaksine/koronavaksinasjo... | . 125 Norway | NOR | Pfizer/BioNTech | 2021-02-21 | Norwegian Institute of Public Health | https://www.fhi.no/sv/vaksine/koronavaksinasjo... | . 145 Scotland | NaN | Oxford/AstraZeneca | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 146 Scotland | NaN | Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 176 Wales | NaN | Oxford/AstraZeneca | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . 177 Wales | NaN | Pfizer/BioNTech | 2021-02-21 | Government of the United Kingdom | https://coronavirus.data.gov.uk/details/health... | . Let&#39;s fix them now - . countries_map.loc[countries_map.NAME == &#39;France&#39;, &#39;iso_code&#39;] = &quot;FRA&quot; countries_map.loc[countries_map.NAME == &#39;Norway&#39;, &#39;iso_code&#39;] = &quot;NOR&quot; vaccine_location_data.loc[vaccine_location_data.location.isin([&#39;England&#39;, &#39;Wales&#39;, &#39;Scotland&#39;, &#39;Northern Ireland&#39;]), &#39;iso_code&#39;] = &quot;GBR&quot; . FInally let&#39;s check once again for any mismatches - . vaccine_location_data[~vaccine_location_data.iso_code.isin(countries_map.iso_code)]#[&#39;location&#39;].unique() . location iso_code vaccines last_observation_date source_name source_website . 63 Gibraltar | GIB | Pfizer/BioNTech | 2021-02-21 | Government of Gibraltar | https://twitter.com/GibraltarGov/status/136389... | . 119 Northern Cyprus | OWID_NCY | Pfizer/BioNTech | 2021-01-22 | Ministry of Health | https://cyprus-mail.com/2021/01/22/coronavirus... | . 120 Northern Cyprus | OWID_NCY | Sinovac | 2021-01-22 | Ministry of Health | https://cyprus-mail.com/2021/01/22/coronavirus... | . For our purposes we can ignore these datapoints. Now let&#39;s merge the dataframes and plot the results. . plot_data = countries_map.merge(vaccine_location_data, how=&#39;inner&#39;, on=&#39;iso_code&#39;) . plot_data.head() . NAME iso_code geometry location vaccines last_observation_date source_name source_website . 0 United States of America | USA | MULTIPOLYGON (((-132.74687 56.52568, -132.7576... | United States | Moderna | 2021-02-22 | Centers for Disease Control and Prevention | https://covid.cdc.gov/covid-data-tracker/#vacc... | . 1 United States of America | USA | MULTIPOLYGON (((-132.74687 56.52568, -132.7576... | United States | Pfizer/BioNTech | 2021-02-22 | Centers for Disease Control and Prevention | https://covid.cdc.gov/covid-data-tracker/#vacc... | . 2 Saint Helena | SHN | MULTIPOLYGON (((-5.69214 -15.99775, -5.78252 -... | Saint Helena | Oxford/AstraZeneca | 2021-02-03 | Government of Saint Helena | https://www.sainthelena.gov.sh/2021/news/covid... | . 3 Anguilla | AIA | POLYGON ((-63.00122 18.22178, -63.16001 18.171... | Anguilla | Oxford/AstraZeneca | 2021-02-14 | Ministry of Health | https://www.facebook.com/MinistryofHealthAngui... | . 4 Falkland Is. | FLK | MULTIPOLYGON (((-58.85020 -51.26992, -58.69751... | Falkland Islands | Oxford/AstraZeneca | 2021-02-15 | Government of the Falkland Islands | https://www.facebook.com/FalkIandsGov/posts/42... | . alt.Chart(plot_data).mark_geoshape().encode( color=&#39;vaccines:N&#39; ).project(&#39;equalEarth&#39;) . This is a little misleading because countries have multiple vaccines approved for their usages but we will get only one color here. So we will facet our chart based on the vaccine . alt.Chart(plot_data).mark_geoshape().encode( color=&#39;vaccines:N&#39;, #tooltip = [&#39;location&#39;], facet=alt.Facet(&#39;vaccines:N&#39;, columns=3) ).properties(width=100, height=100) . Well this is surprising. It should have worked isn&#39;t it? . Unfortunately there is a bug with faceting geoshape plots in Vega-Lite that you can follow in this issue. So for that we will try to achieve faceting by concatenating our charts drawn with filtered data. . alt.concat(*( alt.Chart(plot_data[plot_data[&#39;vaccines&#39;]==vaccine], title=vaccine).mark_geoshape().encode( color=alt.value(&#39;green&#39;), #tooltip=[&#39;location&#39;], ) for vaccine in list(plot_data.vaccines.unique()) ), columns=3, title=&quot;Where each vaccine is being used&quot; ) . This is great. Now we will give it some finishing touches - . base = alt.Chart(countries_map[countries_map.iso_code!=&#39;ATA&#39;]).mark_geoshape(fill=&#39;#eee&#39;, stroke=&quot;#fff&quot;, strokeWidth=0.5).project(&#39;equalEarth&#39;) alt.concat(*( base + alt.Chart(plot_data[plot_data[&#39;vaccines&#39;]==vaccine], title=vaccine, height=200, width=350).mark_geoshape(stroke=&quot;#fff&quot;, strokeWidth=0.5).encode( color=alt.value(&#39;#2e7265&#39;), #tooltip=[&#39;NAME&#39;], ) for vaccine in list(plot_data.vaccines.unique()) ), columns=3, title=&quot;Where each vaccine is being used&quot;, spacing=0 ).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/vaccine/2021/02/20/vaccine-tracker.html",
            "relUrl": "/geospatial/vaccine/2021/02/20/vaccine-tracker.html",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Virus Surge",
            "content": "Today we will make this brilliant visualization in the NYT Article U.S. Virus Cases Climb Toward a Third Peak. Both the static visualization and the interactive range slider. . . import pandas as pd import geopandas as gpd import altair as alt alt.renderers.set_embed_options(actions=False) . RendererRegistry.enable(&#39;default&#39;) . Shapefiles for US States from the topojson/us-atlas repository. Shapefiles from US Census are fine too. . states_shp_uri = &#39;https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json&#39; . states = gpd.read_file(states_shp_uri) states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y . We will remove the states that are not supported in the albersUsa projection. . # Albers USA projection does not support the following. states = states[~((states[&#39;id&#39;] == &#39;69&#39;) | (states[&#39;id&#39;] == &#39;78&#39;) | (states[&#39;id&#39;] == &#39;60&#39;) | (states[&#39;id&#39;] == &#39;72&#39;) | (states[&#39;id&#39;] == &#39;66&#39;))] states.head() . id name geometry lon lat . 0 01 | Alabama | MULTIPOLYGON (((-88.33102 30.23539, -88.13002 ... | -86.828534 | 32.788722 | . 1 02 | Alaska | MULTIPOLYGON (((-150.24276 61.13748, -150.2212... | -152.219940 | 64.201799 | . 2 04 | Arizona | POLYGON ((-114.71951 32.71893, -114.70157 32.7... | -111.664784 | 34.292803 | . 3 08 | Colorado | POLYGON ((-109.04843 41.00026, -108.17982 41.0... | -105.548071 | 38.998238 | . 4 12 | Florida | MULTIPOLYGON (((-80.75043 24.85767, -80.70018 ... | -82.497311 | 28.620301 | . Initial Visualization . alt.Chart(states).mark_geoshape().encode().project(&#39;albersUsa&#39;) . Getting the COVID data as usual from NYT GitHub Repository . us_covid_data_uri = &#39;https://github.com/nytimes/covid-19-data/blob/master/us-counties.csv?raw=true&#39; raw_data = pd.read_csv(us_covid_data_uri) raw_data[&#39;date&#39;] = pd.to_datetime(raw_data[&#39;date&#39;]) covid = raw_data.copy() covid.head() . date county state fips cases deaths . 0 2020-01-21 | Snohomish | Washington | 53061.0 | 1 | 0 | . 1 2020-01-22 | Snohomish | Washington | 53061.0 | 1 | 0 | . 2 2020-01-23 | Snohomish | Washington | 53061.0 | 1 | 0 | . 3 2020-01-24 | Cook | Illinois | 17031.0 | 1 | 0 | . 4 2020-01-24 | Snohomish | Washington | 53061.0 | 1 | 0 | . We will also load the shapefile for counties because we need them for calculating the longitude and latitudes of the centroids of polygons of the counties . counties_shp_uri = &#39;https://cdn.jsdelivr.net/npm/us-atlas@3/counties-10m.json&#39; counties = gpd.read_file(counties_shp_uri) counties.head() . id name geometry . 0 04015 | Mohave | POLYGON ((-114.05190 36.84327, -114.05190 37.0... | . 1 22105 | Tangipahoa | POLYGON ((-90.56715 30.99995, -90.54921 30.999... | . 2 16063 | Lincoln | POLYGON ((-114.59389 43.19860, -114.37494 43.1... | . 3 27119 | Polk | POLYGON ((-97.14633 48.17341, -96.50026 48.174... | . 4 38017 | Cass | POLYGON ((-97.70626 47.23961, -97.45142 47.238... | . Removing any empty geometries beacause that will give errors when calculating longitudes and latitudes from the centroid . counties.geometry.is_empty.any() . True . empty = counties.geometry.is_empty counties = counties[~empty] . counties[&#39;lon&#39;] = counties[&#39;geometry&#39;].centroid.x counties[&#39;lat&#39;] = counties[&#39;geometry&#39;].centroid.y . counties.head() . id name geometry lon lat . 0 04015 | Mohave | POLYGON ((-114.05190 36.84327, -114.05190 37.0... | -113.758228 | 35.704987 | . 1 22105 | Tangipahoa | POLYGON ((-90.56715 30.99995, -90.54921 30.999... | -90.404976 | 30.626271 | . 2 16063 | Lincoln | POLYGON ((-114.59389 43.19860, -114.37494 43.1... | -114.138249 | 43.002348 | . 3 27119 | Polk | POLYGON ((-97.14633 48.17341, -96.50026 48.174... | -96.401592 | 47.774206 | . 4 38017 | Cass | POLYGON ((-97.70626 47.23961, -97.45142 47.238... | -97.248351 | 46.933123 | . counties.rename(columns={&#39;id&#39;: &#39;fips&#39;}, inplace=True) counties.fips = counties.fips.astype(int) . The usual NYC aggregation because of the way data is reported by NYT. . # Extract the boroughs in shapefile geodataframe for NYC boroughs_nyc = counties[counties[&#39;fips&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] boroughs_nyc[&#39;State&#39;] = &quot;NYC&quot; #combined_nyc = boroughs_nyc.dissolve(by=&#39;STATEFP&#39;) agg_nyc_data = boroughs_nyc.dissolve(by=&#39;State&#39;).reset_index() agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y agg_nyc_data = agg_nyc_data.drop(columns=[&#39;State&#39;]) counties = gpd.GeoDataFrame(pd.concat([counties, agg_nyc_data], ignore_index=True)) counties.tail() . fips name geometry lon lat . 3226 28001 | Adams | POLYGON ((-91.37833 31.73273, -91.31732 31.745... | -91.353097 | 31.479837 | . 3227 36069 | Ontario | POLYGON ((-77.58109 42.94432, -77.48418 42.943... | -77.300689 | 42.852556 | . 3228 54053 | Mason | POLYGON ((-82.10001 38.95828, -82.02822 39.028... | -82.026453 | 38.768440 | . 3229 4025 | Yavapai | POLYGON ((-113.33405 35.52805, -113.26226 35.5... | -112.554378 | 34.599427 | . 3230 1 | New York | MULTIPOLYGON (((-74.20356 40.59307, -74.20356 ... | -73.925717 | 40.696604 | . covid = raw_data.copy() covid = covid[~((covid[&#39;state&#39;] == &quot;Puerto Rico&quot;)|(covid[&#39;state&#39;] == &quot;Virgin Islands&quot;)|(covid[&#39;state&#39;] == &quot;Guam&quot;)|(covid[&#39;state&#39;] == &quot;Northern Mariana Islands&quot;))] # Extract the boroughs in shapefile geodataframe for NYC boroughs_nyc = counties[counties[&#39;fips&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] boroughs_nyc[&#39;State&#39;] = &quot;NYC&quot; #combined_nyc = boroughs_nyc.dissolve(by=&#39;STATEFP&#39;) agg_nyc_data = boroughs_nyc.dissolve(by=&#39;State&#39;).reset_index() agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y agg_nyc_data = agg_nyc_data.drop(columns=[&#39;State&#39;]) counties = gpd.GeoDataFrame(pd.concat([counties, agg_nyc_data], ignore_index=True)) # Make fips in covid data for &quot;New York City&quot; as 1 to reflect what we have done above covid.loc[covid[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 #covid = covid.merge(counties[[&#39;fips&#39;, &#39;lon&#39;, &#39;lat&#39;]], on=&#39;fips&#39;, how=&#39;inner&#39;) #covid = covid.drop(columns=[&#39;STATEFP&#39;, &#39;geometry&#39;]) # Getting per day statistics on CASES covid = covid.assign(daily=covid.groupby(&#39;fips&#39;)[&#39;cases&#39;].diff()) #covid = covid[covid[&#39;date&#39;] &gt; &#39;2020-05-01&#39;] # Working with data from May # Getting past 2 weeks data for cases per given date - sum of cases in past 2 weeks covid = covid.assign(past_2_weeks = covid.groupby(&#39;fips&#39;)[&#39;daily&#39;].apply(lambda case: case.rolling(window=14).sum().shift(1))) # Keeping only the latest date data covid = covid[covid[&#39;date&#39;] == &#39;2020-10-27&#39;] # Merging population estimate data - we could have done it earlier but what the heck! ## Getting census data census = pd.read_csv(&#39;co-est2019-alldata.csv&#39;) census = census[[&#39;STATE&#39;, &#39;COUNTY&#39;, &#39;STNAME&#39;, &#39;CTYNAME&#39;,&#39;POPESTIMATE2019&#39;]] # Keeping only 2019 population estimates ## Constructing FIPS from STATE and COUNTY codes in census itself census[&#39;STATE&#39;] = census.STATE.astype(str) census[&#39;COUNTY&#39;] = census.COUNTY.astype(str) census[&#39;STATE&#39;] = census.STATE.str.pad(2, fillchar=&#39;0&#39;) census[&#39;COUNTY&#39;] = census.COUNTY.str.pad(3, fillchar=&#39;0&#39;) census = census.assign(fips = census.STATE + census.COUNTY) census.fips = census.fips.astype(int) census = census.drop(columns=[&#39;STATE&#39;,&#39;COUNTY&#39;,&#39;STNAME&#39;,&#39;CTYNAME&#39;]) ## Merging the census data with covid data on FIPS covid = covid.merge(census, on=&#39;fips&#39;, how=&#39;inner&#39;) ## Getting the per capita data covid = covid.assign(past_2_weeks_per_capita = (covid.past_2_weeks/covid.POPESTIMATE2019)*1000) covid = covid[covid[&#39;past_2_weeks_per_capita&#39;].notna()] # Per Capita Deaths covid = covid.assign(deaths_per_capita = covid.deaths/covid.POPESTIMATE2019*1000) # Per Capita Deaths covid = covid.assign(cases_per_capita = covid.cases/covid.POPESTIMATE2019*1000) # Finally merging with geometry plot_data = counties.merge(covid, on=&#39;fips&#39;, how=&#39;inner&#39;) #plot_data = covid #plot_data = plot_data.drop(columns=[&#39;STATEFP&#39;, &#39;daily&#39;, &#39;past_2_weeks&#39;, &#39;past_2_weeks_per_capita&#39;, &#39;deaths_per_capita&#39;]) print(plot_data.info()) plot_data.head() . &lt;class &#39;geopandas.geodataframe.GeoDataFrame&#39;&gt; Int64Index: 3129 entries, 0 to 3128 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 fips 3129 non-null int64 1 name 3129 non-null object 2 geometry 3129 non-null geometry 3 lon 3129 non-null float64 4 lat 3129 non-null float64 5 date 3129 non-null datetime64[ns] 6 county 3129 non-null object 7 state 3129 non-null object 8 cases 3129 non-null int64 9 deaths 3129 non-null int64 10 daily 3129 non-null float64 11 past_2_weeks 3129 non-null float64 12 POPESTIMATE2019 3129 non-null int64 13 past_2_weeks_per_capita 3129 non-null float64 14 deaths_per_capita 3129 non-null float64 15 cases_per_capita 3129 non-null float64 dtypes: datetime64[ns](1), float64(7), geometry(1), int64(4), object(3) memory usage: 415.6+ KB None . fips name geometry lon lat date county state cases deaths daily past_2_weeks POPESTIMATE2019 past_2_weeks_per_capita deaths_per_capita cases_per_capita . 0 4015 | Mohave | POLYGON ((-114.05190 36.84327, -114.05190 37.0... | -113.758228 | 35.704987 | 2020-10-27 | Mohave | Arizona | 4313 | 230 | 41.0 | 183.0 | 212181 | 0.862471 | 1.083980 | 20.326985 | . 1 22105 | Tangipahoa | POLYGON ((-90.56715 30.99995, -90.54921 30.999... | -90.404976 | 30.626271 | 2020-10-27 | Tangipahoa | Louisiana | 5004 | 126 | 21.0 | 176.0 | 134758 | 1.306045 | 0.935009 | 37.133231 | . 2 16063 | Lincoln | POLYGON ((-114.59389 43.19860, -114.37494 43.1... | -114.138249 | 43.002348 | 2020-10-27 | Lincoln | Idaho | 199 | 1 | 13.0 | 81.0 | 5366 | 15.095043 | 0.186359 | 37.085352 | . 3 27119 | Polk | POLYGON ((-97.14633 48.17341, -96.50026 48.174... | -96.401592 | 47.774206 | 2020-10-27 | Polk | Minnesota | 724 | 4 | 51.0 | 268.0 | 31364 | 8.544828 | 0.127535 | 23.083790 | . 4 38017 | Cass | POLYGON ((-97.70626 47.23961, -97.45142 47.238... | -97.248351 | 46.933123 | 2020-10-27 | Cass | North Dakota | 8925 | 84 | 131.0 | 2259.0 | 181923 | 12.417341 | 0.461734 | 49.059217 | . Making a custom height column with SVG path strings so that we scale the data using that column . plot_data = plot_data.assign(height = plot_data[&#39;past_2_weeks_per_capita&#39;].apply(lambda x: f&quot;M -1 0 L 0 -{x*2} L 1 0&quot;)) . Plotting the data - . #collapse base = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode(text=&#39;name:N&#39;, longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;).project(&#39;albersUsa&#39;) text = alt.Chart(states).mark_text().encode(text=&#39;name:N&#39;, longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;, tooltip=[&#39;name:N&#39;]).project(&#39;albersUsa&#39;) spikes = alt.Chart(plot_data).mark_point( fillOpacity=0.5, fill=&#39;red&#39;, strokeOpacity=1, strokeWidth=1, stroke=&#39;red&#39;, ).encode( shape=alt.Shape(&#39;height&#39;, scale=None), longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;, ).project(&#39;albersUsa&#39;).properties(width=1200, height=900) . . (base+text+spikes).configure_view(stroke=None) . Making the interactive range slider plot . Same data processing steps as above but we don&#39;t restrict ourselves to one date. That&#39;s the idea of the interactive plot. As we move the thumb we change the date for which the data is displayed. . covid = raw_data.copy() # Making this on 27th October so limiting data till 27th becasue publishing it on website will take more time and experiments with the looks and interactivity covid = covid[covid[&#39;date&#39;]&lt;=&#39;2020-10-27&#39;] # Albers USA projection does not support the following. covid = covid[~((covid[&#39;state&#39;] == &quot;Puerto Rico&quot;)|(covid[&#39;state&#39;] == &quot;Virgin Islands&quot;)|(covid[&#39;state&#39;] == &quot;Guam&quot;)|(covid[&#39;state&#39;] == &quot;Northern Mariana Islands&quot;))] covid.loc[covid[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 # Getting per day statistics on CASES covid = covid.assign(daily=covid.groupby(&#39;fips&#39;)[&#39;cases&#39;].diff()) #covid = covid[covid[&#39;date&#39;] &gt; &#39;2020-05-01&#39;] # Working with data from May # Getting past 2 weeks data for cases per given date - sum of cases in past 2 weeks covid = covid.assign(past_2_weeks = covid.groupby(&#39;fips&#39;)[&#39;daily&#39;].apply(lambda case: case.rolling(window=14).sum().shift(1))) # Merging population estimate data - we could have done it earlier but what the heck! ## Getting census data census = pd.read_csv(&#39;co-est2019-alldata.csv&#39;) census = census[[&#39;STATE&#39;, &#39;COUNTY&#39;, &#39;STNAME&#39;, &#39;CTYNAME&#39;,&#39;POPESTIMATE2019&#39;]] # Keeping only 2019 population estimates ## Constructing FIPS from STATE and COUNTY codes in census itself census[&#39;STATE&#39;] = census.STATE.astype(str) census[&#39;COUNTY&#39;] = census.COUNTY.astype(str) census[&#39;STATE&#39;] = census.STATE.str.pad(2, fillchar=&#39;0&#39;) census[&#39;COUNTY&#39;] = census.COUNTY.str.pad(3, fillchar=&#39;0&#39;) census = census.assign(fips = census.STATE + census.COUNTY) census.fips = census.fips.astype(int) nyc = census[(census[&#39;STATE&#39;] == &#39;36&#39;) &amp; (census[&#39;COUNTY&#39;].isin([&#39;005&#39;, &#39;047&#39;, &#39;061&#39;, &#39;081&#39;, &#39;085&#39;]))] nyc_census = pd.DataFrame([[36, 1, &#39;New York&#39;, &#39;New York City&#39;,nyc[&#39;POPESTIMATE2019&#39;].sum(),1]], columns=[&#39;STATE&#39;,&#39;COUNTY&#39;,&#39;STNAME&#39;,&#39;CTYNAME&#39;,&#39;POPESTIMATE2019&#39;,&#39;fips&#39;]) census = pd.concat([census, nyc_census]) census = census.drop(columns=[&#39;STATE&#39;,&#39;COUNTY&#39;,&#39;STNAME&#39;,&#39;CTYNAME&#39;]) ## Merging the census data with covid data on FIPS covid = covid.merge(census, on=&#39;fips&#39;, how=&#39;inner&#39;) ## Getting the per capita data covid = covid.assign(past_2_weeks_per_capita = covid.past_2_weeks/covid.POPESTIMATE2019*1000) covid = covid[covid[&#39;past_2_weeks_per_capita&#39;].notna()] # Getting latitude and longitude covid = covid.merge(counties, on=&#39;fips&#39;, how=&#39;inner&#39;) covid = covid.drop(columns=[&#39;county&#39;, &#39;geometry&#39;, &#39;state&#39;, &#39;fips&#39;, &#39;cases&#39;, &#39;deaths&#39;, &#39;daily&#39;, &#39;past_2_weeks&#39;, &#39;POPESTIMATE2019&#39;, &#39;name&#39;]) # Selecting dates when past two weeks sum of cases peaked #bonus = covid[covid[&#39;past_2_weeks_per_capita&#39;]&gt;2.5] print(covid.info()) covid.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 606193 entries, 0 to 606192 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 date 606193 non-null datetime64[ns] 1 past_2_weeks_per_capita 606193 non-null float64 2 lon 606193 non-null float64 3 lat 606193 non-null float64 dtypes: datetime64[ns](1), float64(3) memory usage: 23.1 MB None . date past_2_weeks_per_capita lon lat . 0 2020-02-05 | 0.0 | -121.697119 | 48.047601 | . 1 2020-02-06 | 0.0 | -121.697119 | 48.047601 | . 2 2020-02-07 | 0.0 | -121.697119 | 48.047601 | . 3 2020-02-08 | 0.0 | -121.697119 | 48.047601 | . 4 2020-02-09 | 0.0 | -121.697119 | 48.047601 | . To play with the interactive chart online in this page, I have restricted the data by filtering data where the per capita last 2 weeks cases were more than 2.5 If you run it locally then you can run the whole thing using the data_server. . covid = covid.assign(height = covid[&#39;past_2_weeks_per_capita&#39;].apply(lambda x: f&quot;M -1 0 L 0 -{x*2} L 1 0&quot;)) covid = covid[covid[&#39;past_2_weeks_per_capita&#39;]&gt;2.5] print(covid.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 136425 entries, 336 to 606187 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 date 136425 non-null datetime64[ns] 1 past_2_weeks_per_capita 136425 non-null float64 2 lon 136425 non-null float64 3 lat 136425 non-null float64 4 height 136425 non-null object dtypes: datetime64[ns](1), float64(3), object(1) memory usage: 6.2+ MB None . To be able to view the interactive chart, I have uploaded the data that Altair needs as a json file and I will directly pass the url to the Chart class. . url = &quot;https://raw.githubusercontent.com/armsp/covidviz/master/assets/virus_surge_data.json&quot; . #collapse def timestamp(t): return pd.to_datetime(t).timestamp() * 1000 slider = alt.binding_range(name=&#39;till_date:&#39;, step=1 * 24 * 60 * 60 * 1000, min=timestamp(min(bonus[&#39;date&#39;])), max=timestamp(max(bonus[&#39;date&#39;]))) day = alt.selection_single(bind=slider, name=&quot;slider&quot;, fields=[&#39;date&#39;], init={&#39;date&#39;:timestamp(min(bonus[&#39;date&#39;]))}) spikes = alt.Chart(url,).mark_point( fillOpacity=0.3, fill=&#39;red&#39;, strokeOpacity=1, strokeWidth=1, stroke=&#39;red&#39; ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height:N&quot;, scale=None), # size=&#39;cases:Q&#39; ).project( type=&#39;albersUsa&#39; ).add_selection(day).transform_filter( &quot;(year(datum.date) == year(slider.date[0])) &amp;&amp; (month(datum.date) == month(slider.date[0])) &amp;&amp; (date(datum.date) == date(slider.date[0]))&quot; ) . . Wait for a few seconds for the data to load in the background - its over 20MB. Interactivity only works once the data is loaded. . (base+text+spikes).properties(width=1500, height=800, padding={&#39;top&#39;: 225, &#39;bottom&#39;: 5}).configure_view(stroke=None) .",
            "url": "https://armsp.github.io/covidviz/interactive/nyt/geospatial/2020/10/29/Virus_Surge.html",
            "relUrl": "/interactive/nyt/geospatial/2020/10/29/Virus_Surge.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Job Reports by Industry",
            "content": ". Today we will make the chart that looks like above which appeared the following reports on Slowdown of job growth in various industries - . https://www.nytimes.com/2020/09/04/business/augusts-slowdown-in-job-growth-spanned-many-industries.html | https://www.nytimes.com/2020/09/04/business/economy/jobs-report.html | . Finding out the data used by the articles was tedious but I managed it anyways. Here&#39;s how did it - . Main cited source is Beaureau of Labour Statistics | Painfully going through the website, didn&#39;t make much sense of all that was there. Too much info. | Google Search - BLS employment by industry data | Opened this link and thought this looks interesting - https://www.bls.gov/charts/employment-situation/employment-levels-by-industry.htm# | We know now that we need Employmnt by Industry Data | Lets try API - https://www.bls.gov/data/#api -&gt; https://www.bls.gov/developers/ Python Example - https://www.bls.gov/developers/api_python.htm#python2 | . | So we need Series ID for the tables | Data Tools -&gt; Series Report -&gt; Series ID Formats -&gt; Employment &amp; Unemployment -&gt; National Employment, Hours, and Earnings -&gt; That has all the information about how to construct the query. | Queried Answer matches exactly the Show Table result on the 3rd Step! | import altair as alt import requests import pandas as pd alt.renderers.set_embed_options(actions=False) uri = &quot;https://api.bls.gov/publicAPI/v2/timeseries/data/&quot; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&#39;} headers = {&#39;Content-type&#39;: &#39;application/json&#39;} . r = requests.post(uri, data=&#39;{&quot;seriesid&quot;:[&quot;CES6500000001&quot;, &quot;CES6000000001&quot;, &quot;CES3000000001&quot;, &quot;CES7000000001&quot;, &quot;CES4200000001&quot;, &quot;CES2000000001&quot;], &quot;startyear&quot;:&quot;2016&quot;, &quot;endyear&quot;:&quot;2020&quot;}&#39;, headers=headers) . data = pd.concat([pd.DataFrame({&#39;type&#39;: x[&#39;seriesID&#39;], **(pd.concat([pd.Series(y).to_frame().T for y in x[&#39;data&#39;]])[::-1].to_dict(orient=&#39;list&#39;))}) for x in r.json()[&#39;Results&#39;][&#39;series&#39;]]) data = data.reset_index(drop=True) data.head() . type year period periodName latest value footnotes calculations . 0 CES6500000001 | 2016 | M01 | January | NaN | 22337 | [{}] | NaN | . 1 CES6500000001 | 2016 | M02 | February | NaN | 22405 | [{}] | NaN | . 2 CES6500000001 | 2016 | M03 | March | NaN | 22452 | [{}] | NaN | . 3 CES6500000001 | 2016 | M04 | April | NaN | 22510 | [{}] | NaN | . 4 CES6500000001 | 2016 | M05 | May | NaN | 22567 | [{}] | NaN | . data[&#39;value&#39;] = data[&#39;value&#39;].astype(float) data[&#39;time&#39;] = pd.to_datetime(data[&#39;year&#39;]+data[&#39;periodName&#39;], format=&quot;%Y%B&quot;) . data = data.assign(change = data.groupby(&#39;type&#39;)[&#39;value&#39;].transform(&#39;diff&#39;).reset_index(drop=True)) data = data.assign(cumulative_change = data.groupby(&#39;type&#39;)[&#39;value&#39;].apply(lambda x: x - x.iloc[0])) data.head() . type year period periodName latest value footnotes calculations time change cumulative_change . 0 CES6500000001 | 2016 | M01 | January | NaN | 22337.0 | [{}] | NaN | 2016-01-01 | NaN | 0.0 | . 1 CES6500000001 | 2016 | M02 | February | NaN | 22405.0 | [{}] | NaN | 2016-02-01 | 68.0 | 68.0 | . 2 CES6500000001 | 2016 | M03 | March | NaN | 22452.0 | [{}] | NaN | 2016-03-01 | 47.0 | 115.0 | . 3 CES6500000001 | 2016 | M04 | April | NaN | 22510.0 | [{}] | NaN | 2016-04-01 | 58.0 | 173.0 | . 4 CES6500000001 | 2016 | M05 | May | NaN | 22567.0 | [{}] | NaN | 2016-05-01 | 57.0 | 230.0 | . data[&#39;type&#39;] = data[&#39;type&#39;].apply(lambda x: &#39;Construction&#39; if x == &#39;CES2000000001&#39; else &#39;Education and health&#39; if x == &#39;CES6500000001&#39; else &#39;Business and professional services&#39; if x == &#39;CES6000000001&#39; else &#39;Manufacturing&#39; if x == &#39;CES3000000001&#39; else &#39;Leisure and hospitality&#39; if x == &#39;CES7000000001&#39; else &#39;Retail&#39;) . plot_data = data.copy() plot_data = plot_data.assign(since_feb = plot_data[&#39;type&#39;].map(plot_data.groupby(&#39;type&#39;).apply(lambda x: int(x[&#39;value&#39;].iloc[-1] - x[x[&#39;time&#39;] == &#39;2020-02-01&#39;][&#39;value&#39;])))) . plot_data.groupby(&#39;type&#39;).apply(lambda x: int(x[x[&#39;time&#39;] == &#39;2020-02-01&#39;][&#39;value&#39;] - x[&#39;value&#39;].iloc[-1])).reset_index(level=0,drop=True) . 0 1386 1 394 2 1397 3 3840 4 647 5 483 dtype: int64 . # plot_data[plot_data[&#39;type&#39;] == &#39;Education and health&#39;] . plot_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 342 entries, 0 to 341 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 type 342 non-null object 1 year 342 non-null object 2 period 342 non-null object 3 periodName 342 non-null object 4 latest 6 non-null object 5 value 342 non-null float64 6 footnotes 342 non-null object 7 calculations 90 non-null object 8 time 342 non-null datetime64[ns] 9 change 336 non-null float64 10 cumulative_change 342 non-null float64 11 since_feb 342 non-null int64 dtypes: datetime64[ns](1), float64(3), int64(1), object(7) memory usage: 32.2+ KB . We will use step-after interpolation method. Find out more about meaning of various interpolation methods at https://github.com/d3/d3-shape/blob/master/README.md#curves . alt.Chart(data, height=200, title=&quot;Some industries are approaching pre-pandemic employment, but leisure and hospitality jobs are lagging far behind&quot;).mark_area(line=True, interpolate=&#39;step-after&#39;,).encode( x= alt.X(&#39;time&#39;, title=None), y= alt.Y(&#39;cumulative_change:Q&#39;, title=None, scale=alt.Scale(domain=[-3000, 3000])), # row = alt.Row(&#39;type:N&#39;, rows=3) facet = alt.Facet(&#39;type:N&#39;, columns=2, spacing={&#39;row&#39;: -200}, sort=[&#39;Construction&#39;, &#39;Education and health&#39;, &#39;Business and professional services&#39;, &#39;Manufacturing&#39;, &#39;Retail&#39;, &#39;Leisure and hospitality&#39;], title=&quot;Cumulative change in jobs since August 2016, by industry&quot; ) ).resolve_axis(y=&#39;independent&#39;, x=&#39;independent&#39;) . Let&#39;s try to colour them differently from February - . alt.Chart(data, height=200, title=&quot;Some industries are approaching pre-pandemic employment, but leisure and hospitality jobs are lagging far behind&quot; ).mark_area( line=True, interpolate=&#39;step-after&#39; ).transform_calculate( recent = alt.datum.time &gt; alt.expr.toDate(&#39;2020-01-31&#39;) ).encode( x= alt.X(&#39;time&#39;, title=None), y= alt.Y(&#39;cumulative_change:Q&#39;, title=None, scale=alt.Scale(domain=[-3000, 3000])), color = &#39;recent:N&#39;, stroke = &#39;recent:N&#39;, # row = alt.Row(&#39;type:N&#39;, rows=3) facet = alt.Facet(&#39;type:N&#39;, columns=2, spacing={&#39;row&#39;: -200}, sort=[&#39;Construction&#39;, &#39;Education and health&#39;, &#39;Business and professional services&#39;, &#39;Manufacturing&#39;, &#39;Retail&#39;, &#39;Leisure and hospitality&#39;], title=&quot;Cumulative change in jobs since August 2016, by industry&quot; ) ).resolve_axis(y=&#39;independent&#39;, x=&#39;independent&#39;) . The lines are overlapped. Let&#39;s try a different approach - . base = alt.Chart(height=200).transform_calculate( recent = alt.datum.time &gt;= alt.expr.toDate(&#39;2020-01-01&#39;) ).encode( x= alt.X(&#39;time:T&#39;, title=None), y= alt.Y(&#39;cumulative_change:Q&#39;, title=None, scale=alt.Scale(domain=[-3000, 3000])), tooltip=[&#39;time&#39;] ) area = base.mark_area(interpolate=&#39;step-after&#39;, fillOpacity=0.6).encode(color = &#39;recent:N&#39;) line = base.mark_line(interpolate=&#39;step-after&#39;, color=&quot;blue&quot;).encode(color = &#39;recent:N&#39;)#&#39;recent:N&#39;,) alt.layer(area,line, data=plot_data).facet(alt.Facet(&#39;type:N&#39;, sort=[&#39;Construction&#39;, &#39;Education and health&#39;, &#39;Business and professional services&#39;, &#39;Manufacturing&#39;, &#39;Retail&#39;, &#39;Leisure and hospitality&#39;], title=&quot;Cumulative change in jobs since August 2016, by industry&quot; ) ).resolve_axis(y=&#39;independent&#39;, x=&#39;independent&#39;).configure_facet(columns=2).properties(title=&quot;Some industries are approaching pre-pandemic employment, but leisure and hospitality jobs are lagging far behind&quot;) . That was better. Let&#39;s make it aesthetically pleasing - . base = alt.Chart(height=200).transform_calculate( recent = alt.datum.time &gt;= alt.expr.toDate(&#39;2020-01-01&#39;) ).encode( x= alt.X(&#39;time:T&#39;, title=None, axis=alt.Axis(format=&quot;%y&quot;, domainDash=[2,2.5], domainWidth=1.5, tickCount=5, domain=False, labelPadding=1)), y= alt.Y(&#39;cumulative_change:Q&#39;, title=None, axis=alt.Axis(domain=False), scale=alt.Scale(domain=[-2000, 2000])), ) area = base.mark_area(interpolate=&#39;step-after&#39;, fillOpacity=0.1).encode(color = alt.Color(&#39;recent:N&#39;, legend=None)) line = base.mark_line(interpolate=&#39;step-after&#39;, strokeCap=&quot;round&quot;).encode(color = alt.Color(&#39;recent:N&#39;, scale=alt.Scale(domain=[&#39;false&#39;, &#39;true&#39;], range=[&#39;black&#39;, &#39;rgb(22, 174, 205)&#39;])))#&#39;recent:N&#39;,) dot = base.mark_circle(color=&#39;rgb(22, 174, 205)&#39;, size=50).encode( x=&#39;max(time)&#39;, y=alt.Y(&#39;cumulative_change:Q&#39;, aggregate={&#39;argmax&#39;: &#39;time&#39;}), ) text = base.mark_text(align=&#39;left&#39;, dx=10).encode( x=&#39;max(time)&#39;, y=alt.Y(&#39;cumulative_change:Q&#39;, aggregate={&#39;argmax&#39;: &#39;time&#39;}), text=alt.Text(&#39;since_feb:Q&#39;, aggregate={&#39;argmax&#39;: &#39;time&#39;}) ) h_rule = alt.Chart(pd.DataFrame({&#39;zero&#39;: [[0]]})).mark_rule(strokeDash=[2,2]).encode(y=&#39;zero:Q&#39;) alt.layer(area,line,dot,text, h_rule, data=plot_data).facet(facet=alt.Facet(&#39;type:N&#39;, sort=[&#39;Construction&#39;, &#39;Education and health&#39;, &#39;Business and professional services&#39;, &#39;Manufacturing&#39;, &#39;Retail&#39;, &#39;Leisure and hospitality&#39;], header=alt.Header(title=&quot;Cumulative change in jobs since August 2016, by industry&quot;, titleOrient=&quot;top&quot;, titleAnchor=&#39;start&#39;, titleFontSize=15, titleColor=&#39;grey&#39;) ), spacing={&quot;row&quot;: -100} ).resolve_axis(y=&#39;independent&#39;, x=&#39;independent&#39;).configure_facet(columns=2).configure_axis(grid=False).configure_view(stroke=None).properties(title=&quot;Some industries are approaching pre-pandemic employment, but leisure and hospitality jobs are lagging far behind&quot;).configure_title(fontSize=17) . We cannot fix the row spacing because of the last chart - all the facet charts will have the area of the sub-chart with the mximum area. .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/09/03/Job-Report.html",
            "relUrl": "/nyt/2020/09/03/Job-Report.html",
            "date": " • Sep 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Mobility across US",
            "content": "Today we will make a graph that I just love - Ridgeline Plots. We will visualize the median mobility across US states as they appeared in the article by Axios : How the coronavirus pandemic changed mobility habits, by state . . The data comes from Descartes Lab&#39;s GitHub Repository . import altair as alt import pandas as pd mobility_uri = &#39;https://raw.githubusercontent.com/descarteslabs/DL-COVID-19/master/DL-us-mobility-daterow.csv&#39; alt.renderers.set_embed_options(actions=False) . RendererRegistry.enable(&#39;default&#39;) . mobility = pd.read_csv(mobility_uri) mobility.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 631087 entries, 0 to 631086 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 date 631087 non-null object 1 country_code 631087 non-null object 2 admin_level 631087 non-null int64 3 admin1 630854 non-null object 4 admin2 618971 non-null object 5 fips 630854 non-null float64 6 samples 631087 non-null int64 7 m50 631087 non-null float64 8 m50_index 631087 non-null int64 dtypes: float64(2), int64(3), object(4) memory usage: 43.3+ MB . mobility.head() . date country_code admin_level admin1 admin2 fips samples m50 m50_index . 0 2020-03-01 | US | 1 | Alabama | NaN | 1.0 | 133826 | 8.331 | 79 | . 1 2020-03-02 | US | 1 | Alabama | NaN | 1.0 | 143632 | 10.398 | 98 | . 2 2020-03-03 | US | 1 | Alabama | NaN | 1.0 | 146009 | 10.538 | 100 | . 3 2020-03-04 | US | 1 | Alabama | NaN | 1.0 | 149352 | 10.144 | 96 | . 4 2020-03-05 | US | 1 | Alabama | NaN | 1.0 | 144109 | 10.982 | 104 | . mobility_states = mobility[mobility[&#39;admin2&#39;].isnull() &amp; mobility[&#39;admin1&#39;].notnull()] mobility_states.head() . date country_code admin_level admin1 admin2 fips samples m50 m50_index . 0 2020-03-01 | US | 1 | Alabama | NaN | 1.0 | 133826 | 8.331 | 79 | . 1 2020-03-02 | US | 1 | Alabama | NaN | 1.0 | 143632 | 10.398 | 98 | . 2 2020-03-03 | US | 1 | Alabama | NaN | 1.0 | 146009 | 10.538 | 100 | . 3 2020-03-04 | US | 1 | Alabama | NaN | 1.0 | 149352 | 10.144 | 96 | . 4 2020-03-05 | US | 1 | Alabama | NaN | 1.0 | 144109 | 10.982 | 104 | . The data also contains the aggreagted median mobility for US as a country. We will filter that for our chart as we want only states - . #mobility_states.groupby(&#39;admin1&#39;)[&#39;m50_index&#39;].max() . usa_mobility = mobility[mobility[&#39;admin2&#39;].isnull() &amp; mobility[&#39;admin1&#39;].isnull()] usa_mobility.head() . date country_code admin_level admin1 admin2 fips samples m50 m50_index . 554459 2020-03-01 | US | 0 | NaN | NaN | NaN | 5705566 | 5.320 | 68 | . 554460 2020-03-02 | US | 0 | NaN | NaN | NaN | 5970602 | 7.789 | 99 | . 554461 2020-03-03 | US | 0 | NaN | NaN | NaN | 6100493 | 7.821 | 100 | . 554462 2020-03-04 | US | 0 | NaN | NaN | NaN | 6274372 | 7.783 | 99 | . 554463 2020-03-05 | US | 0 | NaN | NaN | NaN | 6023240 | 8.288 | 105 | . If you are interested in USA&#39;s mobility as a whole then you can visualize the following dataframe - . alt.data_transformers.enable(&#39;json&#39;) #alt.data_transformers.enable(&#39;data_server&#39;) . DataTransformerRegistry.enable(&#39;json&#39;) . I have taken the liberty to also color the facets by the median of the mobility values. To get it exactly like the chart by Axios, just remove the fill encoding. . url = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/2020-08-31_Mobility_Data.json&#39; # comment this when running locally # url = mobility_states # uncomment this when running locally highlight = alt.selection_single(on=&#39;mouseover&#39;, empty=&#39;all&#39;) # it looks like at present &quot;facet&quot; is not accepted as an encoding here even though it has been added as an encoding in a traditional sense of usage alt.Chart(url, height=40, width=700,).mark_area().transform_window( avg_m50 =&#39;mean(m50_index)&#39;, frame=(-6,0), groupby=[&#39;fips&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None, axis=alt.Axis(domain=True, ticks=False, labels=True, format=&quot;%b&quot;, tickCount=5)), y = alt.Y(&#39;avg_m50:Q&#39;, title=None, axis=None, scale=alt.Scale(range=[50, -100], zero=False),),#zero=False is important because of how vega-lite handles the range of data internally. When we used the raw values without averaging then it did not think that we may have negative values etc so the graph was not expanded for that. But when we did the averaging, somehow it decided to have some space for negative values, that&#39;s why your graph was pulled up from x-axis. By mentioning that we will not show zero or below values you manually fixed that range and the graph was pulled down as it was earlier. fillOpacity= alt.condition(highlight, alt.value(0.8), alt.value(0.4)), fill = alt.Fill( &#39;median(avg_m50):Q&#39;, legend=None, scale=alt.Scale(domain=[0,170],scheme=&#39;yellowgreenblue&#39;) #setting up domain gives favourable colours ), facet = alt.Facet(&#39;admin1:N&#39;, title=None, columns=1, header=alt.Header(labelAngle=0, labelOrient=&#39;left&#39;, labelAlign=&#39;left&#39;, labelAnchor=&#39;middle&#39;)) ).configure_facet(spacing=0,).properties(bounds=&#39;flush&#39;, title=&#39;Median Mobility&#39;,).configure_view(stroke=None).configure_title( anchor=&#39;middle&#39; ).add_selection(highlight) .",
            "url": "https://armsp.github.io/covidviz/mobility/interactive/axios/2020/08/31/Mobility.html",
            "relUrl": "/mobility/interactive/axios/2020/08/31/Mobility.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Relation b/w median income, average household size & cases per capita in NYC",
            "content": "Today we will make the plots that are present in this webpage that tracks the situation in New York City - New York City Coronavirus Map and Case Count . There are two main plots - . Cases per capita | Plot analyzing the relationship b/w average household size, income and cases per capita | . They have since corrected the graph - . Old . New . #hide_output import pandas as pd import altair as alt import geopandas as gpd alt.renderers.set_embed_options(actions=False) . Fortunately NYC Department of Health and Mental Hygiene publishes their data in their GitHub Repo. It has all the data from cases to the shapefiles too. . But we will use the data from NYC Open Data which is equally good. We will use the NYC Department of Health and Mental Hygiene GitHub repo only for their COVID data per MODZCTA. . The location of the dataset on NYC Open Data portal is https://data.cityofnewyork.us/Health/Modified-Zip-Code-Tabulation-Areas-MODZCTA-/pri4-ifjk which we will export as geojson. . import requests resp = requests.get(&#39;https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&amp;format=GeoJSON&#39;) data = resp.json() nyc_zip = gpd.GeoDataFrame.from_features(data[&#39;features&#39;]) nyc_zip.head() . geometry label modzcta pop_est zcta . 0 MULTIPOLYGON (((-73.98774 40.74407, -73.98819 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | . 1 MULTIPOLYGON (((-73.99750 40.71407, -73.99709 ... | 10002 | 10002 | 74993 | 10002 | . 2 MULTIPOLYGON (((-73.98864 40.72293, -73.98876 ... | 10003 | 10003 | 54682 | 10003 | . 3 MULTIPOLYGON (((-74.00827 40.70772, -74.00937 ... | 10004 | 10004 | 3028 | 10004 | . 4 MULTIPOLYGON (((-74.00783 40.70309, -74.00786 ... | 10005 | 10005 | 8831 | 10005, 10271 | . # If you have the data locally #modzcta = &#39;MODZCTA/Modified Zip Code Tabulation Areas.geojson&#39; #nyc_zip_shp = gpd.read_file(modzcta) . This is how it looks when colored based on population - . alt.Chart(nyc_zip).mark_geoshape().encode( color=&#39;pop_est:Q&#39; ) . Now we will get the Median Household Income data from Census Reporter&#39;s wonderful website. In particular the url query for the 5 boroughs in NYC is https://censusreporter.org/data/table/?table=B19013&amp;geo_ids=860%7C05000US36061,860%7C05000US36047,860%7C05000US36081,860%7C05000US36005,860%7C05000US36085 which I came to know thanks to this comment in NYC Health covid data repo. . Download it and export it to work with it further (I chose the Shapefile, but I suppose others would be fine too). . median_income_path = &#39;MODZCTA/median_income_nyc/acs2018_5yr_B19013_86000US11417/acs2018_5yr_B19013_86000US11417.shp&#39; #push it to datasets repo and link here median_income = gpd.read_file(median_income_path) median_income.head() . geoid name B19013001 B19013001e geometry . 0 86000US10001 | 10001 | 88526.0 | 8060.0 | POLYGON ((-74.00828 40.75027, -74.00783 40.751... | . 1 86000US10002 | 10002 | 35859.0 | 2149.0 | POLYGON ((-73.99750 40.71407, -73.99709 40.714... | . 2 86000US10003 | 10003 | 112131.0 | 13190.0 | POLYGON ((-73.99937 40.73132, -73.99911 40.731... | . 3 86000US10004 | 10004 | 157645.0 | 17195.0 | MULTIPOLYGON (((-73.99814 40.70152, -73.99617 ... | . 4 86000US10005 | 10005 | 173333.0 | 33390.0 | POLYGON ((-74.01251 40.70677, -74.01195 40.707... | . The regions in median income data that are not part of NTC&#39;s modzcta can be seen as follows(basically we are finding out if both datasets show the same regions) - . median_income[median_income[&#39;name&#39;].isin(nyc_zip[&#39;modzcta&#39;]) == False].plot() . &lt;AxesSubplot:&gt; . Fun Fact - the areas marked above are also NOT SHOWN in NYT&#39;s Charts . If you want you can plot the Median Income Data too using - . alt.Chart(median_income).mark_geoshape().encode( color=&#39;B19013001:Q&#39; ) . Now we will get the COVID data per MODZCTA . covid_zip_uri = &#39;https://raw.githubusercontent.com/nychealth/coronavirus-data/master/data-by-modzcta.csv&#39; covid_zip = pd.read_csv(covid_zip_uri) covid_zip.head() . MODIFIED_ZCTA NEIGHBORHOOD_NAME BOROUGH_GROUP COVID_CASE_COUNT COVID_CASE_RATE POP_DENOMINATOR COVID_DEATH_COUNT COVID_DEATH_RATE PERCENT_POSITIVE TOTAL_COVID_TESTS . 0 10001 | Chelsea/NoMad/West Chelsea | Manhattan | 440 | 1867.33 | 23563.03 | 26 | 110.34 | 7.18 | 6124 | . 1 10002 | Chinatown/Lower East Side | Manhattan | 1318 | 1717.14 | 76755.41 | 161 | 209.76 | 8.33 | 15831 | . 2 10003 | East Village/Gramercy/Greenwich Village | Manhattan | 534 | 992.54 | 53801.62 | 35 | 65.05 | 3.91 | 13671 | . 3 10004 | Financial District | Manhattan | 40 | 1095.71 | 3650.61 | 1 | 27.39 | 5.33 | 751 | . 4 10005 | Financial District | Manhattan | 94 | 1119.57 | 8396.11 | 2 | 23.82 | 4.91 | 1914 | . Merge the covid data with NYC shapefile data to be able to see the number of cases per modzcta . def modify_merge(nyc_zip, covid_zip): covid_zip = covid_zip.loc[:, [&#39;MODIFIED_ZCTA&#39;, &#39;BOROUGH_GROUP&#39;, &#39;COVID_CASE_COUNT&#39;, &#39;COVID_DEATH_COUNT&#39;, &#39;NEIGHBORHOOD_NAME&#39;]] nyc_zip[&#39;pop_est&#39;] = nyc_zip[&#39;pop_est&#39;].astype(int) nyc_zip[&#39;modzcta&#39;] = nyc_zip[&#39;modzcta&#39;].astype(int) covid_zip.rename(columns = {&#39;MODIFIED_ZCTA&#39;: &#39;modzcta&#39;}, inplace=True) covid_nyc_zip = nyc_zip.merge(covid_zip, how=&#39;left&#39;, on=&#39;modzcta&#39;) return covid_nyc_zip . covid_nyc_zip = modify_merge(nyc_zip, covid_zip) covid_nyc_zip.head() . geometry label modzcta pop_est zcta BOROUGH_GROUP COVID_CASE_COUNT COVID_DEATH_COUNT NEIGHBORHOOD_NAME . 0 MULTIPOLYGON (((-73.98774 40.74407, -73.98819 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | Manhattan | 440.0 | 26.0 | Chelsea/NoMad/West Chelsea | . 1 MULTIPOLYGON (((-73.99750 40.71407, -73.99709 ... | 10002 | 10002 | 74993 | 10002 | Manhattan | 1318.0 | 161.0 | Chinatown/Lower East Side | . 2 MULTIPOLYGON (((-73.98864 40.72293, -73.98876 ... | 10003 | 10003 | 54682 | 10003 | Manhattan | 534.0 | 35.0 | East Village/Gramercy/Greenwich Village | . 3 MULTIPOLYGON (((-74.00827 40.70772, -74.00937 ... | 10004 | 10004 | 3028 | 10004 | Manhattan | 40.0 | 1.0 | Financial District | . 4 MULTIPOLYGON (((-74.00783 40.70309, -74.00786 ... | 10005 | 10005 | 8831 | 10005, 10271 | Manhattan | 94.0 | 2.0 | Financial District | . Customary covid cases visualization per modfied zip code tabulation area - . alt.Chart(covid_nyc_zip).mark_geoshape().encode( color=&#39;COVID_CASE_COUNT&#39; ) . Now we will merge median income data with the covid cases per zip code tabulation area data we just derieved above - . def modify_merge(covid_nyc_zip, median_income): median_income.rename(columns={&#39;name&#39;: &#39;modzcta&#39;, &#39;B19013001&#39;: &#39;median_income&#39;}, inplace=True) median_income[&#39;modzcta&#39;] = median_income[&#39;modzcta&#39;].astype(int) median_income = median_income.drop(columns=[&#39;geometry&#39;, &#39;B19013001e&#39;]) covid_income_zip = covid_nyc_zip.merge(median_income, how=&#39;inner&#39;, on=&#39;modzcta&#39;) covid_income_zip = covid_income_zip.assign( #case_per_people = covid_income_zip[&#39;COVID_CASE_COUNT&#39;]/covid_income_zip[&#39;POP_DENOMINATOR&#39;], #their earlier calculation case_per_people = covid_income_zip[&#39;COVID_CASE_COUNT&#39;]/covid_income_zip[&#39;pop_est&#39;], # current calculation ) return covid_income_zip . covid_income_zip = modify_merge(covid_nyc_zip, median_income) covid_income_zip.head() . geometry label modzcta pop_est zcta BOROUGH_GROUP COVID_CASE_COUNT COVID_DEATH_COUNT NEIGHBORHOOD_NAME geoid median_income case_per_people . 0 MULTIPOLYGON (((-73.98774 40.74407, -73.98819 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | Manhattan | 440.0 | 26.0 | Chelsea/NoMad/West Chelsea | 86000US10001 | 88526.0 | 0.019071 | . 1 MULTIPOLYGON (((-73.99750 40.71407, -73.99709 ... | 10002 | 10002 | 74993 | 10002 | Manhattan | 1318.0 | 161.0 | Chinatown/Lower East Side | 86000US10002 | 35859.0 | 0.017575 | . 2 MULTIPOLYGON (((-73.98864 40.72293, -73.98876 ... | 10003 | 10003 | 54682 | 10003 | Manhattan | 534.0 | 35.0 | East Village/Gramercy/Greenwich Village | 86000US10003 | 112131.0 | 0.009766 | . 3 MULTIPOLYGON (((-74.00827 40.70772, -74.00937 ... | 10004 | 10004 | 3028 | 10004 | Manhattan | 40.0 | 1.0 | Financial District | 86000US10004 | 157645.0 | 0.013210 | . 4 MULTIPOLYGON (((-74.00783 40.70309, -74.00786 ... | 10005 | 10005 | 8831 | 10005, 10271 | Manhattan | 94.0 | 2.0 | Financial District | 86000US10005 | 173333.0 | 0.010644 | . Now let&#39;s plot this data and we will find that it exactly MATCHES NYT&#39;s chart for cases per people v/s median income - . alt.Chart(covid_income_zip).mark_circle().encode( color=alt.Color(&#39;median_income:Q&#39;, sort = &quot;descending&quot;), x=alt.X(&#39;median_income:Q&#39;), y=&#39;case_per_people:Q&#39; ) . Getting the number of people per household data . This data can be calculated from Household Type by Household Size data, again from Census Reporter - the url query would be https://censusreporter.org/data/table/?table=B11016&amp;geo_ids=860|05000US36061,860|05000US36047,860|05000US36081,860|05000US36005,860|05000US36085 . Getting data like these honestly deserves its own blog post. It took me a very long time to find the proper data. Perhaps I will write about it some other time. . household_path = &#39;MODZCTA/household_type_by_household_size/csvofsame/acs2018_5yr_B11016_86000US11417.csv&#39; household = pd.read_csv(household_path) household.head() . geoid name Total Total, Error Family Households Family Households, Error 2-person household B11016003, Error 3-person household B11016004, Error ... 3-person nonfamily household B11016012, Error 4-person nonfamily household B11016013, Error 5-person nonfamily household B11016014, Error 6-person nonfamily household B11016015, Error 7 or more-person nonfamily household B11016016, Error . 0 86000US10001 | 10001 | 12431 | 521 | 3838 | 413 | 2330 | 338 | 819 | 295 | ... | 253 | 97 | 63 | 55 | 0 | 22 | 0 | 22 | 0 | 22 | . 1 86000US10002 | 10002 | 33540 | 614 | 16565 | 738 | 8090 | 620 | 4287 | 432 | ... | 621 | 157 | 84 | 87 | 0 | 28 | 0 | 28 | 0 | 28 | . 2 86000US10003 | 10003 | 26124 | 703 | 7946 | 551 | 5182 | 472 | 1413 | 316 | ... | 586 | 212 | 94 | 70 | 38 | 38 | 0 | 28 | 0 | 28 | . 3 86000US10004 | 10004 | 1659 | 238 | 709 | 183 | 410 | 160 | 196 | 106 | ... | 42 | 31 | 0 | 12 | 0 | 12 | 0 | 12 | 0 | 12 | . 4 86000US10005 | 10005 | 4374 | 337 | 1614 | 316 | 944 | 260 | 358 | 182 | ... | 199 | 96 | 15 | 25 | 0 | 17 | 0 | 17 | 0 | 17 | . 5 rows × 34 columns . def consolidated_household_per_zip(df): household_data = pd.DataFrame() household_data = household_data.assign( one_person = df[&#39;1-person nonfamily household&#39;], two_person = df[&#39;2-person household&#39;] + df[&#39;2-person nonfamily household&#39;], three_person = df[&#39;3-person household&#39;] + df[&#39;3-person nonfamily household&#39;], four_person = df[&#39;4-person household&#39;] + df[&#39;4-person nonfamily household&#39;], five_person = df[&#39;5-person household&#39;] + df[&#39;5-person nonfamily household&#39;], six_person = df[&#39;6-person household&#39;] + df[&#39;6-person nonfamily household&#39;], seven_or_more_person = df[&#39;7 or more-person household&#39;] + df[&#39;7 or more-person nonfamily household&#39;], modzcta = df[&#39;name&#39;], total = df[&#39;Total&#39;] #avg_people_per_household = df.apply(lambda x: (x[1]+2*x[2]+3*x[3]+4*x[4]+5*x[5]+6*x[6]+7*x[7])/(x[&#39;total&#39;]), axis=1) ) return household_data household_data = consolidated_household_per_zip(household) household_data.head() . one_person two_person three_person four_person five_person six_person seven_or_more_person modzcta total . 0 6710 | 3897 | 1072 | 429 | 307 | 16 | 0 | 10001 | 12431 | . 1 14319 | 10041 | 4908 | 2796 | 1162 | 95 | 219 | 10002 | 33540 | . 2 14377 | 8265 | 1999 | 1367 | 109 | 7 | 0 | 10003 | 26124 | . 3 794 | 524 | 238 | 95 | 8 | 0 | 0 | 10004 | 1659 | . 4 1674 | 1816 | 557 | 314 | 13 | 0 | 0 | 10005 | 4374 | . Now we will merge the household data too to arrive at the final data for plotting - . plot_data = covid_income_zip.merge(household_data, how=&#39;inner&#39;, on=&#39;modzcta&#39;) . Calculating Average people per household is simple enough, jsut divide the population with the number of households - . plot_data[&#39;avg_person_per_household&#39;] = plot_data[&#39;pop_est&#39;]/plot_data[&#39;total&#39;] . Finally let&#39;s plot the data side by side - horizontal concatenatioon using | operator - . income = alt.Chart(plot_data).mark_circle().encode( color=alt.Color(&#39;median_income:Q&#39;, sort = &quot;descending&quot;), x=alt.X(&#39;median_income:Q&#39;), y=&#39;case_per_people:Q&#39; ) household_size = alt.Chart(plot_data).mark_circle().encode( #color=alt.Color(&#39;median_income:Q&#39;, sort = &quot;descending&quot;), x=alt.X(&#39;avg_person_per_household:Q&#39;), y=&#39;case_per_people:Q&#39; ) income|household_size . Let&#39;s beautify it a little and add interactivity so that whenever you select an area in one of the graphs, the corresponding points gets highlighted in the second graph and you can drag the selection around to see that actually that the outbreak is worse in poorer areas where more people live together. . brush = alt.selection_interval(encodings=[&#39;x&#39;]) chart = alt.Chart(plot_data).mark_circle(size=100).encode( color=alt.condition(brush, &#39;BOROUGH_GROUP:N&#39;, alt.value(&#39;lightgray&#39;), legend=alt.Legend(title=&quot;Borough&quot;)), y=alt.Y(&#39;case_per_people:Q&#39;) ).add_selection(brush).properties(width=525, height=480) alt.hconcat( chart.encode( x=alt.X(&#39;median_income:Q&#39;, axis=alt.Axis(tickCount=3, format=&quot;$d&quot;,titleOpacity=0.6, title=&#39;Higher Median Income -&gt;&#39;)), y=alt.Y(&#39;case_per_people:Q&#39;, axis=alt.Axis(format=&quot;%&quot;,titleOpacity=0.6, tickCount=4, title=&quot;Cases in % of population&quot;)) ), chart.encode( x=alt.X(&#39;avg_person_per_household:Q&#39;, scale=alt.Scale(zero=False), axis=alt.Axis(tickCount=3,titleOpacity=0.6, title=&#39;More People per Household -&gt;&#39;)), y=alt.Y(&#39;case_per_people:Q&#39;, axis=alt.Axis(format=&quot;%&quot;, tickCount=4, title=None)) ) ).configure_view(strokeWidth=0).configure_axis(grid=True,gridOpacity=0.6,labelOpacity=0.5) . Way to reproduce the old incorrect graph - . Get the total population data as 1*(1 person households) + .... + 7*(7 or more person households) . Hope you noticed the error over there, but it perfectly captures the old graph as you can see in my old tweet which I had implemented that way - Working on my next set of #dataviz. The regions in NYC with high cases per capita are also regions with the lowest median incomes and largest average household size. Inspired by @nytgraphics and made using #Python and #altair Follow the blog 👉 https://t.co/JfXtHy5hPh pic.twitter.com/QawWEEgb4B . &mdash; Shantam Raj (@RajShantam) July 4, 2020 Gif of the above implementation - . Now let&#39;s make the geospatial chloropleth plot cases per capita like the second chart at the top of the post . Since we have the data in order, - we will use the covid_income_zip data beause it already has the &quot;case_per_people&quot; field that we have to plot - potting is as simple as - . alt.Chart(covid_income_zip).mark_geoshape().encode( color=&#39;case_per_people&#39; ) . Using their color scheme - . def color_it(x): if x&lt;(1/50): return &quot;less than 1/50&quot; elif x&lt;(1/40): return &quot;1/50 to 1/40&quot; elif x&lt;(1/30): return &quot;1/40 to 1/30&quot; else: return &quot;more than 1/30&quot; covid_income_zip = covid_income_zip.assign(share_of_population=covid_income_zip[&#39;case_per_people&#39;].apply(color_it)) alt.Chart(covid_income_zip).mark_geoshape().encode( color=alt.Color(&#39;share_of_population&#39;, scale=alt.Scale(domain=[&#39;less than 1/50&#39;, &#39;1/50 to 1/40&#39;, &#39;1/40 to 1/30&#39;, &#39;more than 1/30&#39;], range=[&#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;])) ).properties(width=600, height=600) . How do we add borough names on top? . I love this part - time for some actual spatial analysis. To show the Borough names we will actually dissolve the areas by borough and get the centroid and finally place our text mark there . borough = covid_income_zip.dissolve(by=&#39;BOROUGH_GROUP&#39;) borough[&#39;lon&#39;] = borough[&#39;geometry&#39;].centroid.x borough[&#39;lat&#39;] = borough[&#39;geometry&#39;].centroid.y borough = borough.reset_index() borough . BOROUGH_GROUP geometry label modzcta pop_est zcta COVID_CASE_COUNT COVID_DEATH_COUNT NEIGHBORHOOD_NAME geoid median_income case_per_people share_of_population lon lat . 0 Bronx | MULTIPOLYGON (((-73.84290 40.82857, -73.84287 ... | 10451 | 10451 | 47798 | 10451 | 1737.0 | 152.0 | Concourse/Melrose | 86000US10451 | 28921.0 | 0.036340 | more than 1/30 | -73.866550 | 40.853688 | . 1 Brooklyn | POLYGON ((-73.93190 40.59469, -73.93193 40.594... | 11201 | 11201 | 62823 | 11201 | 818.0 | 95.0 | Brooklyn Heights/DUMBO/Downtown Brooklyn | 86000US11201 | 124996.0 | 0.013021 | less than 1/50 | -73.949561 | 40.645380 | . 2 Manhattan | MULTIPOLYGON (((-74.00760 40.70299, -74.00765 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | 440.0 | 26.0 | Chelsea/NoMad/West Chelsea | 86000US10001 | 88526.0 | 0.019071 | less than 1/50 | -73.966228 | 40.778413 | . 3 Queens | MULTIPOLYGON (((-73.86496 40.56663, -73.86509 ... | 11004, 11005 | 11004 | 19216 | 11004, 11005, 11040 | 633.0 | 68.0 | Bellerose/Douglaston-Little Neck | 86000US11004 | 92657.0 | 0.032941 | 1/40 to 1/30 | -73.819049 | 40.710385 | . 4 Staten Island | POLYGON ((-74.20999 40.51055, -74.21013 40.510... | 10301 | 10301 | 38733 | 10301 | 1314.0 | 107.0 | Silver Lake/St. George | 86000US10301 | 55802.0 | 0.033925 | more than 1/30 | -74.153793 | 40.581313 | . Let&#39;s plot it and put the legend on top - . a = alt.Chart(covid_income_zip).mark_geoshape().encode( color=alt.Color( &#39;share_of_population&#39;, scale=alt.Scale(domain=[&#39;less than 1/50&#39;, &#39;1/50 to 1/40&#39;, &#39;1/40 to 1/30&#39;, &#39;more than 1/30&#39;], range=[&#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;]), legend=alt.Legend(title=&quot;Share of Population&quot;, orient=&#39;top&#39;, symbolType=&#39;square&#39;, columnPadding=20, symbolSize=200) ), tooltip = [&#39;BOROUGH_GROUP&#39;, &#39;NEIGHBORHOOD_NAME&#39;, &#39;modzcta&#39;, &#39;COVID_CASE_COUNT&#39;, &#39;COVID_DEATH_COUNT&#39;] ).properties(width=600, height=600) b = alt.Chart(borough).mark_text().encode( longitude=&#39;lon&#39;, latitude=&#39;lat&#39;, text=&#39;BOROUGH_GROUP:N&#39; ) a+b .",
            "url": "https://armsp.github.io/covidviz/interactive/geospatial/nyt/2020/08/23/NYC-covid-cases-income-people-per-household.html",
            "relUrl": "/interactive/geospatial/nyt/2020/08/23/NYC-covid-cases-income-people-per-household.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Decrease in GDP of USA",
            "content": "Tis post will show you how to make the GDP chart in the article by titled Big Tech Earnings Surge as Economy Slumps . . Nothing that it&#39;s a percent change from previous quarter data, we will extract the data for this chart as follows from Bureau of Economic Analysis - . Go here - https://apps.bea.gov/iTable/index_nipa.cfm | Click on &quot;Begin using this data&quot; | Click on Section 1 | Click on Table 1.1.1 - Percent Change From Preceding Period in Real Gross Domestic Product - Annualized | Click on Modify | Select on all years | Repeat the steps for Table 1.1.3 - Real Gross Domestic Product, Quantity Indexes | . There are however some important things to know before going further which will save you a lot of time and give you a better understanding of what you are doing, which I did not understand the first time and had to dig up a lot to figure. Read this article and then this. . Now you know that GDP is reported in a rather peculiar way - annulazied GDP. It mean&#39;s they report values of GDP such that if the current state of affairs continue then what would happen by the end of the year . The formula for annunalizing is - . $ g_{m} = left[ left( frac{X_{m}}{X_{m-1}} right)^n -1 right] cdot100 $ . where n is 4 for quarterly available data and 12 for monthly data . import pandas as pd import altair as alt from functools import wraps import datetime as dt alt.renderers.set_embed_options(actions=False) def log_step(func): @wraps(func) def wrapper(*args, **kwargs): &quot;&quot;&quot;timing or logging etc&quot;&quot;&quot; start = dt.datetime.now() output = func(*args, **kwargs) end = dt.datetime.now() print(f&quot;After function {func.__name__} ran, shape of dataframe is - {output.shape}, execution time is - {end-start}&quot;) return output return wrapper @log_step def read_transpose_gdp_data(path, if_uri): if if_uri: pass else: cols = pd.read_csv(path, skiprows=4, header=None, nrows=1) us_gdp = pd.read_csv(path, skiprows=4, header=None, usecols=[i for i in cols if i != 0]) us_gdp = us_gdp.T us_gdp.iloc[0,0] = &#39;year&#39; us_gdp.iloc[0,1] = &#39;quarter&#39; us_gdp.columns = us_gdp.iloc[0] us_gdp = us_gdp[1:] return us_gdp @log_step def clean_gdp_data(df): df.columns = [x.strip() for x in df.columns] #df.columns = [x.strip() if type(x) != float else x for x in list(df.columns)] df[&#39;Gross domestic product&#39;] = df[&#39;Gross domestic product&#39;].astype(float) df[&#39;year&#39;] = df[&#39;year&#39;].astype(int) df = df.reset_index(drop=True) #df.rename(columns={&#39;Gross domestic product&#39;: &#39;gdp&#39;}, inplace=True) #df[&#39;year&#39;] = pd.to_datetime(df[&#39;year&#39;], format=&quot;%Y&quot;) df.drop(df.columns[3:], inplace=True, axis=1) return df @log_step def rename_cols(df, col_dict): df.rename(columns=col_dict, inplace=True) return df @log_step def assign_columns(df): df = df.assign(non_annualized_gdp = df[&#39;real_gdp&#39;].diff()/df[&#39;real_gdp&#39;].shift(1)*100) return df @log_step def reshape_concat_column(df, col): df = df[1:].reset_index() df = df.assign(annualized_gdp = col) return df def year_as_time(df): df[&#39;year&#39;] = pd.to_datetime(df[&#39;year&#39;], format=&quot;%Y&quot;) return df . Annualized GDP . ann_gdp = (read_transpose_gdp_data(path=&#39;usa_gdp.csv&#39;, if_uri=False) .pipe(clean_gdp_data) .pipe(rename_cols, col_dict={&#39;Gross domestic product&#39;: &#39;ann_gdp&#39;}) ) ann_gdp.head() . After function read_transpose_gdp_data ran, shape of dataframe is - (293, 30), execution time is - 0:00:00.080267 After function clean_gdp_data ran, shape of dataframe is - (293, 3), execution time is - 0:00:00.004031 After function rename_cols ran, shape of dataframe is - (293, 3), execution time is - 0:00:00.000345 . year quarter ann_gdp . 0 1947 | Q2 | -1.0 | . 1 1947 | Q3 | -0.8 | . 2 1947 | Q4 | 6.4 | . 3 1948 | Q1 | 6.2 | . 4 1948 | Q2 | 6.8 | . When we plot this we will find that it looks very much like the graph in the article. We just have to play around the facet spacings to make it look continuous like a single bar chart instead of a faceted chart . alt.Chart(ann_gdp).mark_bar(width=5).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;ann_gdp:Q&#39;, facet=alt.Facet(&#39;year:O&#39;, bounds=&#39;flush&#39;, spacing={&#39;column&#39;:0}, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.ann_gdp &gt; 0, alt.value(&#39;green&#39;), alt.value(&#39;red&#39;)) ).configure_axis(grid=False).configure_view(strokeWidth=1, step=5) . Real GDP . The chart you see above is the annualized GDP which suggests that the US economy will shrink by a third if things stay exactly like the way they are now. Which is certainly not representative of current times. Fortunately the BEA also provides the Real GDP as Quantity Index units. If you apply the formaula to that data you get the data above i.e Table 1.1.1. Real GDP is a transformed version of nominal GDP Let&#39;s chart Table . Nominal GDP is reported as billions or trillions of dollars . re_gdp = (read_transpose_gdp_data(path=&#39;us_gdp.csv&#39;, if_uri=False) .pipe(clean_gdp_data) .pipe(rename_cols, col_dict={&#39;Gross domestic product&#39;: &#39;real_gdp&#39;})) re_gdp.head() . After function read_transpose_gdp_data ran, shape of dataframe is - (294, 28), execution time is - 0:00:00.066090 After function clean_gdp_data ran, shape of dataframe is - (294, 3), execution time is - 0:00:00.004825 After function rename_cols ran, shape of dataframe is - (294, 3), execution time is - 0:00:00.000353 . year quarter real_gdp . 0 1947 | Q1 | 12.552 | . 1 1947 | Q2 | 12.519 | . 2 1947 | Q3 | 12.493 | . 3 1947 | Q4 | 12.688 | . 4 1948 | Q1 | 12.879 | . Plotting this we will see how GDP has increased over the years - . alt.Chart(re_gdp).mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;real_gdp:Q&#39;, column = alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)) ).configure_view(strokeWidth=0, step=4) . Let&#39;s calculate non-annualized gdp from the real gdp - . usa_gdp = re_gdp.pipe(assign_columns) usa_gdp.head() . After function assign_columns ran, shape of dataframe is - (294, 4), execution time is - 0:00:00.003361 . year quarter real_gdp non_annualized_gdp . 0 1947 | Q1 | 12.552 | NaN | . 1 1947 | Q2 | 12.519 | -0.262906 | . 2 1947 | Q3 | 12.493 | -0.207684 | . 3 1947 | Q4 | 12.688 | 1.560874 | . 4 1948 | Q1 | 12.879 | 1.505359 | . Highlighting the quarters where we had negative growth compared to previous quarter (using non-annualized gdp) - . alt.Chart(usa_gdp).mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;real_gdp:Q&#39;, column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.non_annualized_gdp &gt; 0, alt.value(&#39;#76a4a5&#39;), alt.value(&#39;#d0573a&#39;)) ).configure_axis(grid=True).configure_view(strokeWidth=0, step=4) . Finally let&#39;s plot the non-annualized GDP. We will see that it&#39;s no longer close to -30 but to -10, just like the chart in the article. . alt.Chart(usa_gdp).mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;non_annualized_gdp:Q&#39;, column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.non_annualized_gdp &gt; 0, alt.value(&#39;#76a4a5&#39;), alt.value(&#39;#d0573a&#39;)) ).configure_axis(grid=False).configure_view(strokeWidth=0, step=4) . Let&#39;s add annualized gdp data to real and non-annualized gdp data to that we may plot them together for a bigger picture . usa_gdp = (usa_gdp .pipe(reshape_concat_column, ann_gdp[&#39;ann_gdp&#39;]) # because Annualized GDP and Real GDP have different shapes .pipe(year_as_time)) usa_gdp.head() . After function reshape_concat_column ran, shape of dataframe is - (293, 6), execution time is - 0:00:00.003945 . index year quarter real_gdp non_annualized_gdp annualized_gdp . 0 1 | 1947-01-01 | Q2 | 12.519 | -0.262906 | -1.0 | . 1 2 | 1947-01-01 | Q3 | 12.493 | -0.207684 | -0.8 | . 2 3 | 1947-01-01 | Q4 | 12.688 | 1.560874 | 6.4 | . 3 4 | 1948-01-01 | Q1 | 12.879 | 1.505359 | 6.2 | . 4 5 | 1948-01-01 | Q2 | 13.092 | 1.653855 | 6.8 | . To see the contrast b/w the two to understand how misleading annualized gdp can be we will layer them on top of each other - . a = alt.Chart().mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, domain=False, ticks=False)), y=&#39;non_annualized_gdp:Q&#39;, #column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.non_annualized_gdp &gt; 0, alt.value(&#39;blue&#39;), alt.value(&#39;maroon&#39;)), #text = &#39;num:Q&#39; ) b = alt.Chart().mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;annualized_gdp:Q&#39;, #column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.annualized_gdp &gt; 0, alt.value(&#39;orange&#39;), alt.value(&#39;violet&#39;)), ) n_ann_gdp = alt.Chart().transform_filter({&#39;field&#39;: &#39;year&#39;, &#39;oneOf&#39;: [2008, 2020], &#39;timeUnit&#39;: &#39;year&#39;}).mark_text(color=&#39;maroon&#39;, dx=-12, dy=7, fontSize=12).encode( x=alt.X(&#39;quarter:O&#39;, title=None, aggregate={&#39;argmin&#39;: &#39;non_annualized_gdp&#39;}),# axis=alt.Axis(labels=False, domain=False, ticks=False)), y=&#39;min(non_annualized_gdp):Q&#39;, text=alt.Text(&#39;min(non_annualized_gdp):Q&#39;, format=&#39;.2&#39;, ) #x=alt.X(&#39;quarter&#39;, aggregate={&#39;argmin&#39;: &#39;non_annualized_gdp&#39;}) ) ann_gdp = alt.Chart().transform_filter(alt.FieldOneOfPredicate(field=&#39;year&#39;, oneOf=[2008, 2020], timeUnit=&#39;year&#39;)).mark_text(color=&#39;violet&#39;, dx=-12, dy=0, fontSize=12).encode( x=alt.X(&#39;quarter:O&#39;, title=None, aggregate={&#39;argmin&#39;: &#39;annualized_gdp&#39;}),# axis=alt.Axis(labels=False, domain=False, ticks=False)), y=alt.Y(&#39;min(annualized_gdp):Q&#39;, title=&#39;Annualized GDP, Real GDP&#39;), text=alt.Text(&#39;min(annualized_gdp):Q&#39;, format=&#39;.2&#39;, ) #x=alt.X(&#39;quarter&#39;, aggregate={&#39;argmin&#39;: &#39;non_annualized_gdp&#39;}) ) alt.layer(b, a, n_ann_gdp, ann_gdp, data=usa_gdp).facet( column=alt.Facet(&#39;year:T&#39;, header=alt.Header(labels=True, labelColor=&#39;grey&#39;, labelOrient=&#39;bottom&#39;, format=&quot;%y&quot;, formatType=&#39;time&#39;, title=None)), spacing=0, bounds=&#39;flush&#39; ).configure_axis(domain=False, labelColor=&#39;grey&#39;, tickColor=&#39;lightgrey&#39;, domainColor=&#39;lightgrey&#39;, titleColor=&#39;grey&#39; ).configure_view(strokeWidth=0, step=4).configure_axisY(grid=True,) . We can see clearly that the damage to the economy is greater than the 2008 recession .",
            "url": "https://armsp.github.io/covidviz/facet/nyt/gdp/2020/08/17/GDP.html",
            "relUrl": "/facet/nyt/gdp/2020/08/17/GDP.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "US's failure to control the virus",
            "content": "Today we will make the first chart from the article The unique US failure to control the virus . . . . import pandas as pd import altair as alt from functools import wraps import datetime as dt alt.renderers.set_embed_options(actions=False) population_uri = &#39;https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv?raw=true&#39; deaths_ts_uri = &#39;https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv?raw=true&#39; gdp_current_us_dollars = &#39;https://gist.githubusercontent.com/armsp/58b43f28b4bf880f3874db80630dec44/raw/959a34a1797b0e3fdc860a6ef0057c62ee898dd7/gdp.csv&#39; . deaths_ts = pd.read_csv(deaths_ts_uri) deaths_ts.head() . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 10/16/20 10/17/20 10/18/20 10/19/20 10/20/20 10/21/20 10/22/20 10/23/20 10/24/20 10/25/20 . 0 NaN | Afghanistan | 33.93911 | 67.709953 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1485 | 1488 | 1492 | 1497 | 1499 | 1501 | 1505 | 1507 | 1511 | 1514 | . 1 NaN | Albania | 41.15330 | 20.168300 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 443 | 448 | 451 | 454 | 458 | 462 | 465 | 469 | 473 | 477 | . 2 NaN | Algeria | 28.03390 | 1.659600 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1841 | 1846 | 1856 | 1865 | 1873 | 1880 | 1888 | 1897 | 1907 | 1914 | . 3 NaN | Andorra | 42.50630 | 1.521800 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 59 | 59 | 59 | 62 | 62 | 63 | 63 | 69 | 69 | 69 | . 4 NaN | Angola | -11.20270 | 17.873900 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 234 | 241 | 247 | 248 | 251 | 255 | 260 | 265 | 267 | 268 | . 5 rows × 282 columns . gdp_us = pd.read_csv(gdp_current_us_dollars) gdp_us.head() . Series Name Series Code Country Name Country Code 2019 [YR2019] . 0 GDP per capita (current US$) | NY.GDP.PCAP.CD | Afghanistan | AFG | 502.115486913067 | . 1 GDP per capita (current US$) | NY.GDP.PCAP.CD | Albania | ALB | 5352.85741103671 | . 2 GDP per capita (current US$) | NY.GDP.PCAP.CD | Algeria | DZA | 3948.34327892571 | . 3 GDP per capita (current US$) | NY.GDP.PCAP.CD | American Samoa | ASM | .. | . 4 GDP per capita (current US$) | NY.GDP.PCAP.CD | Andorra | AND | 40886.3911648431 | . population = pd.read_csv(population_uri) population.head() . UID iso2 iso3 code3 FIPS Admin2 Province_State Country_Region Lat Long_ Combined_Key Population . 0 4 | AF | AFG | 4.0 | NaN | NaN | NaN | Afghanistan | 33.93911 | 67.709953 | Afghanistan | 38928341.0 | . 1 8 | AL | ALB | 8.0 | NaN | NaN | NaN | Albania | 41.15330 | 20.168300 | Albania | 2877800.0 | . 2 12 | DZ | DZA | 12.0 | NaN | NaN | NaN | Algeria | 28.03390 | 1.659600 | Algeria | 43851043.0 | . 3 20 | AD | AND | 20.0 | NaN | NaN | NaN | Andorra | 42.50630 | 1.521800 | Andorra | 77265.0 | . 4 24 | AO | AGO | 24.0 | NaN | NaN | NaN | Angola | -11.20270 | 17.873900 | Angola | 32866268.0 | . def log_step(func): @wraps(func) def wrapper(dataf, *args, **kwargs): &quot;&quot;&quot;timing or logging etc&quot;&quot;&quot; start = dt.datetime.now() output = func(dataf, *args, **kwargs) end = dt.datetime.now() print(f&quot;After function {func.__name__} ran, shape of dataframe is - {output.shape}, execution time is - {end-start}&quot;) return output return wrapper @log_step def start_pipeline(dataf): return dataf.copy() @log_step def remove_cols(dataf, *arg, **kwargs): #print(list(arg)) result = dataf.drop(columns=list(arg)) return result @log_step def remove_null(dataf): return dataf.dropna() def rename_cols(dataf, *arg, **kwargs): &quot;&quot;&quot;Rename column names of raw dataframes to something digestable and that looks better in visualization and does not have spaces in between cause altair does not like that&quot;&quot;&quot; result = dataf.rename(columns=kwargs) return result @log_step def filter_rows(dataf, which, **kwargs): if which == &#39;gdp&#39;: result = dataf[dataf[&#39;current_us&#39;] != &#39;..&#39;] return result elif which == &#39;pop&#39;: result = dataf[pd.isnull(dataf[&#39;Province_State&#39;])] return result def set_dtypes(dataf): &quot;&quot;&quot;set the datatypes of columns&quot;&quot;&quot; # can use data.assign(col = lambda d: pd.to_datetime(d[&#39;col&#39;])) or col = pd.to_datetime(d[&#39;col&#39;]) dataf[&#39;current_us&#39;] = dataf[&#39;current_us&#39;].astype(float) return dataf # def remove_outliers(dataf): # &quot;&quot;&quot;remove outliers&quot;&quot;&quot; # return dataf # def add_features(dataf): # return dataf @log_step def clean(dataf): agg_deaths = dataf.groupby(&#39;Country/Region&#39;).sum().reset_index() agg_deaths = agg_deaths[agg_deaths[&#39;Country/Region&#39;].isin(pop_w_gdp[&#39;Country/Region&#39;])].set_index(&#39;Country/Region&#39;) result = agg_deaths.T.reset_index().rename_axis(None, axis=1).rename(columns={&#39;index&#39;: &#39;Date&#39;}) result[&#39;Date&#39;] = pd.to_datetime(result[&#39;Date&#39;], format=&quot;%m/%d/%y&quot;) result = result[result[&#39;Date&#39;] &lt; &#39;8/5/20&#39;] #convert cumulative deaths to daily deaths per million for col in result: if col != &#39;Date&#39;: result[col] = result[col].diff() result[col] = (result[col]/int(countries_population[countries_population[&#39;Country/Region&#39;] == col][&#39;Population&#39;]))*1000000 return result . gdp = (gdp_us .pipe(start_pipeline) .pipe(remove_null) .pipe(remove_cols, *[&#39;Series Name&#39;, &#39;Series Code&#39;]) .pipe(rename_cols, **{&#39;2019 [YR2019]&#39;: &#39;current_us&#39;}) .pipe(filter_rows, &#39;gdp&#39;) .pipe(set_dtypes)) countries_population = (population .pipe(start_pipeline) .pipe(filter_rows, &#39;pop&#39;) .pipe(remove_cols, *[&#39;UID&#39;, &#39;iso2&#39;, &#39;code3&#39;, &#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Lat&#39;, &#39;Long_&#39;, &#39;Combined_Key&#39;]) .pipe(rename_cols, **{&#39;iso3&#39;: &#39;Country Code&#39;, &#39;Country_Region&#39;:&#39;Country/Region&#39;})) # Combining population with GDP pop_w_gdp = countries_population.merge(gdp, how=&#39;inner&#39;, on=&#39;Country Code&#39;) # Filter for only wealthy countries i.e GDP &gt; 25000 USD and population &gt; 10 million pop_w_gdp = pop_w_gdp[(pop_w_gdp[&#39;current_us&#39;] &gt; 25000) &amp; (pop_w_gdp[&#39;Population&#39;] &gt; 10000000)] # Making daily deaths per million data for plotting plot_data = (deaths_ts .pipe(start_pipeline) .pipe(remove_cols, *[&#39;Province/State&#39;, &#39;Lat&#39;, &#39;Long&#39;]) .pipe(clean) .pipe(remove_null) ) plot_data.head() . After function start_pipeline ran, shape of dataframe is - (269, 5), execution time is - 0:00:00.000376 After function remove_null ran, shape of dataframe is - (264, 5), execution time is - 0:00:00.182202 After function remove_cols ran, shape of dataframe is - (264, 3), execution time is - 0:00:00.073348 After function filter_rows ran, shape of dataframe is - (222, 3), execution time is - 0:00:00.005289 After function start_pipeline ran, shape of dataframe is - (4154, 12), execution time is - 0:00:00.006278 After function filter_rows ran, shape of dataframe is - (189, 12), execution time is - 0:00:00.012600 After function remove_cols ran, shape of dataframe is - (189, 3), execution time is - 0:00:00.005748 After function start_pipeline ran, shape of dataframe is - (267, 282), execution time is - 0:00:00.000790 After function remove_cols ran, shape of dataframe is - (267, 279), execution time is - 0:00:00.003718 After function clean ran, shape of dataframe is - (196, 14), execution time is - 0:00:00.212112 After function remove_null ran, shape of dataframe is - (195, 14), execution time is - 0:00:00.004697 . Date Australia Belgium Canada France Germany Italy Japan Korea, South Netherlands Spain Sweden US United Kingdom . 1 2020-01-23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 2020-01-24 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 2020-01-25 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 2020-01-26 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 2020-01-27 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . This is an interactive plot. Hover over the plot to see lines getting highlighted. .",
            "url": "https://armsp.github.io/covidviz/interactive/nyt/2020/08/10/US_failed.html",
            "relUrl": "/interactive/nyt/2020/08/10/US_failed.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Changes in spending",
            "content": "Today we will study the charts in the article The Rich Cut Their Spending. That Has Hurt All the Workers Who Count on It. These charts tell us something very important about how the spending has been cut differently across different classes. . . . Note: The vertical lines correspond to the following dates - . First stimulus checks - April 17 | States in the process of reopening - May 1 | . The data for this analysis is taken from Opportunity Labs where they publish their data in this dashboard. . What&#39;s important about this data is best summed up by - . One of the things this crisis has made salient is how interdependent our health was, said Michael Stepner, an economist at the University of Toronto. We’re seeing the mirror of that on the economic side. . #hide_output import pandas as pd import altair as alt alt.renderers.set_embed_options(actions=False) . Drop in consumer spending . The rich drive more of the economy than they did 50 years ago. And more workers depend on them. . For the highest-income quarter, spending has recovered much more slowly, after falling by 36 percent at the lowest point. . . Important: We will use data till July only, so the uri used for the data is for the commit of a particular day. If you want to use the latest data then replace the spending_uri with this - &#8217;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/Affinity%20-%20National%20-%20Daily.csv&amp;#8217; . spending_uri = &#39;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/8d9fae46fab3e386a8f4ce798de09a016cbda0f9/data/Affinity%20-%20National%20-%20Daily.csv&#39; #spending_uri = &#39;https://raw.githubusercontent.com/Opportunitylab/EconomicTracker/main/data/Affinity%20-%20National%20-%20Weekly.csv&#39; # for latest data spending = pd.read_csv(spending_uri) spending.head() . year month day spend_acf spend_aer spend_all spend_all_inchigh spend_all_inclow spend_all_incmiddle spend_apg spend_grf spend_hcs spend_tws . 0 2020 | 1 | 24 | -0.00510 | -0.02360 | -0.006440 | -0.005790 | -0.00752 | -0.00654 | -0.00952 | -0.00954 | -0.00328 | -0.005840 | . 1 2020 | 1 | 25 | 0.00202 | -0.01820 | 0.000432 | -0.000625 | -0.00201 | 0.00199 | 0.00400 | 0.00991 | -0.00469 | 0.000839 | . 2 2020 | 1 | 26 | -0.00896 | -0.02220 | -0.002710 | -0.000425 | -0.00668 | -0.00315 | 0.00152 | 0.01920 | -0.00647 | 0.002720 | . 3 2020 | 1 | 27 | -0.01350 | -0.00762 | -0.012200 | -0.011000 | -0.01590 | -0.01190 | -0.00671 | -0.00980 | -0.00755 | -0.015700 | . 4 2020 | 1 | 28 | -0.01550 | -0.01270 | -0.013700 | -0.013300 | -0.01630 | -0.01320 | -0.00492 | -0.01820 | -0.00243 | -0.009870 | . def add_format_date(df): df[&#39;date&#39;] = df[&#39;year&#39;].astype(str) + &#39;-&#39; + df[&#39;month&#39;].astype(str) + &#39;-&#39; + df[&#39;day&#39;].astype(str) df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], format=&quot;%Y-%m-%d&quot;) return df . spending = spending.pipe(add_format_date) spending.head() . year month day spend_acf spend_aer spend_all spend_all_inchigh spend_all_inclow spend_all_incmiddle spend_apg spend_grf spend_hcs spend_tws date . 0 2020 | 1 | 24 | -0.00510 | -0.02360 | -0.006440 | -0.005790 | -0.00752 | -0.00654 | -0.00952 | -0.00954 | -0.00328 | -0.005840 | 2020-01-24 | . 1 2020 | 1 | 25 | 0.00202 | -0.01820 | 0.000432 | -0.000625 | -0.00201 | 0.00199 | 0.00400 | 0.00991 | -0.00469 | 0.000839 | 2020-01-25 | . 2 2020 | 1 | 26 | -0.00896 | -0.02220 | -0.002710 | -0.000425 | -0.00668 | -0.00315 | 0.00152 | 0.01920 | -0.00647 | 0.002720 | 2020-01-26 | . 3 2020 | 1 | 27 | -0.01350 | -0.00762 | -0.012200 | -0.011000 | -0.01590 | -0.01190 | -0.00671 | -0.00980 | -0.00755 | -0.015700 | 2020-01-27 | . 4 2020 | 1 | 28 | -0.01550 | -0.01270 | -0.013700 | -0.013300 | -0.01630 | -0.01320 | -0.00492 | -0.01820 | -0.00243 | -0.009870 | 2020-01-28 | . Plotting the data - . base=alt.Chart(spending).transform_fold([&#39;spend_all_inchigh&#39;, &#39;spend_all_inclow&#39;, &#39;spend_all_incmiddle&#39;]).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-02-14&#39;)).mark_line().encode( x=alt.X(&#39;date:T&#39;, title=None, axis=alt.Axis(format=&quot;%b%e&quot;, tickCount=5, labelOffset=0, tickOffset=0, labelPadding=25, ticks=False)), #y=&#39;spend_all_inchigh:Q&#39;, #x2=&#39;date:Q&#39;, y=alt.Y(&#39;value:Q&#39;, title=None, axis=alt.Axis(format=&quot;%&quot;, tickCount=10)), color=&#39;key:N&#39; #detail=&#39;date&#39; ).properties(width=900, height=600) lines={&#39;lines&#39;: [&#39;2020-04-15&#39;, &#39;2020-05-01&#39;], &#39;y1&#39;: [0,0], &#39;y2&#39;: [-0.4, -0.4]} lines1={&#39;lines&#39;: [&#39;2020-04-15&#39;], &#39;text&#39;: [&#39;First stimulus n checks received&#39;], &#39;y&#39;: [-0.03]} lines2={&#39;lines&#39;: [&#39;2020-05-01&#39;], &#39;text&#39;: [&#39;Half of states in n process of reopening&#39;], &#39;y&#39;: [-0.03]} vert_line = alt.Chart(pd.DataFrame(lines)).mark_rule(strokeDash=[5,5], stroke=&#39;grey&#39;).encode( x=&#39;lines:T&#39;, y=alt.Y(&#39;y1:Q&#39;, scale=alt.Scale(zero=False)), #y2=alt.Y2(&#39;y2:Q&#39;) ) text1 = alt.Chart(pd.DataFrame(lines1)).mark_text(lineBreak=&#39; n&#39;, dx=-10, align=&#39;right&#39;).encode( text = &#39;text:N&#39;, y = &#39;y:Q&#39;, x = &#39;lines:T&#39; ) text2 = alt.Chart(pd.DataFrame(lines2)).mark_text(lineBreak=&#39; n&#39;,dx=10, align=&#39;left&#39;).encode( text = &#39;text:N&#39;, y = &#39;y:Q&#39;, x = &#39;lines:T&#39;, ) alt.layer(base, vert_line, text1, text2).configure_view(strokeWidth=0).configure_axis(grid=False).configure_axisX(orient=&#39;top&#39;, offset=-67) . We can use the same techniques for the vertical lines and text overlay as the chart above in the following charts. Since the idea is similar I will not implement them for all, instead just plot the line charts. . Small businesses in the richest neighborhoods have had the biggest drops in revenue . . Note: Latest data from now on . revenue_uri = &#39;https://raw.githubusercontent.com/Opportunitylab/EconomicTracker/main/data/Womply%20Revenue%20-%20National%20-%20Daily.csv&#39; revenue = pd.read_csv(revenue_uri) revenue = revenue.pipe(add_format_date) revenue.head() . year month day revenue_all revenue_inchigh revenue_inclow revenue_incmiddle revenue_ss40 revenue_ss65 revenue_ss70 date . 0 2020 | 1 | 10 | -0.01170 | -0.00490 | -0.0274 | -0.00860 | -0.00999 | -0.03060 | -0.0162 | 2020-01-10 | . 1 2020 | 1 | 11 | -0.00348 | 0.00729 | -0.0222 | -0.00302 | -0.00692 | -0.00958 | -0.0138 | 2020-01-11 | . 2 2020 | 1 | 12 | 0.00195 | 0.00998 | -0.0175 | 0.00372 | -0.00488 | 0.00284 | -0.0117 | 2020-01-12 | . 3 2020 | 1 | 13 | -0.01350 | -0.00551 | -0.0481 | -0.00665 | -0.00459 | -0.04560 | -0.0217 | 2020-01-13 | . 4 2020 | 1 | 14 | -0.00138 | 0.00071 | -0.0231 | 0.00392 | -0.00174 | 0.00809 | -0.0168 | 2020-01-14 | . alt.Chart(revenue).mark_line().transform_fold([&#39;revenue_inclow&#39;, &#39;revenue_incmiddle&#39;, &#39;revenue_inchigh&#39;]).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-02-14&#39;)).encode( x=&#39;date:T&#39;, y= &#39;value:Q&#39;, color= &#39;key:N&#39; ) . Low-wage workers in the richest neighborhoods have had the biggest drop in earnings . . Important: The file for this data has been removed and not updated since July. So we will use the data from the particular commit that had this file. . earning_uri = &#39;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/5f914ee4e71f56a33857b63e0bd07d71bc31e847/data/Low%20Inc%20Earnings%20Small%20Businesses%20-%20National%20-%20Daily.csv&#39; earning = pd.read_csv(earning_uri) earning = earning.pipe(add_format_date) earning.head() . year month day pay pay31_33 pay44_45 pay48_49 pay62 pay72 pay_inclow pay_incmiddle pay_inchigh date . 0 2020 | 1 | 1 | . | . | . | . | . | . | . | . | . | 2020-01-01 | . 1 2020 | 1 | 2 | . | . | . | . | . | . | . | . | . | 2020-01-02 | . 2 2020 | 1 | 3 | . | . | . | . | . | . | . | . | . | 2020-01-03 | . 3 2020 | 1 | 4 | . | . | . | . | . | . | . | . | . | 2020-01-04 | . 4 2020 | 1 | 5 | . | . | . | . | . | . | . | . | . | 2020-01-05 | . alt.Chart(earning).mark_line().transform_fold([&#39;pay&#39;, &#39;pay_inclow&#39;, &#39;pay_incmiddle&#39;, &#39;pay_inchigh&#39;]).encode( x=&#39;date:T&#39;, y= &#39;value:Q&#39;, color= &#39;key:N&#39; ) . Low-wage workers in the richest neighborhoods have had the biggest drop in employment . . Important: This file was also eventually removed. So we will use the file from the commit that still had this file- . #employment_uri = &#39;https://raw.githubusercontent.com/Opportunitylab/EconomicTracker/main/data/Low%20Inc%20Emp%20Small%20Businesses%20-%20National%20-%20Daily.csv&#39; # original file employment_uri = &#39;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/ba8c0096efb873d90f10cd720576c4ec5e6fc42e/data/Low%20Inc%20Emp%20Small%20Businesses%20-%20National%20-%20Daily.csv&#39; employment = pd.read_csv(employment_uri) employment = employment.pipe(add_format_date) employment.head() . year month day emp emp31_33 emp44_45 emp48_49 emp62 emp72 emp_inclow emp_incmiddle emp_inchigh date . 0 2020 | 1 | 1 | . | . | . | . | . | . | . | . | . | 2020-01-01 | . 1 2020 | 1 | 2 | . | . | . | . | . | . | . | . | . | 2020-01-02 | . 2 2020 | 1 | 3 | . | . | . | . | . | . | . | . | . | 2020-01-03 | . 3 2020 | 1 | 4 | . | . | . | . | . | . | . | . | . | 2020-01-04 | . 4 2020 | 1 | 5 | . | . | . | . | . | . | . | . | . | 2020-01-05 | . alt.Chart(employment).mark_line().transform_fold([&#39;emp_inclow&#39;, &#39;emp_incmiddle&#39;, &#39;emp_inchigh&#39;]).encode( x=&#39;date:T&#39;, y= &#39;value:Q&#39;, color= &#39;key:N&#39; ) .",
            "url": "https://armsp.github.io/covidviz/spending/nyt/2020/07/15/Spending.html",
            "relUrl": "/spending/nyt/2020/07/15/Spending.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Latest epicenters of infection and death toll in the US",
            "content": "Today we will make a very special graph that looks like the article See How the Coronavirus Death Toll Grew Across the U.S. . . This will require us to use some special techniques using SVG Path elements. . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=True) # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf.head() . date county state fips cases deaths . 0 2020-01-21 | Snohomish | Washington | 53061.0 | 1 | 0 | . 1 2020-01-22 | Snohomish | Washington | 53061.0 | 1 | 0 | . 2 2020-01-23 | Snohomish | Washington | 53061.0 | 1 | 0 | . 3 2020-01-24 | Cook | Illinois | 17031.0 | 1 | 0 | . 4 2020-01-24 | Snohomish | Washington | 53061.0 | 1 | 0 | . # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in county data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . NYT publishes the data for New York City in a different way. So we will add custom FIPS for New York City and Puerto Rico too whose county level information is not present. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;].head() . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . cdf.loc[cdf[&#39;state&#39;] == &#39;Puerto Rico&#39;, &#39;fips&#39;] = 2 cdf[cdf[&#39;state&#39;] == &#39;Puerto Rico&#39;].head() . date county state fips cases deaths . 1858 2020-03-13 | Unknown | Puerto Rico | 2.0 | 3 | 0 | . 2220 2020-03-14 | Unknown | Puerto Rico | 2.0 | 4 | 0 | . 2642 2020-03-15 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . 3107 2020-03-16 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . 3637 2020-03-17 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . Extracting the latest cases and deaths - . aggregate = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;first&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) aggregate.head() . fips county date state cases deaths . 0 1.0 | New York City | 2020-07-04 | New York | 221395 | 22630 | . 1 2.0 | Unknown | 2020-07-04 | Puerto Rico | 7787 | 155 | . 2 1001.0 | Autauga | 2020-07-04 | Alabama | 591 | 13 | . 3 1003.0 | Baldwin | 2020-07-04 | Alabama | 863 | 10 | . 4 1005.0 | Barbour | 2020-07-04 | Alabama | 350 | 2 | . Combining the 5 boroughs - New York, Kings, Queens, Bronx and Richmond - into one and adding that spatial area in the geodatatrame . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . county = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county[&#39;fips&#39;] = county[&#39;GEOID&#39;] county[&#39;fips&#39;] = county[&#39;fips&#39;].astype(&#39;int&#39;) county.head() . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 0 29 | 227 | 00758566 | 0500000US29227 | 29227 | Worth | 06 | 690564983 | 493903 | POLYGON ((-94.63203 40.57176, -94.53388 40.570... | -94.423288 | 40.479456 | 29227 | . 1 31 | 061 | 00835852 | 0500000US31061 | 31061 | Franklin | 06 | 1491355860 | 487899 | POLYGON ((-99.17940 40.35068, -98.72683 40.350... | -98.952991 | 40.176363 | 31061 | . 2 36 | 013 | 00974105 | 0500000US36013 | 36013 | Chautauqua | 06 | 2746047476 | 1139407865 | POLYGON ((-79.76195 42.26986, -79.62748 42.324... | -79.366918 | 42.227692 | 36013 | . 3 37 | 181 | 01008591 | 0500000US37181 | 37181 | Vance | 06 | 653713542 | 42178610 | POLYGON ((-78.49773 36.51467, -78.45728 36.541... | -78.406712 | 36.368814 | 37181 | . 4 47 | 183 | 01639799 | 0500000US47183 | 47183 | Weakley | 06 | 1503107848 | 3707114 | POLYGON ((-88.94916 36.41010, -88.81642 36.410... | -88.719909 | 36.298962 | 47183 | . We will actually work with Metropolitan Statistical Areas instead of counties, so we need more work to do. We have the MSA Shapefile as well as a dataset that has the counties that combine to form MSAs from the US Census . msa = pd.read_csv(&#39;core_msa_list.csv&#39;, sep=&quot;;&quot;) msa_shp = gpd.read_file(&#39;shapes/cb_2019_us_cbsa_500k/cb_2019_us_cbsa_500k.shp&#39;) #msa[msa[&#39;CBSA Title&#39;].str.startswith(&#39;New York&#39;)] . msa.head() . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County . 0 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 13 | Central | . 1 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 45 | Outlying | . 2 10140 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 27 | Central | . 3 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 59 | Outlying | . 4 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | . msa_shp.head() . CSAFP CBSAFP AFFGEOID GEOID NAME LSAD ALAND AWATER geometry . 0 425 | 37620 | 310M500US37620 | 37620 | Parkersburg-Vienna, WV | M1 | 1551452495 | 32408833 | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | . 1 None | 45980 | 310M500US45980 | 45980 | Troy, AL | M2 | 1740647520 | 2336975 | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | . 2 548 | 49020 | 310M500US49020 | 49020 | Winchester, VA-WV | M1 | 2752545068 | 16892497 | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | . 3 142 | 45180 | 310M500US45180 | 45180 | Talladega-Sylacauga, AL | M2 | 1908293036 | 60927931 | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | . 4 None | 25060 | 310M500US25060 | 25060 | Gulfport-Biloxi, MS | M1 | 5739122781 | 2105780374 | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | . msa[&#39;FIPS State Code&#39;] = msa[&#39;FIPS State Code&#39;].astype(str) msa[&#39;FIPS County Code&#39;] = msa[&#39;FIPS County Code&#39;].astype(str) . state_fips_max_length = msa[&#39;FIPS State Code&#39;].map(len).max() county_fips_max_length = msa[&#39;FIPS County Code&#39;].map(len).max() . msa[&#39;FIPS State Code&#39;] = msa[&#39;FIPS State Code&#39;].apply(lambda x: &#39;0&#39;*(state_fips_max_length - len(x))+x) msa[&#39;FIPS County Code&#39;] = msa[&#39;FIPS County Code&#39;].apply(lambda x: &#39;0&#39;*(county_fips_max_length - len(x))+x) . msa[&#39;fips&#39;] = msa[&#39;FIPS State Code&#39;]+msa[&#39;FIPS County Code&#39;] msa[&#39;fips&#39;] = msa[&#39;fips&#39;].astype(float) . Now we will add a row for New York . nyc_temp = pd.DataFrame({&#39;CBSA Code&#39;: 35620,&#39;Metropolitan Division Code&#39;: None, &#39;CSA Code&#39;: 408, &#39;CBSA Title&#39;: None,&#39;Metropolitan/Micropolitan Statistical Area&#39;: None, &#39;Metropolitan Division Title&#39;: None,&#39;CSA Title&#39;: None,&#39;County/County Equivalent&#39;: None, &#39;State Name&#39;: &#39;New York&#39;, &#39;FIPS State Code&#39;: 36, &#39;FIPS County Code&#39;: None, &#39;Central/Outlying County&#39;: None, &#39;fips&#39;: 1},index=[0]) msa = pd.concat([msa, nyc_temp], ignore_index=True) msa . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips . 0 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 013 | Central | 46013.0 | . 1 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 045 | Outlying | 46045.0 | . 2 10140 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 027 | Central | 53027.0 | . 3 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 059 | Outlying | 48059.0 | . 4 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | 48253.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1912 49700 | NaN | 472.0 | Yuba City, CA | Metropolitan Statistical Area | NaN | Sacramento-Roseville, CA | Yuba County | California | 06 | 115 | Central | 6115.0 | . 1913 49740 | NaN | NaN | Yuma, AZ | Metropolitan Statistical Area | NaN | NaN | Yuma County | Arizona | 04 | 027 | Central | 4027.0 | . 1914 49780 | NaN | 198.0 | Zanesville, OH | Micropolitan Statistical Area | NaN | Columbus-Marion-Zanesville, OH | Muskingum County | Ohio | 39 | 119 | Central | 39119.0 | . 1915 49820 | NaN | NaN | Zapata, TX | Micropolitan Statistical Area | NaN | NaN | Zapata County | Texas | 48 | 505 | Central | 48505.0 | . 1916 35620 | None | 408.0 | None | None | None | None | None | New York | 36 | None | None | 1.0 | . 1917 rows × 13 columns . msa[&#39;CBSA Code&#39;] = msa[&#39;CBSA Code&#39;].astype(float) . msa[msa[&#39;fips&#39;].isin(aggregate[&#39;fips&#39;]) == False] . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips . 8 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Aguada Municipio | Puerto Rico | 72 | 003 | Central | 72003.0 | . 9 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Aguadilla Municipio | Puerto Rico | 72 | 005 | Central | 72005.0 | . 10 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Añasco Municipio | Puerto Rico | 72 | 011 | Central | 72011.0 | . 11 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Isabela Municipio | Puerto Rico | 72 | 071 | Central | 72071.0 | . 12 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Lares Municipio | Puerto Rico | 72 | 081 | Central | 72081.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1591 42180.0 | NaN | 434.0 | Santa Isabel, PR | Micropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Santa Isabel Municipio | Puerto Rico | 72 | 133 | Central | 72133.0 | . 1903 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Guánica Municipio | Puerto Rico | 72 | 055 | Central | 72055.0 | . 1904 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Guayanilla Municipio | Puerto Rico | 72 | 059 | Central | 72059.0 | . 1905 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Peñuelas Municipio | Puerto Rico | 72 | 111 | Central | 72111.0 | . 1906 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Yauco Municipio | Puerto Rico | 72 | 153 | Central | 72153.0 | . 88 rows × 13 columns . # Puerto Rico does not provide data at county level. So we will have to do a similar exercise like NYC for PR and aggregate it statewise. # But since in albersUsa projection PR is filtered anyways, we won&#39;t be doing that exercise right away. Once I figure out how to use custom projection in the default # albersUsa projection that is used by Vega-Lite under the hood, we will include Puerto Rico aggregate[aggregate[&#39;state&#39;].str.startswith(&#39;Puerto&#39;)] . fips county date state cases deaths . 1 2.0 | Unknown | 2020-07-04 | Puerto Rico | 7787 | 155 | . aggregate[aggregate[&#39;fips&#39;].isin(msa[&#39;fips&#39;]) == False] . fips county date state cases deaths . 1 2.0 | Unknown | 2020-07-04 | Puerto Rico | 7787 | 155 | . 7 1011.0 | Bullock | 2020-07-04 | Alabama | 373 | 11 | . 8 1013.0 | Butler | 2020-07-04 | Alabama | 626 | 28 | . 11 1019.0 | Cherokee | 2020-07-04 | Alabama | 88 | 7 | . 13 1023.0 | Choctaw | 2020-07-04 | Alabama | 196 | 12 | . ... ... | ... | ... | ... | ... | ... | . 3052 56027.0 | Niobrara | 2020-07-04 | Wyoming | 2 | 0 | . 3053 56029.0 | Park | 2020-07-04 | Wyoming | 61 | 0 | . 3054 56031.0 | Platte | 2020-07-04 | Wyoming | 4 | 0 | . 3056 56035.0 | Sublette | 2020-07-04 | Wyoming | 6 | 0 | . 3060 56043.0 | Washakie | 2020-07-04 | Wyoming | 38 | 5 | . 1233 rows × 6 columns . Now we will merge the aggregated data with msa since msa is expanded on fips(CBSA repeats), which is the only unique column. . msa = msa.merge(aggregate, how=&#39;inner&#39;, on=&#39;fips&#39;) msa . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips county date state cases deaths . 0 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 013 | Central | 46013.0 | Brown | 2020-07-04 | South Dakota | 343 | 2 | . 1 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 045 | Outlying | 46045.0 | Edmunds | 2020-07-04 | South Dakota | 8 | 0 | . 2 10140.0 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 027 | Central | 53027.0 | Grays Harbor | 2020-07-04 | Washington | 26 | 0 | . 3 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 059 | Outlying | 48059.0 | Callahan | 2020-07-04 | Texas | 19 | 2 | . 4 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | 48253.0 | Jones | 2020-07-04 | Texas | 610 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1824 49700.0 | NaN | 472.0 | Yuba City, CA | Metropolitan Statistical Area | NaN | Sacramento-Roseville, CA | Yuba County | California | 06 | 115 | Central | 6115.0 | Yuba | 2020-07-04 | California | 120 | 2 | . 1825 49740.0 | NaN | NaN | Yuma, AZ | Metropolitan Statistical Area | NaN | NaN | Yuma County | Arizona | 04 | 027 | Central | 4027.0 | Yuma | 2020-07-04 | Arizona | 7062 | 110 | . 1826 49780.0 | NaN | 198.0 | Zanesville, OH | Micropolitan Statistical Area | NaN | Columbus-Marion-Zanesville, OH | Muskingum County | Ohio | 39 | 119 | Central | 39119.0 | Muskingum | 2020-07-04 | Ohio | 81 | 1 | . 1827 49820.0 | NaN | NaN | Zapata, TX | Micropolitan Statistical Area | NaN | NaN | Zapata County | Texas | 48 | 505 | Central | 48505.0 | Zapata | 2020-07-04 | Texas | 56 | 0 | . 1828 35620.0 | None | 408.0 | None | None | None | None | None | New York | 36 | None | None | 1.0 | New York City | 2020-07-04 | New York | 221395 | 22630 | . 1829 rows × 18 columns . msa.rename(columns={&#39;CBSA Code&#39;: &#39;CBSAFP&#39;}, inplace=True) . msa_shp[&#39;CBSAFP&#39;] = msa_shp[&#39;CBSAFP&#39;].astype(float) . msa_shp[&#39;lon&#39;] = msa_shp[&#39;geometry&#39;].centroid.x msa_shp[&#39;lat&#39;] = msa_shp[&#39;geometry&#39;].centroid.y . Now we will aggregate the msa data on CBSAFP so that it becomes similar to msa_shp geodataframe . msa_agg = msa.groupby(&#39;CBSAFP&#39;, as_index=False).agg({&#39;CBSA Title&#39;: &#39;first&#39;, &#39;State Name&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;sum&#39;, &#39;deaths&#39;: &#39;sum&#39;}) . msa_agg.head() . CBSAFP CBSA Title State Name date cases deaths . 0 10100.0 | Aberdeen, SD | South Dakota | 2020-07-04 | 351 | 2 | . 1 10140.0 | Aberdeen, WA | Washington | 2020-07-04 | 26 | 0 | . 2 10180.0 | Abilene, TX | Texas | 2020-07-04 | 1209 | 8 | . 3 10220.0 | Ada, OK | Oklahoma | 2020-07-04 | 44 | 2 | . 4 10300.0 | Adrian, MI | Michigan | 2020-07-04 | 966 | 10 | . Now we will merge msa_agg with msa and make the heights column that contains a custom SVG Path string per row, since right now Vega-Lite does not support &quot;scaleY&quot; as an encoding. In Vega, we don&#39;t have to do it this way, we can just provide a single svg path for an equilateral triangle and then stretch it(using scaleY) based on &quot;cases&quot; or &quot;deaths&quot;. . msa_shp = msa_shp.merge(msa_agg, how=&#39;inner&#39;, on=&#39;CBSAFP&#39;) . msa_shp[&#39;height&#39;] = msa_shp[&#39;deaths&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/50} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) msa_shp.head() . CSAFP CBSAFP AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat CBSA Title State Name date cases deaths height . 0 425 | 37620.0 | 310M500US37620 | 37620 | Parkersburg-Vienna, WV | M1 | 1551452495 | 32408833 | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-07-04 | 123 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | . 1 None | 45980.0 | 310M500US45980 | 45980 | Troy, AL | M2 | 1740647520 | 2336975 | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-07-04 | 427 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | . 2 548 | 49020.0 | 310M500US49020 | 49020 | Winchester, VA-WV | M1 | 2752545068 | 16892497 | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-07-04 | 892 | 10 | M -1.5 0 L0 -0.2 L1.5 0 | . 3 142 | 45180.0 | 310M500US45180 | 45180 | Talladega-Sylacauga, AL | M2 | 1908293036 | 60927931 | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-07-04 | 311 | 8 | M -1.5 0 L0 -0.16 L1.5 0 | . 4 None | 25060.0 | 310M500US25060 | 25060 | Gulfport-Biloxi, MS | M1 | 5739122781 | 2105780374 | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-07-04 | 1692 | 40 | M -1.5 0 L0 -0.8 L1.5 0 | . msa_shp.drop([&#39;CSAFP&#39;, &#39;AFFGEOID&#39;, &#39;GEOID&#39;, &#39;LSAD&#39;, &#39;ALAND&#39;, &#39;AWATER&#39;], axis=1, inplace=True) . msa_shp.head() . CBSAFP NAME geometry lon lat CBSA Title State Name date cases deaths height . 0 37620.0 | Parkersburg-Vienna, WV | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-07-04 | 123 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | . 1 45980.0 | Troy, AL | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-07-04 | 427 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | . 2 49020.0 | Winchester, VA-WV | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-07-04 | 892 | 10 | M -1.5 0 L0 -0.2 L1.5 0 | . 3 45180.0 | Talladega-Sylacauga, AL | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-07-04 | 311 | 8 | M -1.5 0 L0 -0.16 L1.5 0 | . 4 25060.0 | Gulfport-Biloxi, MS | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-07-04 | 1692 | 40 | M -1.5 0 L0 -0.8 L1.5 0 | . Let&#39;s finally plot this graph of deaths till now - . spikes = alt.Chart(msa_shp).transform_filter(alt.datum.deaths&gt;0).mark_point( fillOpacity=1, fill=alt.Gradient( gradient=&quot;linear&quot;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;red&#39;, offset=0.5)], x1=1, x2=1, y1=1, y2=0 ), #dx=10, #dy=-30, strokeOpacity=1, strokeWidth=1, stroke=&#39;red&#39; ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height:N&quot;, scale=None), #tooltip=[&#39;CBSA Title:N&#39;, &#39;deaths:Q&#39;], #color = alt.condition(selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1200, height=800 ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) (state+spikes).configure_view(strokeWidth=0) . Now we will study only last week&#39;s average cases per day to see where the indections are on the rise . #msa_shp[&#39;height_cases&#39;] = msa_shp[&#39;cases&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/1000} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) . cdf[&#39;cases_per_day&#39;] = cdf.groupby(&quot;fips&quot;)[&#39;cases&#39;].diff() . last_week_cases_avg = cdf.groupby(&quot;fips&quot;)[&quot;cases_per_day&quot;].apply(lambda x: x.iloc[-7:].mean()) . last_week_cases_avg = last_week_cases_avg.reset_index() . last_week_cases_avg.columns = [&#39;fips&#39;, &#39;avg_cases_last_week&#39;] . last_week_cases_avg.head() . fips avg_cases_last_week . 0 1.0 | 319.714286 | . 1 2.0 | 103.000000 | . 2 1001.0 | 13.285714 | . 3 1003.0 | 44.000000 | . 4 1005.0 | 4.714286 | . avg_last_week_cases = cdf.groupby(&quot;fips&quot;).agg({&#39;county&#39;: &#39;first&#39;, &#39;cases_per_day&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) . avg_last_week_cases = avg_last_week_cases.merge(last_week_cases_avg, how=&#39;inner&#39;, on=&#39;fips&#39;) . avg_last_week_cases.head() . fips county cases_per_day date state cases deaths avg_cases_last_week . 0 1.0 | New York City | 367.0 | 2020-07-04 | New York | 221395 | 22630 | 319.714286 | . 1 2.0 | Unknown | 104.0 | 2020-07-04 | Puerto Rico | 7787 | 155 | 103.000000 | . 2 1001.0 | Autauga | 23.0 | 2020-07-04 | Alabama | 591 | 13 | 13.285714 | . 3 1003.0 | Baldwin | 18.0 | 2020-07-04 | Alabama | 863 | 10 | 44.000000 | . 4 1005.0 | Barbour | 2.0 | 2020-07-04 | Alabama | 350 | 2 | 4.714286 | . msa_agg_lastweek = msa.merge(avg_last_week_cases, how=&#39;inner&#39;, on=&#39;fips&#39;) msa_agg_lastweek.head() . CBSAFP Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code ... state_x cases_x deaths_x county_y cases_per_day date_y state_y cases_y deaths_y avg_cases_last_week . 0 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | ... | South Dakota | 343 | 2 | Brown | -1.0 | 2020-07-04 | South Dakota | 343 | 2 | 0.571429 | . 1 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | ... | South Dakota | 8 | 0 | Edmunds | 1.0 | 2020-07-04 | South Dakota | 8 | 0 | 0.142857 | . 2 10140.0 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | ... | Washington | 26 | 0 | Grays Harbor | 0.0 | 2020-07-04 | Washington | 26 | 0 | 0.142857 | . 3 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | ... | Texas | 19 | 2 | Callahan | 1.0 | 2020-07-04 | Texas | 19 | 2 | 0.428571 | . 4 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | ... | Texas | 610 | 0 | Jones | 0.0 | 2020-07-04 | Texas | 610 | 0 | 0.428571 | . 5 rows × 25 columns . # Average of multiple series is same as sum of their averages so &#39;avg_cases_last_week&#39;: &#39;sum&#39; works well msa_agg_lastweek = msa_agg_lastweek.groupby(&#39;CBSAFP&#39;, as_index=False).agg({&#39;CBSA Title&#39;: &#39;first&#39;, &#39;State Name&#39;: &#39;last&#39;, &#39;avg_cases_last_week&#39;: &#39;sum&#39;}) . msa_agg_lastweek.head() . CBSAFP CBSA Title State Name avg_cases_last_week . 0 10100.0 | Aberdeen, SD | South Dakota | 0.714286 | . 1 10140.0 | Aberdeen, WA | Washington | 0.142857 | . 2 10180.0 | Abilene, TX | Texas | 21.857143 | . 3 10220.0 | Ada, OK | Oklahoma | 1.285714 | . 4 10300.0 | Adrian, MI | Michigan | 4.000000 | . msa_shp = msa_shp.merge(msa_agg_lastweek, how=&#39;inner&#39;, on=&#39;CBSAFP&#39;) msa_shp.head() . CBSAFP NAME geometry lon lat CBSA Title_x State Name_x date cases deaths height CBSA Title_y State Name_y avg_cases_last_week . 0 37620.0 | Parkersburg-Vienna, WV | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-07-04 | 123 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | Parkersburg-Vienna, WV | West Virginia | 6.428571 | . 1 45980.0 | Troy, AL | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-07-04 | 427 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | Troy, AL | Alabama | 4.714286 | . 2 49020.0 | Winchester, VA-WV | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-07-04 | 892 | 10 | M -1.5 0 L0 -0.2 L1.5 0 | Winchester, VA-WV | West Virginia | 9.000000 | . 3 45180.0 | Talladega-Sylacauga, AL | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-07-04 | 311 | 8 | M -1.5 0 L0 -0.16 L1.5 0 | Talladega-Sylacauga, AL | Alabama | 11.857143 | . 4 25060.0 | Gulfport-Biloxi, MS | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-07-04 | 1692 | 40 | M -1.5 0 L0 -0.8 L1.5 0 | Gulfport-Biloxi, MS | Mississippi | 60.000000 | . Adding a new column called height_last_week_avg_cases that has the custom SVG paths like we did earlier - . msa_shp[&#39;height_last_week_avg_cases&#39;] = msa_shp[&#39;avg_cases_last_week&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/10} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) . spikes = alt.Chart(msa_shp).mark_point( fillOpacity=1, fill=alt.Gradient( gradient=&quot;linear&quot;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;red&#39;, offset=0.5)], x1=1, x2=1, y1=1, y2=0 ), #dx=10, #dy=-30, stroke=&#39;red&#39;, strokeOpacity=1, strokeWidth=1 ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height_last_week_avg_cases:N&quot;, scale=None), tooltip=[&#39;NAME:N&#39;, &#39;avg_cases_last_week:Q&#39;] ).project( type=&#39;albersUsa&#39; ).properties( width=1200, height=800 ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) (state+spikes).configure_view(strokeWidth=0) . It&#39;s clear that now the cases are increasing much rapidly in the South especially in the states of - Texas, California, Florida and Arizona . There you have it! .",
            "url": "https://armsp.github.io/covidviz/geospatial/nyt/2020/07/01/MSA-cases-and-deaths.html",
            "relUrl": "/geospatial/nyt/2020/07/01/MSA-cases-and-deaths.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Per Capita COVID-19 cases worldwide",
            "content": "Today we will make the Per Capita covid cases worldwide from the article Coronavirus Map: Tracking the Global Outbreak that looks like the following - . . import altair as alt import pandas as pd import geopandas as gpd alt.renderers.set_embed_options(actions=False) . RendererRegistry.enable(&#39;default&#39;) . We will use the JHU CSSE Dataset for the cases as well as the population. For the map we will use the shapefiles from Natural Earth. . population_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv&#39; population_data = pd.read_csv(population_uri) latest_cases_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/10-29-2020.csv&#39; latest_cases = pd.read_csv(latest_cases_uri) world_shapefile_uri = &quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip&quot; world = gpd.read_file(world_shapefile_uri) . We have to make some changes so that we get things right. For example there&#39;s quite a few names to change for the countries because the cases dataset doesn&#39;t have identifier informations. Then in the map file we have to merge a few countries and segregate a few based on how JHU CSSE reports their cases. . In the Map . Somalia is combination of Somalia and Somaliland // NYT shows them together(combined) | Greenland is separate from Denmark // latestcases shows them together but NYT shows them separately | . latest_cases.head() . FIPS Admin2 Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active Combined_Key Incidence_Rate Case-Fatality_Ratio . 0 NaN | NaN | NaN | Afghanistan | 2020-10-30 04:24:49 | 33.93911 | 67.709953 | 41268 | 1532 | 34239 | 5497.0 | Afghanistan | 106.010169 | 3.712319 | . 1 NaN | NaN | NaN | Albania | 2020-10-30 04:24:49 | 41.15330 | 20.168300 | 20315 | 499 | 11007 | 8809.0 | Albania | 705.921190 | 2.456313 | . 2 NaN | NaN | NaN | Algeria | 2020-10-30 04:24:49 | 28.03390 | 1.659600 | 57332 | 1949 | 39635 | 15748.0 | Algeria | 130.742614 | 3.399498 | . 3 NaN | NaN | NaN | Andorra | 2020-10-30 04:24:49 | 42.50630 | 1.521800 | 4567 | 73 | 3260 | 1234.0 | Andorra | 5910.826377 | 1.598423 | . 4 NaN | NaN | NaN | Angola | 2020-10-30 04:24:49 | -11.20270 | 17.873900 | 10269 | 275 | 3736 | 6258.0 | Angola | 31.244801 | 2.677963 | . latest_cases[latest_cases[&#39;Country_Region&#39;].str.contains(&#39;Denmark&#39;)] . FIPS Admin2 Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active Combined_Key Incidence_Rate Case-Fatality_Ratio . 174 NaN | NaN | Faroe Islands | Denmark | 2020-10-30 04:24:49 | 61.8926 | -6.9118 | 494 | 0 | 479 | 15.0 | Faroe Islands, Denmark | 1010.948532 | 0.000000 | . 175 NaN | NaN | Greenland | Denmark | 2020-10-30 04:24:49 | 71.7069 | -42.6043 | 17 | 0 | 16 | 1.0 | Greenland, Denmark | 29.944339 | 0.000000 | . 176 NaN | NaN | NaN | Denmark | 2020-10-30 04:24:49 | 56.2639 | 9.5018 | 44034 | 716 | 33601 | 9717.0 | Denmark | 760.228880 | 1.626016 | . latest_cases.loc[latest_cases[&#39;Province_State&#39;]==&#39;Greenland&#39;, &#39;Country_Region&#39;] = &quot;Greenland&quot; population_data.loc[population_data[&#39;Province_State&#39;]==&#39;Greenland&#39;, &#39;Country_Region&#39;] = &quot;Greenland&quot; population_data.loc[population_data[&#39;Province_State&#39;]==&#39;Greenland&#39;, &#39;Combined_Key&#39;] = &quot;Greenland&quot; . latest_cases = latest_cases.drop([&#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Last_Update&#39;, &#39;Lat&#39;, &#39;Long_&#39;, &#39;Combined_Key&#39;, &#39;Incidence_Rate&#39;, &#39;Case-Fatality_Ratio&#39;], axis=1) latest_cases = latest_cases.groupby(&#39;Country_Region&#39;).aggregate({&#39;Confirmed&#39;: &#39;sum&#39;, &#39;Recovered&#39;: &#39;sum&#39;, &#39;Deaths&#39;: &#39;sum&#39;, &#39;Active&#39;: &#39;sum&#39;, }) latest_cases = latest_cases.reset_index() latest_cases.head() . Country_Region Confirmed Recovered Deaths Active . 0 Afghanistan | 41268 | 34239 | 1532 | 5497.0 | . 1 Albania | 20315 | 11007 | 499 | 8809.0 | . 2 Algeria | 57332 | 39635 | 1949 | 15748.0 | . 3 Andorra | 4567 | 3260 | 73 | 1234.0 | . 4 Angola | 10269 | 3736 | 275 | 6258.0 | . world = world[~(world[&#39;CONTINENT&#39;]==&#39;Antarctica&#39;)] world = world[[&#39;SOVEREIGNT&#39;, &#39;ADMIN&#39;, &#39;NAME&#39;, &#39;POP_EST&#39;, &#39;POP_YEAR&#39;, &#39;ISO_A3&#39;, &#39;CONTINENT&#39;, &#39;geometry&#39;]] world.head() . SOVEREIGNT ADMIN NAME POP_EST POP_YEAR ISO_A3 CONTINENT geometry . 0 Fiji | Fiji | Fiji | 920938 | 2017 | FJI | Oceania | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 United Republic of Tanzania | United Republic of Tanzania | Tanzania | 53950935 | 2017 | TZA | Africa | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 Western Sahara | Western Sahara | W. Sahara | 603253 | 2017 | ESH | Africa | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 Canada | Canada | Canada | 35623680 | 2017 | CAN | North America | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 United States of America | United States of America | United States of America | 326625791 | 2017 | USA | North America | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).encode().project(&#39;equalEarth&#39;) . world[world[&#39;NAME&#39;].str.contains(&#39;Green&#39;)] . SOVEREIGNT ADMIN NAME POP_EST POP_YEAR ISO_A3 CONTINENT geometry . 22 Denmark | Greenland | Greenland | 57713 | 2017 | GRL | North America | POLYGON ((-46.76379 82.62796, -43.40644 83.225... | . somalia = world[world[&#39;NAME&#39;].str.contains(&#39;Somali&#39;)] somalia = somalia.dissolve(by=&#39;CONTINENT&#39;).reset_index() somalia . CONTINENT geometry SOVEREIGNT ADMIN NAME POP_EST POP_YEAR ISO_A3 . 0 Africa | POLYGON ((41.58513 -1.68325, 40.99300 -0.85829... | Somalia | Somalia | Somalia | 7531386 | 2017 | SOM | . world=pd.concat([world, somalia]) . alt.Chart(somalia).mark_geoshape().encode() . Making sure that names are same . world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;eSwatini&#39;), &#39;ADMIN&#39;] = &#39;Eswatini&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;Palestine&#39;), &#39;ADMIN&#39;] = &#39;West Bank and Gaza&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;Republic of Serbia&#39;), &#39;ADMIN&#39;] = &#39;Serbia&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;United Republic of Tanzania&#39;), &#39;ADMIN&#39;] = &#39;Tanzania&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;São Tomé and Principe&#39;), &#39;ADMIN&#39;] = &#39;Sao Tome and Principe&#39; . latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Korea, South&#39;, &#39;Country_Region&#39;] = &#39;South Korea&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&quot;Cote d&#39;Ivoire&quot;, &#39;Country_Region&#39;] = &#39;Ivory Coast&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Timor-Leste&#39;, &#39;Country_Region&#39;] = &#39;East Timor&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Taiwan*&#39;, &#39;Country_Region&#39;] = &#39;Taiwan&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Burma&#39;, &#39;Country_Region&#39;] = &#39;Myanmar&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;US&#39;, &#39;Country_Region&#39;] = &#39;United States of America&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Czech Republic&#39;, &#39;Country_Region&#39;] = &#39;Czechia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;North Macedonia&#39;, &#39;Country_Region&#39;] = &#39;Macedonia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Bahamas&#39;, &#39;Country_Region&#39;] = &#39;The Bahamas&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country_Region&#39;] = &#39;Democratic Republic of the Congo&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country_Region&#39;] = &#39;Republic of the Congo&#39; . We will ignore this for now as they are ships/cruises (NYT does however show them as aggregates of corresponding countries) . latest_cases[latest_cases[&#39;Country_Region&#39;].isin(world[&#39;ADMIN&#39;]) == False] . Country_Region Confirmed Recovered Deaths Active . 3 Andorra | 4567 | 3260 | 73 | 1234.0 | . 5 Antigua and Barbuda | 124 | 115 | 3 | 6.0 | . 12 Bahrain | 81262 | 78102 | 317 | 2843.0 | . 14 Barbados | 234 | 217 | 7 | 10.0 | . 29 Cabo Verde | 8603 | 7796 | 95 | 712.0 | . 38 Comoros | 517 | 494 | 7 | 16.0 | . 48 Diamond Princess | 712 | 659 | 13 | 40.0 | . 50 Dominica | 38 | 29 | 0 | 9.0 | . 70 Grenada | 28 | 24 | 0 | 4.0 | . 76 Holy See | 27 | 15 | 0 | 12.0 | . 102 Liechtenstein | 476 | 265 | 1 | 210.0 | . 105 MS Zaandam | 9 | 0 | 2 | 7.0 | . 109 Maldives | 11616 | 10733 | 37 | 846.0 | . 111 Malta | 5866 | 3880 | 59 | 1927.0 | . 112 Marshall Islands | 2 | 0 | 0 | 2.0 | . 114 Mauritius | 439 | 389 | 10 | 40.0 | . 117 Monaco | 347 | 264 | 2 | 81.0 | . 144 Saint Kitts and Nevis | 19 | 19 | 0 | 0.0 | . 145 Saint Lucia | 76 | 27 | 0 | 49.0 | . 146 Saint Vincent and the Grenadines | 74 | 70 | 0 | 4.0 | . 147 San Marino | 928 | 721 | 42 | 165.0 | . 148 Sao Tome and Principe | 944 | 904 | 16 | 24.0 | . 152 Seychelles | 153 | 149 | 0 | 4.0 | . 154 Singapore | 57994 | 57899 | 28 | 67.0 | . Extracting population data for countries - . population_data = population_data.drop([&#39;UID&#39;, &#39;code3&#39;, &#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Lat&#39;, &#39;Long_&#39;], axis=1) population_data = population_data[population_data[&#39;Country_Region&#39;] == population_data[&#39;Combined_Key&#39;]] population_data = population_data.reset_index(drop=True) population_data.head() . iso2 iso3 Country_Region Combined_Key Population . 0 AF | AFG | Afghanistan | Afghanistan | 38928341.0 | . 1 AL | ALB | Albania | Albania | 2877800.0 | . 2 DZ | DZA | Algeria | Algeria | 43851043.0 | . 3 AD | AND | Andorra | Andorra | 77265.0 | . 4 AO | AGO | Angola | Angola | 32866268.0 | . population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Taiwan*&#39;, &#39;Country_Region&#39;] = &#39;Taiwan&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Korea, South&#39;, &#39;Country_Region&#39;] = &#39;South Korea&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&quot;Cote d&#39;Ivoire&quot;, &#39;Country_Region&#39;] = &#39;Ivory Coast&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Timor-Leste&#39;, &#39;Country_Region&#39;] = &#39;East Timor&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;US&#39;, &#39;Country_Region&#39;] = &#39;United States of America&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Czech Republic&#39;, &#39;Country_Region&#39;] = &#39;Czechia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Burma&#39;, &#39;Country_Region&#39;] = &#39;Myanmar&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;North Macedonia&#39;, &#39;Country_Region&#39;] = &#39;Macedonia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Bahamas&#39;, &#39;Country_Region&#39;] = &#39;The Bahamas&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country_Region&#39;] = &#39;Democratic Republic of the Congo&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country_Region&#39;] = &#39;Republic of the Congo&#39; . world.columns = [&#39;SOVEREIGNT&#39;, &#39;Country_Region&#39;, &#39;NAME&#39;, &#39;POP_EST&#39;, &#39;POP_YEAR&#39;, &#39;ISO_A3&#39;, &#39;CONTINENT&#39;, &#39;geometry&#39;] world = world.merge(latest_cases, on=&#39;Country_Region&#39;, how=&#39;left&#39;) world = world.merge(population_data, on=&#39;Country_Region&#39;, how=&#39;left&#39;) world[&#39;per_capita&#39;] = world[&#39;Confirmed&#39;]/world[&#39;Population&#39;] . world.head() . SOVEREIGNT Country_Region NAME POP_EST POP_YEAR ISO_A3 CONTINENT geometry Confirmed Recovered Deaths Active iso2 iso3 Combined_Key Population per_capita . 0 Fiji | Fiji | Fiji | 920938 | 2017 | FJI | Oceania | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | 34.0 | 31.0 | 2.0 | 1.0 | FJ | FJI | Fiji | 896444.0 | 0.000038 | . 1 United Republic of Tanzania | Tanzania | Tanzania | 53950935 | 2017 | TZA | Africa | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | 509.0 | 183.0 | 21.0 | 305.0 | TZ | TZA | Tanzania | 59734213.0 | 0.000009 | . 2 Western Sahara | Western Sahara | W. Sahara | 603253 | 2017 | ESH | Africa | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | 10.0 | 8.0 | 1.0 | 1.0 | EH | ESH | Western Sahara | 597330.0 | 0.000017 | . 3 Canada | Canada | Canada | 35623680 | 2017 | CAN | North America | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | 231383.0 | 194105.0 | 10127.0 | 27152.0 | CA | CAN | Canada | 37855702.0 | 0.006112 | . 4 United States of America | United States of America | United States of America | 326625791 | 2017 | USA | North America | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | 8944934.0 | 3554336.0 | 228656.0 | 5161921.0 | US | USA | US | 329466283.0 | 0.027150 | . world[&#39;code&#39;] = world[&#39;per_capita&#39;].apply(lambda x: &#39;Less than 1 in 1000&#39; if x &lt;= (1/1000) else &#39;Less than 1 in 500&#39; if x&lt;= (1/500) else &#39;Less than 1 in 333&#39; if x&lt;= (1/333) else &#39;No Cases reported&#39; if pd.isnull(x) else &#39;Greater than 1 in 333&#39;) world[&#39;Share of Population&#39;] = world[&#39;Population&#39;]/world[&#39;Confirmed&#39;] world[&#39;Share of Population&#39;] = world[&#39;Share of Population&#39;].round() world[&#39;Share of Population&#39;] = world[&#39;Share of Population&#39;].apply(lambda x: f&quot;1 in {str(x).split(&#39;.&#39;)[0]}&quot;) . alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).transform_filter(alt.datum.Country_Region != &#39;Antarctica&#39;).encode( color=alt.Color(&#39;code:N&#39;, scale=alt.Scale(domain=[&#39;No Cases reported&#39;, &#39;Less than 1 in 1000&#39;, &#39;Less than 1 in 500&#39;, &#39;Less than 1 in 333&#39;, &#39;Greater than 1 in 333&#39;], range=[&#39;lightgrey&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;]),legend=alt.Legend(title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200)), tooltip = [&#39;Country_Region&#39;, &#39;Confirmed&#39;, &#39;Share of Population&#39;] ).properties(width=1400, height=800).project(&#39;equalEarth&#39;).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/interactive/chloropleth/nyt/2020/06/28/World-Per-Capita-Cases.html",
            "relUrl": "/geospatial/interactive/chloropleth/nyt/2020/06/28/World-Per-Capita-Cases.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Temporary decline in CO₂ due to COVID-19",
            "content": "Today we will work on the following graph from the article Emissions Are Surging Back as Countries and States Reopen - . . I downloaded the dataset as an Excel file and saved data for individual countries as csv files. . import altair as alt import pandas as pd from functools import wraps import datetime as dt . #hide_output alt.renderers.set_embed_options(actions=False) def log_step(func): @wraps(func) def wrapper(*args, **kwargs): &quot;&quot;&quot;timing or logging etc&quot;&quot;&quot; start = dt.datetime.now() output = func(*args, **kwargs) end = dt.datetime.now() print(f&quot;After function {func.__name__} ran, shape of dataframe is - {output.shape}, execution time is - {end-start}&quot;) return output return wrapper @log_step def read_concat_country_data(): india = pd.read_csv(&#39;ind_co2_em.csv&#39;) india = india.iloc[1:] china = pd.read_csv(&#39;china_co2_em.csv&#39;, sep=&#39;;&#39;) china = china.iloc[1:] us = pd.read_csv(&#39;us_co2_em.csv&#39;, sep=&#39;;&#39;) us = us.iloc[1:] euuk = pd.read_csv(&#39;euuk_co2_em.csv&#39;, sep=&#39;;&#39;) euuk = euuk.iloc[1:] globl = pd.read_csv(&#39;global_co2_em.csv&#39;, sep=&#39;;&#39;) globl = globl.iloc[1:] data = pd.concat([china, india, euuk, us, globl]) return data @log_step def drop_columns(df, cols): df.drop(columns = cols, inplace=True) return df def set_datatypes(df): df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) df[list(df.columns)[3:]] = df[list(df.columns)[3:]].apply(pd.to_numeric) return df @log_step def make_plotting_data(df): &#39;&#39;&#39;Remove GLOBAL, subtract the sum of countries emissions from GLOBAL to get REST (of the world) data&#39;&#39;&#39; except_global_data = df[df[&#39;REGION_CODE&#39;] != &#39;GLOBAL&#39;] global_data = df[df[&#39;REGION_CODE&#39;] == &#39;GLOBAL&#39;].reset_index(drop=True) countries_emissions = except_global_data.groupby(&#39;DATE&#39;, as_index=False).sum()#.reindex(except_global_data.columns, axis=1).fillna({&#39;REGION_CODE&#39;: &#39;RST&#39;, &#39;REGION_NAME&#39;: &#39;REST&#39;}) rest_emissions_data = global_data[list(global_data.columns)[3:]] - countries_emissions#[list(countries_emissions.columns)[5:]] rest_emissions_data = rest_emissions_data.reindex(global_data.columns, axis=1).fillna({&#39;REGION_CODE&#39;: &#39;RST&#39;, &#39;REGION_NAME&#39;: &#39;REST&#39;, &#39;DATE&#39;: global_data[&#39;DATE&#39;]}) plot_data = pd.concat([except_global_data, rest_emissions_data]) return plot_data . emission_data = (read_concat_country_data() .pipe(drop_columns, *[[&#39;REGION_ID&#39;, &#39;TIME_POINT&#39;]]) .pipe(set_datatypes)) emission_data.head() . After function read_concat_country_data ran, shape of dataframe is - (815, 26), execution time is - 0:00:00.055744 After function drop_columns ran, shape of dataframe is - (815, 24), execution time is - 0:00:00.001328 . REGION_CODE REGION_NAME DATE TOTAL_CO2_MED PWR_CO2_MED IND_CO2_MED TRS_CO2_MED PUB_CO2_MED RES_CO2_MED AVI_CO2_MED ... PUB_CO2_LOW RES_CO2_LOW AVI_CO2_LOW TOTAL_CO2_HIGH PWR_CO2_HIGH IND_CO2_HIGH TRS_CO2_HIGH PUB_CO2_HIGH RES_CO2_HIGH AVI_CO2_HIGH . 1 CHN | China | 2020-01-01 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 CHN | China | 2020-01-02 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 CHN | China | 2020-01-03 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 CHN | China | 2020-01-04 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 CHN | China | 2020-01-05 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 24 columns . If you observe the chart closely you will realize that the graph is stacked, so that is what we will do right away using altair&#39;s area chart - . alt.Chart(emission_data).mark_area().encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_NAME:N&#39;),#,scale=alt.Scale(scheme=&#39;reds&#39;)), ).properties(width=800, height=400) . This is close but not exactly like what we saw in the article. If you look closely you&#39;d realize that the order of countries is different. So we will try to follow the same order using the order encoding field. . alt.Chart(emission_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;),#,scale=alt.Scale(scheme=&#39;reds&#39;)), order=&#39;order:O&#39; ).properties(width=800, height=400) . #This is exactly like it. Let&#39;s change the colors, I probably would have done it the following way - # alt.Chart(emission_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( # x=alt.X(&#39;DATE:T&#39;), # y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), # color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;GLOBAL&#39;], range=[&quot;#c9c9c9&quot;, &quot;#aaaaaa&quot;, &quot;#888888&quot;, &quot;#686868&quot;, &quot;#454545&quot;])), # order=&#39;order:O&#39; # ).properties(width=800, height=400) . To make it just like the graph in the article, we will get the colors from here . alt.Chart(emission_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;GLOBAL&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) . If you look closely, you would notice that we are capturing the trend perfectly, however the area for &quot;REST of the world&quot;(GLOBAL) is much more than what it should be. That is because, its duplicating the data from US, EU, India, and China. So we need to subtract the contributions of these places from the global data and then stack them. . plot_data = emission_data.pipe(make_plotting_data) . After function make_plotting_data ran, shape of dataframe is - (815, 24), execution time is - 0:00:00.063036 . alt.Chart(plot_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;))), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400).configure_view(strokeWidth=0).configure_axis(grid=False) . This looks exactly like the chart in the article. Right now there is no way to properly add text in a stacked chart&#39;s corresponding area, but let&#39;s try it anyways so that once this option is available in Vega-Lite we will fix this code immediately later on. . base = alt.Chart(plot_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;))), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) text = alt.Chart(plot_data).mark_text().encode( x=alt.X(&#39;DATE:T&#39;, aggregate=&#39;median&#39;, ), #y=alt.Y(&#39;variety:N&#39;), #detail=&#39;REGION_CODE:N&#39;, text=alt.Text(&#39;REGION_NAME:N&#39;), y=&#39;min(TOTAL_CO2_MED):Q&#39;, #text=&#39;REGION_NAME:N&#39; ) (base+text).configure_view(strokeWidth=0).configure_axis(grid=False) . You can get clever about it and provide hardcoded positions for text and then plot it so that&#39;s what we will do - . We will get the dates where TOTAL_CO2_MED is minimum for each region and add out hardcoded positions to it . plot_data = plot_data.reset_index(drop=True) #Important since indices repeat due to concatenation text_position = plot_data.loc[plot_data.groupby(&#39;REGION_NAME&#39;)[&#39;TOTAL_CO2_MED&#39;].idxmin(), [&#39;DATE&#39;, &#39;REGION_NAME&#39;]].reset_index(drop=True) text_position . DATE REGION_NAME . 0 2020-02-18 | China | . 1 2020-04-01 | EU and UK | . 2 2020-03-28 | India | . 3 2020-04-09 | REST | . 4 2020-04-12 | USA | . text_position[&#39;POSITION&#39;] = [-2,-4,-2,-14,-7] text_position[&#39;REGION_NAME&#39;] = [&#39;China&#39;, &#39;E.U. and Britain&#39;,&#39;India&#39;, &#39;Rest of the world&#39;, &#39;United States&#39;,] text_position . DATE REGION_NAME POSITION . 0 2020-02-18 | China | -2 | . 1 2020-04-01 | E.U. and Britain | -4 | . 2 2020-03-28 | India | -2 | . 3 2020-04-09 | Rest of the world | -14 | . 4 2020-04-12 | United States | -7 | . base = alt.Chart(plot_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;), orient=&#39;top&#39;, tickCount=6), title=None), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;, title=&quot;Million metric tons CO₂&quot;, axis=alt.Axis(domain=False)), color=alt.Color(&#39;REGION_CODE:N&#39;, legend=None, scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) text = alt.Chart(text_position).mark_text(size=13).encode( x=alt.X(&#39;DATE:T&#39;), #y=alt.Y(&#39;variety:N&#39;), #detail=&#39;REGION_CODE:N&#39;, text=alt.Text(&#39;REGION_NAME:N&#39;), y=&#39;POSITION:Q&#39;, #text=&#39;REGION_NAME:N&#39; ) (base+text).configure_view(strokeWidth=0).configure_axis(grid=False) . While we are at it we can also make the following graph of global emissions by sector - . . The main idea behind these plots is layering an area plot on top of a line chart with the area shaded by the LOW and HIGH columns - . global_emission = pd.read_csv(&#39;global_co2_em.csv&#39;, sep=&#39;;&#39;) global_emission = global_emission.iloc[1:] global_emission = (global_emission .pipe(drop_columns, *[[&#39;REGION_ID&#39;, &#39;TIME_POINT&#39;, &#39;REGION_CODE&#39;, &#39;REGION_NAME&#39;, &#39;TOTAL_CO2_MED&#39;, &#39;TOTAL_CO2_HIGH&#39;, &#39;TOTAL_CO2_LOW&#39;]]) .pipe(set_datatypes)) . After function drop_columns ran, shape of dataframe is - (163, 19), execution time is - 0:00:00.001131 . line = alt.Chart(global_emission).mark_line().encode( x=&#39;DATE:T&#39;, y=alt.Y(&#39;TRS_CO2_MED:Q&#39;), ) band = line.mark_area(opacity=0.3).encode( x=&#39;DATE:T&#39;, y=alt.Y(&#39;TRS_CO2_LOW:Q&#39;), y2=alt.Y2(&#39;TRS_CO2_HIGH:Q&#39;), ) line+band . Now we are going to change the data so that we can facet it properly like in the article&#39;s chart - . data = pd.concat([pd.melt(global_emission.filter(regex=&#39;_MED|DATE&#39;), id_vars=[&#39;DATE&#39;], var_name=&#39;MED_KEY&#39;, value_name=&#39;MED_VALUES&#39;), pd.melt(global_emission.filter(regex=&#39;_HIGH|DATE&#39;), id_vars=[&#39;DATE&#39;], var_name=&#39;HIGH_KEY&#39;, value_name=&#39;HIGH_VALUES&#39;), pd.melt(global_emission.filter(regex=&#39;_LOW|DATE&#39;), id_vars=[&#39;DATE&#39;], var_name=&#39;LOW_KEY&#39;, value_name=&#39;LOW_VALUES&#39;)], axis=1).T.drop_duplicates().T . data = data.assign(sector = data[&#39;MED_KEY&#39;].apply(lambda x: &quot;Road transportation and shipping&quot; if x.startswith(&#39;TRS&#39;) else &quot;Industry&quot; if x.startswith(&#39;IND&#39;) else &quot;Power&quot; if x.startswith(&#39;PWR&#39;) else &quot;Aviation&quot; if x.startswith(&#39;AVI&#39;) else &quot;Public buildings and commerce&quot; if x.startswith(&#39;PUB&#39;) else &quot;Residential&quot;)) #data . area_low_high = alt.Chart().mark_area(opacity=0.5).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=&quot;%b&quot;)), y2= &#39;HIGH_VALUES:Q&#39;, y= alt.Y(&#39;LOW_VALUES:Q&#39;, axis=alt.Axis(domain=False, tickCount=5)) ) line_med = alt.Chart().mark_line().encode( x=&#39;DATE:T&#39;, y=&#39;MED_VALUES:Q&#39; ) alt.layer(area_low_high, line_med, data=data).facet( facet=alt.Column(&#39;sector:N&#39;, title=&quot;Change in global CO u2082 emissions by sector&quot;, sort=[&#39;Road transportation and shipping&#39;, &#39;Industry&#39;, &#39;Power&#39;, &#39;Aviation&#39;, &#39;Public buildings and commerce&#39;, &#39;Residential&#39;], header=alt.Header(labelFontSize=15, labelAnchor=&#39;start&#39;, labelFontWeight=&#39;bold&#39;) ), columns=3, ).configure_axis(grid=False, title=None).configure_axisX(orient=&#39;top&#39;, labelPadding=20, offset=-27).configure_view(strokeWidth=0).resolve_scale(x=&#39;independent&#39;).configure_header( titleFontSize=20, labelFontSize=14, titlePadding=50 ) .",
            "url": "https://armsp.github.io/covidviz/climate/emission/nyt/2020/06/26/CO2-Emissions.html",
            "relUrl": "/climate/emission/nyt/2020/06/26/CO2-Emissions.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Hot Spots",
            "content": "Today we will make the Hot Spots map from the article Coronavirus Map: Tracking the Global Outbreak that looks like the following - . import altair as alt import pandas as pd import geopandas as gpd alt.renderers.set_embed_options(actions=False) . RendererRegistry.enable(&#39;default&#39;) . #world = gpd.read_file(&quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip&quot;) world = gpd.read_file(&quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip&quot;) . world.head() . featurecla scalerank LABELRANK SOVEREIGNT SOV_A3 ADM0_DIF LEVEL TYPE ADMIN ADM0_A3 ... NAME_KO NAME_NL NAME_PL NAME_PT NAME_RU NAME_SV NAME_TR NAME_VI NAME_ZH geometry . 0 Admin-0 country | 1 | 6 | Fiji | FJI | 0 | 2 | Sovereign country | Fiji | FJI | ... | 피지 | Fiji | Fidżi | Fiji | Фиджи | Fiji | Fiji | Fiji | 斐濟 | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 Admin-0 country | 1 | 3 | United Republic of Tanzania | TZA | 0 | 2 | Sovereign country | United Republic of Tanzania | TZA | ... | 탄자니아 | Tanzania | Tanzania | Tanzânia | Танзания | Tanzania | Tanzanya | Tanzania | 坦桑尼亚 | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 Admin-0 country | 1 | 7 | Western Sahara | SAH | 0 | 2 | Indeterminate | Western Sahara | SAH | ... | 서사하라 | Westelijke Sahara | Sahara Zachodnia | Saara Ocidental | Западная Сахара | Västsahara | Batı Sahra | Tây Sahara | 西撒哈拉 | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 Admin-0 country | 1 | 2 | Canada | CAN | 0 | 2 | Sovereign country | Canada | CAN | ... | 캐나다 | Canada | Kanada | Canadá | Канада | Kanada | Kanada | Canada | 加拿大 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 Admin-0 country | 1 | 2 | United States of America | US1 | 1 | 2 | Country | United States of America | USA | ... | 미국 | Verenigde Staten van Amerika | Stany Zjednoczone | Estados Unidos | Соединённые Штаты Америки | USA | Amerika Birleşik Devletleri | Hoa Kỳ | 美国 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . 5 rows × 95 columns . world = world[[&#39;ADMIN&#39;, &#39;NAME&#39;, &#39;ISO_A3&#39;, &#39;CONTINENT&#39;, &#39;geometry&#39;]] world . ADMIN NAME ISO_A3 CONTINENT geometry . 0 Fiji | Fiji | FJI | Oceania | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 United Republic of Tanzania | Tanzania | TZA | Africa | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 Western Sahara | W. Sahara | ESH | Africa | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 Canada | Canada | CAN | North America | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 United States of America | United States of America | USA | North America | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . ... ... | ... | ... | ... | ... | . 172 Republic of Serbia | Serbia | SRB | Europe | POLYGON ((18.82982 45.90887, 18.82984 45.90888... | . 173 Montenegro | Montenegro | MNE | Europe | POLYGON ((20.07070 42.58863, 19.80161 42.50009... | . 174 Kosovo | Kosovo | -99 | Europe | POLYGON ((20.59025 41.85541, 20.52295 42.21787... | . 175 Trinidad and Tobago | Trinidad and Tobago | TTO | North America | POLYGON ((-61.68000 10.76000, -61.10500 10.890... | . 176 South Sudan | S. Sudan | SSD | Africa | POLYGON ((30.83385 3.50917, 29.95350 4.17370, ... | . 177 rows × 5 columns . world = world[~(world[&#39;CONTINENT&#39;]==&#39;Antarctica&#39;)] alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).encode() . somalia = world[world[&#39;NAME&#39;].str.contains(&#39;Somali&#39;)] somalia = somalia.dissolve(by=&#39;CONTINENT&#39;).reset_index() world=pd.concat([world, somalia]) . world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;eSwatini&#39;), &#39;ADMIN&#39;] = &#39;Eswatini&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;Palestine&#39;), &#39;ADMIN&#39;] = &#39;West Bank and Gaza&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;Republic of Serbia&#39;), &#39;ADMIN&#39;] = &#39;Serbia&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;United Republic of Tanzania&#39;), &#39;ADMIN&#39;] = &#39;Tanzania&#39; world.loc[world[&#39;ADMIN&#39;].str.contains(&#39;São Tomé and Principe&#39;), &#39;ADMIN&#39;] = &#39;Sao Tome and Principe&#39; . world.columns = [&#39;Country_Region&#39;, &#39;NAME&#39;, &#39;iso3&#39;, &#39;CONTINENT&#39;, &#39;geometry&#39;] . population_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv&#39; population_data = pd.read_csv(population_uri) population_data = population_data.drop([&#39;UID&#39;, &#39;iso2&#39;, &#39;code3&#39;, &#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;], axis=1) population_data = population_data[population_data[&#39;Country_Region&#39;] == population_data[&#39;Combined_Key&#39;]] population_data = population_data.drop([&#39;Country_Region&#39;, &#39;Combined_Key&#39;], axis=1) population_data = population_data.reset_index(drop=True) population_data.head() . iso3 Lat Long_ Population . 0 AFG | 33.93911 | 67.709953 | 38928341.0 | . 1 ALB | 41.15330 | 20.168300 | 2877800.0 | . 2 DZA | 28.03390 | 1.659600 | 43851043.0 | . 3 AND | 42.50630 | 1.521800 | 77265.0 | . 4 AGO | -11.20270 | 17.873900 | 32866268.0 | . population_data . iso3 Lat Long_ Population . 0 AFG | 33.939110 | 67.709953 | 3.892834e+07 | . 1 ALB | 41.153300 | 20.168300 | 2.877800e+06 | . 2 DZA | 28.033900 | 1.659600 | 4.385104e+07 | . 3 AND | 42.506300 | 1.521800 | 7.726500e+04 | . 4 AGO | -11.202700 | 17.873900 | 3.286627e+07 | . ... ... | ... | ... | ... | . 185 ZWE | -19.015438 | 29.154857 | 1.486293e+07 | . 186 AUS | -25.000000 | 133.000000 | 2.545970e+07 | . 187 CAN | 60.000000 | -95.000000 | 3.785570e+07 | . 188 CHN | 30.592800 | 114.305500 | 1.404676e+09 | . 189 USA | 40.000000 | -100.000000 | 3.294663e+08 | . 190 rows × 4 columns . uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; time_s_raw = pd.read_csv(uri) time_s_raw . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 10/22/20 10/23/20 10/24/20 10/25/20 10/26/20 10/27/20 10/28/20 10/29/20 10/30/20 10/31/20 . 0 NaN | Afghanistan | 33.939110 | 67.709953 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 40626 | 40687 | 40768 | 40833 | 40937 | 41032 | 41145 | 41268 | 41334 | 41425 | . 1 NaN | Albania | 41.153300 | 20.168300 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 18250 | 18556 | 18858 | 19157 | 19445 | 19729 | 20040 | 20315 | 20634 | 20875 | . 2 NaN | Algeria | 28.033900 | 1.659600 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 55357 | 55630 | 55880 | 56143 | 56419 | 56706 | 57026 | 57332 | 57651 | 57942 | . 3 NaN | Andorra | 42.506300 | 1.521800 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3811 | 4038 | 4038 | 4038 | 4325 | 4410 | 4517 | 4567 | 4665 | 4756 | . 4 NaN | Angola | -11.202700 | 17.873900 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 8582 | 8829 | 9026 | 9381 | 9644 | 9871 | 10074 | 10269 | 10558 | 10805 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 263 NaN | West Bank and Gaza | 31.952200 | 35.233200 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 49134 | 49579 | 49989 | 50442 | 50952 | 51528 | 51948 | 52571 | 53075 | 53520 | . 264 NaN | Western Sahara | 24.215500 | -12.885800 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | . 265 NaN | Yemen | 15.552727 | 48.516388 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2057 | 2060 | 2060 | 2060 | 2060 | 2060 | 2061 | 2062 | 2062 | 2063 | . 266 NaN | Zambia | -13.133897 | 27.849332 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 16035 | 16095 | 16117 | 16117 | 16200 | 16243 | 16285 | 16325 | 16415 | 16432 | . 267 NaN | Zimbabwe | -19.015438 | 29.154857 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 8242 | 8257 | 8269 | 8276 | 8303 | 8315 | 8320 | 8349 | 8362 | 8367 | . 268 rows × 288 columns . time_s = time_s_raw.groupby(&#39;Country/Region&#39;).agg(dict(zip(time_s_raw.columns[4:], [&#39;sum&#39;]*(len(time_s_raw.columns)-4)))) time_s = time_s.reset_index() time_s . Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 ... 10/22/20 10/23/20 10/24/20 10/25/20 10/26/20 10/27/20 10/28/20 10/29/20 10/30/20 10/31/20 . 0 Afghanistan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 40626 | 40687 | 40768 | 40833 | 40937 | 41032 | 41145 | 41268 | 41334 | 41425 | . 1 Albania | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 18250 | 18556 | 18858 | 19157 | 19445 | 19729 | 20040 | 20315 | 20634 | 20875 | . 2 Algeria | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 55357 | 55630 | 55880 | 56143 | 56419 | 56706 | 57026 | 57332 | 57651 | 57942 | . 3 Andorra | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3811 | 4038 | 4038 | 4038 | 4325 | 4410 | 4517 | 4567 | 4665 | 4756 | . 4 Angola | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 8582 | 8829 | 9026 | 9381 | 9644 | 9871 | 10074 | 10269 | 10558 | 10805 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 185 West Bank and Gaza | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 49134 | 49579 | 49989 | 50442 | 50952 | 51528 | 51948 | 52571 | 53075 | 53520 | . 186 Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | . 187 Yemen | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2057 | 2060 | 2060 | 2060 | 2060 | 2060 | 2061 | 2062 | 2062 | 2063 | . 188 Zambia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 16035 | 16095 | 16117 | 16117 | 16200 | 16243 | 16285 | 16325 | 16415 | 16432 | . 189 Zimbabwe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 8242 | 8257 | 8269 | 8276 | 8303 | 8315 | 8320 | 8349 | 8362 | 8367 | . 190 rows × 285 columns . time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Korea, South&#39;, &#39;Country/Region&#39;] = &#39;South Korea&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&quot;Cote d&#39;Ivoire&quot;, &#39;Country/Region&#39;] = &#39;Ivory Coast&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Timor-Leste&#39;, &#39;Country/Region&#39;] = &#39;East Timor&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Taiwan*&#39;, &#39;Country/Region&#39;] = &#39;Taiwan&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Burma&#39;, &#39;Country/Region&#39;] = &#39;Myanmar&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;US&#39;, &#39;Country/Region&#39;] = &#39;United States of America&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Czech Republic&#39;, &#39;Country/Region&#39;] = &#39;Czechia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;North Macedonia&#39;, &#39;Country/Region&#39;] = &#39;Macedonia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Bahamas&#39;, &#39;Country/Region&#39;] = &#39;The Bahamas&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country/Region&#39;] = &#39;Democratic Republic of the Congo&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country/Region&#39;] = &#39;Republic of the Congo&#39; . time_s_T = time_s.set_index(&#39;Country/Region&#39;).T time_s_T . Country/Region Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia Austria ... United Kingdom Uruguay Uzbekistan Venezuela Vietnam West Bank and Gaza Western Sahara Yemen Zambia Zimbabwe . 1/22/20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1/23/20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | . 1/24/20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | . 1/25/20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | . 1/26/20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | ... | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 10/27/20 41032 | 19729 | 56706 | 4410 | 9871 | 124 | 1116609 | 80410 | 27553 | 86102 | ... | 920664 | 2916 | 65881 | 90400 | 1172 | 51528 | 10 | 2060 | 16243 | 8315 | . 10/28/20 41145 | 20040 | 57026 | 4517 | 10074 | 124 | 1130533 | 82651 | 27565 | 89496 | ... | 945378 | 2981 | 66141 | 90876 | 1173 | 51948 | 10 | 2061 | 16285 | 8320 | . 10/29/20 41268 | 20315 | 57332 | 4567 | 10269 | 124 | 1143800 | 85034 | 27579 | 93949 | ... | 968456 | 3044 | 66392 | 91280 | 1177 | 52571 | 10 | 2062 | 16325 | 8349 | . 10/30/20 41334 | 20634 | 57651 | 4665 | 10558 | 127 | 1157179 | 87432 | 27585 | 99576 | ... | 992874 | 3082 | 66628 | 91589 | 1177 | 53075 | 10 | 2062 | 16415 | 8362 | . 10/31/20 41425 | 20875 | 57942 | 4756 | 10805 | 128 | 1166924 | 89813 | 27595 | 104925 | ... | 1014793 | 3124 | 66932 | 92013 | 1180 | 53520 | 10 | 2063 | 16432 | 8367 | . 284 rows × 190 columns . time_s_T = time_s_T.apply(lambda x: x.diff(), axis=0) . time_s_T . Country/Region Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia Austria ... United Kingdom Uruguay Uzbekistan Venezuela Vietnam West Bank and Gaza Western Sahara Yemen Zambia Zimbabwe . 1/22/20 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1/23/20 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1/24/20 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1/25/20 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1/26/20 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 10/27/20 95.0 | 284.0 | 287.0 | 85.0 | 227.0 | 0.0 | 14308.0 | 1600.0 | 14.0 | 2835.0 | ... | 22924.0 | 44.0 | 214.0 | 353.0 | 3.0 | 576.0 | 0.0 | 0.0 | 43.0 | 12.0 | . 10/28/20 113.0 | 311.0 | 320.0 | 107.0 | 203.0 | 0.0 | 13924.0 | 2241.0 | 12.0 | 3394.0 | ... | 24714.0 | 65.0 | 260.0 | 476.0 | 1.0 | 420.0 | 0.0 | 1.0 | 42.0 | 5.0 | . 10/29/20 123.0 | 275.0 | 306.0 | 50.0 | 195.0 | 0.0 | 13267.0 | 2383.0 | 14.0 | 4453.0 | ... | 23078.0 | 63.0 | 251.0 | 404.0 | 4.0 | 623.0 | 0.0 | 1.0 | 40.0 | 29.0 | . 10/30/20 66.0 | 319.0 | 319.0 | 98.0 | 289.0 | 3.0 | 13379.0 | 2398.0 | 6.0 | 5627.0 | ... | 24418.0 | 38.0 | 236.0 | 309.0 | 0.0 | 504.0 | 0.0 | 0.0 | 90.0 | 13.0 | . 10/31/20 91.0 | 241.0 | 291.0 | 91.0 | 247.0 | 1.0 | 9745.0 | 2381.0 | 10.0 | 5349.0 | ... | 21919.0 | 42.0 | 304.0 | 424.0 | 3.0 | 445.0 | 0.0 | 1.0 | 17.0 | 5.0 | . 284 rows × 190 columns . Just get the sum of last 7 days - that&#39;s all you need per country. . hot_spots = time_s_T.tail(7).sum().to_frame(name=&quot;past_7_days&quot;).reset_index().rename(columns={&#39;Country/Region&#39;: &#39;Country_Region&#39;}) hot_spots.past_7_days = hot_spots.past_7_days/7 hot_spots . Country_Region past_7_days . 0 Afghanistan | 93.857143 | . 1 Albania | 288.142857 | . 2 Algeria | 294.571429 | . 3 Andorra | 102.571429 | . 4 Angola | 254.142857 | . ... ... | ... | . 185 West Bank and Gaza | 504.428571 | . 186 Western Sahara | 0.000000 | . 187 Yemen | 0.428571 | . 188 Zambia | 45.000000 | . 189 Zimbabwe | 14.000000 | . 190 rows × 2 columns . world = world.merge(hot_spots, on=&#39;Country_Region&#39;, how=&#39;inner&#39;) world.info() . &lt;class &#39;geopandas.geodataframe.GeoDataFrame&#39;&gt; Int64Index: 167 entries, 0 to 166 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 Country_Region 167 non-null object 1 NAME 167 non-null object 2 iso3 167 non-null object 3 CONTINENT 167 non-null object 4 geometry 167 non-null geometry 5 past_7_days 167 non-null float64 dtypes: float64(1), geometry(1), object(4) memory usage: 9.1+ KB . world = world.merge(population_data, on=&#39;iso3&#39;, how=&#39;left&#39;) world . Country_Region NAME iso3 CONTINENT geometry past_7_days Lat Long_ Population . 0 Fiji | Fiji | FJI | Oceania | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | 0.142857 | -17.713400 | 178.065000 | 896444.0 | . 1 Tanzania | Tanzania | TZA | Africa | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | 0.000000 | -6.369028 | 34.888822 | 59734213.0 | . 2 Western Sahara | W. Sahara | ESH | Africa | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | 0.000000 | 24.215500 | -12.885800 | 597330.0 | . 3 Canada | Canada | CAN | North America | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | 3037.428571 | 60.000000 | -95.000000 | 37855702.0 | . 4 United States of America | United States of America | USA | North America | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | 78380.571429 | 40.000000 | -100.000000 | 329466283.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 162 Serbia | Serbia | SRB | Europe | POLYGON ((18.82982 45.90887, 18.82984 45.90888... | 1154.571429 | 44.016500 | 21.005900 | 8737370.0 | . 163 Montenegro | Montenegro | MNE | Europe | POLYGON ((20.07070 42.58863, 19.80161 42.50009... | 244.571429 | 42.708678 | 19.374390 | 628062.0 | . 164 Kosovo | Kosovo | -99 | Europe | POLYGON ((20.59025 41.85541, 20.52295 42.21787... | 224.428571 | NaN | NaN | NaN | . 165 Trinidad and Tobago | Trinidad and Tobago | TTO | North America | POLYGON ((-61.68000 10.76000, -61.10500 10.890... | 27.000000 | 10.691800 | -61.222500 | 1399491.0 | . 166 South Sudan | S. Sudan | SSD | Africa | POLYGON ((30.83385 3.50917, 29.95350 4.17370, ... | 3.857143 | 6.877000 | 31.307000 | 11193729.0 | . 167 rows × 9 columns . world = world.assign(average_daily_per_capita = (world.past_7_days/world.Population)*100000) world . Country_Region NAME iso3 CONTINENT geometry past_7_days Lat Long_ Population average_daily_per_capita . 0 Fiji | Fiji | FJI | Oceania | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | 0.142857 | -17.713400 | 178.065000 | 896444.0 | 0.015936 | . 1 Tanzania | Tanzania | TZA | Africa | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | 0.000000 | -6.369028 | 34.888822 | 59734213.0 | 0.000000 | . 2 Western Sahara | W. Sahara | ESH | Africa | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | 0.000000 | 24.215500 | -12.885800 | 597330.0 | 0.000000 | . 3 Canada | Canada | CAN | North America | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | 3037.428571 | 60.000000 | -95.000000 | 37855702.0 | 8.023702 | . 4 United States of America | United States of America | USA | North America | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | 78380.571429 | 40.000000 | -100.000000 | 329466283.0 | 23.790165 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 162 Serbia | Serbia | SRB | Europe | POLYGON ((18.82982 45.90887, 18.82984 45.90888... | 1154.571429 | 44.016500 | 21.005900 | 8737370.0 | 13.214176 | . 163 Montenegro | Montenegro | MNE | Europe | POLYGON ((20.07070 42.58863, 19.80161 42.50009... | 244.571429 | 42.708678 | 19.374390 | 628062.0 | 38.940651 | . 164 Kosovo | Kosovo | -99 | Europe | POLYGON ((20.59025 41.85541, 20.52295 42.21787... | 224.428571 | NaN | NaN | NaN | NaN | . 165 Trinidad and Tobago | Trinidad and Tobago | TTO | North America | POLYGON ((-61.68000 10.76000, -61.10500 10.890... | 27.000000 | 10.691800 | -61.222500 | 1399491.0 | 1.929273 | . 166 South Sudan | S. Sudan | SSD | Africa | POLYGON ((30.83385 3.50917, 29.95350 4.17370, ... | 3.857143 | 6.877000 | 31.307000 | 11193729.0 | 0.034458 | . 167 rows × 10 columns . alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).encode( color=alt.Color(&#39;average_daily_per_capita:Q&#39;), tooltip = [&#39;Country_Region:N&#39;, &#39;average_daily_per_capita:Q&#39;] ).properties(width=1400, height=800).project(&#39;equalEarth&#39;).configure_view(strokeWidth=0) . world = world.assign(Average_Daily_Cases = world.average_daily_per_capita.apply(lambda x: 16 if x&gt;14 else x)) . color_scale = [&#39;#f2df91&#39;, &#39;#f9c467&#39;, &#39;#ffa83e&#39;, &#39;#ff8b24&#39;, &#39;#fd6a0b&#39;, &#39;#f04f09&#39;, &#39;#e13107&#39;, &#39;#ce0a05&#39;] grey = &quot;#f2f2f2&quot; world_map = alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).encode( color=alt.Color(&#39;Average_Daily_Cases:Q&#39;, scale=alt.Scale(range=color_scale), bin=alt.Bin(base=2, step=2, extent=[0,16])), tooltip = [&#39;Country_Region:N&#39;, &#39;average_daily_per_capita:Q&#39;] ).properties(width=1400, height=600).project(&#39;equalEarth&#39;) world_map.configure_view(strokeWidth=0) . This is a custom color scale. Exact results with countries that do not publish data can only be reached when we scale color using a color field. Otherwise this is the best we can do. .",
            "url": "https://armsp.github.io/covidviz/geospatial/chloropleth/nyt/2020/06/15/World_Hot_Spots.html",
            "relUrl": "/geospatial/chloropleth/nyt/2020/06/15/World_Hot_Spots.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Geospatial plot of cases in US",
            "content": "Today we will make our first geospatial map from the article Coronavirus in the U.S.: Latest Map and Case Count which looks like the folowing - . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in county data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;]).sum() . fips cases deaths . county . Joplin 0.0 | 329 | 4 | . Kansas City 0.0 | 85094 | 1700 | . New York City 0.0 | 15615980 | 1528538 | . Unknown 0.0 | 885104 | 41064 | . #hide_output cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;, &#39;state&#39;]).sum() . NYT publishes the data for New York City in a different way by combining the results of the 5 boroughs that comprise it. So we will combine them too and add a new row in the dataset with a custom fips of 1. Let&#39;s start by making this change in the raw NYT dataset itself. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;] . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | . 262876 2020-06-23 | New York City | New York | 1.0 | 217803 | 21817 | . 265930 2020-06-24 | New York City | New York | 1.0 | 218089 | 21838 | . 268988 2020-06-25 | New York City | New York | 1.0 | 218429 | 21856 | . 272054 2020-06-26 | New York City | New York | 1.0 | 218799 | 21893 | . 275123 2020-06-27 | New York City | New York | 1.0 | 219157 | 21913 | . 119 rows × 6 columns . # collapse latest_cases = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) latest_cases . . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . 1 1001.0 | Autauga | 2020-06-27 | Alabama | 498 | 12 | . 2 1003.0 | Baldwin | 2020-06-27 | Alabama | 555 | 10 | . 3 1005.0 | Barbour | 2020-06-27 | Alabama | 317 | 1 | . 4 1007.0 | Bibb | 2020-06-27 | Alabama | 161 | 1 | . ... ... | ... | ... | ... | ... | ... | . 3038 56037.0 | Sweetwater | 2020-06-27 | Wyoming | 81 | 0 | . 3039 56039.0 | Teton | 2020-06-27 | Wyoming | 119 | 1 | . 3040 56041.0 | Uinta | 2020-06-27 | Wyoming | 167 | 0 | . 3041 56043.0 | Washakie | 2020-06-27 | Wyoming | 38 | 5 | . 3042 56045.0 | Weston | 2020-06-27 | Wyoming | 1 | 0 | . 3043 rows × 6 columns . Now we have to make the changes in our shapefile too. For that we need to **dissolve** the 5 buroughs into one single geospatial entity. . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . agg_nyc_data . STATEFP geometry COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER lon lat fips . 0 36 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | -73.927011 | 40.695278 | 1 | . # hide_output county_nyc = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county_nyc[&#39;fips&#39;] = county_nyc[&#39;GEOID&#39;] county_nyc[&#39;fips&#39;] = county_nyc[&#39;fips&#39;].astype(&#39;int&#39;) county_nyc # generate FIPS in the shapefile itself by combining STATEFP and COUNTYFP #county2[&#39;STATEFP&#39;] + county2[&#39;COUNTYFP&#39;] #latest_cases[&#39;fips&#39;] = latest_cases[&#39;fips&#39;].astype(&#39;int&#39;) . latest_cases[&#39;fips&#39;].isin(county_nyc[&#39;fips&#39;]).value_counts() . True 3043 Name: fips, dtype: int64 . latest_cases[latest_cases[&#39;county&#39;] == &#39;New York City&#39;] . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . county_nyc[county_nyc[&#39;fips&#39;] == 1] . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 3220 36 | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | -73.927011 | 40.695278 | 1 | . # collapse latest_cases_w_fips = county_nyc.merge(latest_cases, how=&#39;left&#39;, on=&#39;fips&#39;) circle_selection = alt.selection_single(on=&#39;mouseover&#39;, empty=&#39;none&#39;) circles = alt.Chart(latest_cases_w_fips).mark_point(fillOpacity=0.2, fill=&#39;red&#39;, strokeOpacity=1, color=&#39;red&#39;, strokeWidth=1).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, size=alt.Size(&#39;cases:Q&#39;, scale=alt.Scale(domain=[0, 7000],),legend=alt.Legend(title=&quot;Cases&quot;)), tooltip=[&#39;county:N&#39;, &#39;cases:Q&#39;, &#39;deaths:Q&#39;], color = alt.condition(circle_selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1000, height=700 ).add_selection( circle_selection ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) state_text = state.mark_text().transform_filter(alt.datum.NAME != &#39;Puerto Rico&#39;).encode( longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;, text=&#39;NAME&#39;, ).project( type=&#39;albersUsa&#39; ) . . (state+circles+state_text).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/interactive/nyt/2020/06/12/US-case-counts-geospatial.html",
            "relUrl": "/geospatial/interactive/nyt/2020/06/12/US-case-counts-geospatial.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Deaths above or below normal",
            "content": "We will make charts from the NYT article on What Is the Real Coronavirus Death Toll in Each State? . Initially the charts used to look like these - Since then they have corrected/changed them to the following - . Sources of the datasets used - . CSV Format of National and State Estimates of Excess Deaths from: https://www.cdc.gov/nchs/nvss/vsrr/covid19/excess_deaths.htm | . OR . Export from: https://data.cdc.gov/NCHS/Excess-Deaths-Associated-with-COVID-19/xkkf-xrst/ | . Whats the purpose of this visualization? . Comparing recent totals of deaths from all causes can provide a more complete picture of the pandemic’s impact than tracking only deaths of people with confirmed diagnoses. Epidemiologists refer to fatalities in the gap between the observed and normal numbers of deaths as “excess deaths.” . Indeed, in nearly every state with an unusual number of deaths in recent weeks, that number is higher than the state’s reported number of deaths from Covid-19. On our charts, we have marked the number of official coronavirus deaths with red lines, so you can see how they match up with the total number of excess deaths. . Measuring excess deaths is crude because it does not capture all the details of how people died. But many epidemiologists believe it is the best way to measure the impact of the virus in real time. It shows how the virus is altering normal patterns of mortality where it strikes and undermines arguments that it is merely killing vulnerable people who would have died anyway. . Public health researchers use such methods to measure the impact of catastrophic events when official measures of mortality are flawed. . Measuring excess deaths does not tell us precisely how each person died. It is likely that most of the excess deaths in this period are because of the coronavirus itself, given the dangerousness of the virus and the well-documented problems with testing. But it is also possible that deaths from other causes have risen too, as hospitals have become stressed and people have been scared to seek care for ailments that are typically survivable. Some causes of death may be declining, as people stay inside more, drive less and limit their contact with others. . First we chart the excess deaths. Excess deaths is calculated as the difference b/w all cause mortality data with average expected deaths for the week. These data are available from CDC as mentioned in the Sources section above. . import pandas as pd import numpy as np import altair as alt alt.renderers.set_embed_options(actions=False) . RendererRegistry.enable(&#39;default&#39;) . uri = &#39;https://data.cdc.gov/api/views/xkkf-xrst/rows.csv?accessType=DOWNLOAD&amp;bom=true&amp;format=true%20target=&#39; data = pd.read_csv(uri) data.head() . Week Ending Date State Observed Number Upper Bound Threshold Exceeds Threshold Average Expected Count Excess Lower Estimate Excess Higher Estimate Year Total Excess Lower Estimate in 2020 Total Excess Higher Estimate in 2020 Percent Excess Lower Estimate Percent Excess Higher Estimate Type Outcome Suppress Note . 0 2017-01-14 | Alabama | 1130.0 | 1188.0 | False | 1029.0 | 0.0 | 101.0 | 2017 | 3582 | 5579 | 0.0 | 0.1 | Predicted (weighted) | All causes | NaN | NaN | . 1 2017-01-21 | Alabama | 1048.0 | 1201.0 | False | 1042.0 | 0.0 | 6.0 | 2017 | 3582 | 5579 | 0.0 | 0.0 | Predicted (weighted) | All causes | NaN | NaN | . 2 2017-01-28 | Alabama | 1026.0 | 1216.0 | False | 1057.0 | 0.0 | 0.0 | 2017 | 3582 | 5579 | 0.0 | 0.0 | Predicted (weighted) | All causes | NaN | NaN | . 3 2017-02-04 | Alabama | 1036.0 | 1216.0 | False | 1057.0 | 0.0 | 0.0 | 2017 | 3582 | 5579 | 0.0 | 0.0 | Predicted (weighted) | All causes | NaN | NaN | . 4 2017-02-11 | Alabama | 1058.0 | 1207.0 | False | 1053.0 | 0.0 | 5.0 | 2017 | 3582 | 5579 | 0.0 | 0.0 | Predicted (weighted) | All causes | NaN | NaN | . Extracting data for NYC for year 2020 - . nyc = data[(data[&#39;State&#39;] == &#39;New York City&#39;) &amp; (data[&#39;Year&#39;] == 2020)] . nyc.Outcome.value_counts() . All causes 82 All causes, excluding COVID-19 41 Name: Outcome, dtype: int64 . nyc.Type.value_counts() . Predicted (weighted) 82 Unweighted 41 Name: Type, dtype: int64 . We need the &quot;All causes&quot; &quot;Predicted (weighted)&quot; data. We can either filter it in pandas or do it from Altair itself. For now we are going with Altair. . Calculating excess deaths - . nyc = nyc.assign(excess = nyc[&#39;Observed Number&#39;] - nyc[&#39;Average Expected Count&#39;]) . alt.Chart(nyc).mark_bar().transform_filter(alt.datum.Type==&#39;Predicted (weighted)&#39;).transform_filter(alt.datum.Outcome==&#39;All causes&#39;).encode( y=&#39;excess:Q&#39;, x=&#39;Week Ending Date:T&#39; ) . Let&#39;s beautify it and color code the positive and negative numbers differently - . #collapse bars = alt.Chart(nyc, height=600).mark_bar(width=9).transform_filter(alt.datum.Type==&#39;Predicted (weighted)&#39;).transform_filter(alt.datum.Outcome==&#39;All causes&#39;).encode( x=alt.X(&#39;Week Ending Date:T&#39;, title=None, axis=alt.Axis(grid=False, domain=False,format=&quot;%b&quot;)), y=alt.Y(&#39;excess:Q&#39;, title=None, axis=alt.Axis(domain=False, labelPadding=-50, position=-10, ticks=False, zindex=1, values=list(range(500,7500,500)))), color = alt.condition(alt.datum.excess&gt;0, alt.value(&#39;#ffab00&#39;), alt.value(&#39;#8FB8BB&#39;)) ).properties(width=alt.Step(10)).configure_view(stroke=None) . . bars . For the highlighted grey rectangle we make it like the following rectangle chart and then layer it behind our bar chart. . source = pd.DataFrame([{&#39;start&#39;: &#39;2020-03-15&#39;, &#39;end&#39;: &#39;2020-10-10&#39;, &#39;y2&#39;: 7000, &#39;y&#39;: -100}]) rect = alt.Chart(source).mark_rect(opacity=1, fill=&#39;#eee&#39;, xOffset=5, x2Offset=5).encode( x=&#39;start:T&#39;, x2=&#39;end:T&#39;, y2=&#39;y2:Q&#39;, y=&#39;y:Q&#39; ) . # collapse bars = alt.Chart(nyc, height=1600, width=225).mark_bar(width=5).transform_filter(alt.datum.Type==&#39;Predicted (weighted)&#39;).transform_filter(alt.datum.Outcome==&#39;All causes&#39;).encode( x=alt.X(&#39;Week Ending Date:T&#39;, title=None, axis=alt.Axis(grid=False, offset=23, domain=False, format=&quot;%b&quot;, tickCount=4)), y=alt.Y(&#39;excess:Q&#39;, title=None, scale=alt.Scale(domain=[0, 7000]), axis=alt.Axis(domain=False, labelPadding=-25, position=-10, ticks=False, zindex=1, values=list(range(500,7500,500)))), color = alt.condition(alt.datum.excess&gt;0, alt.value(&#39;#ffab00&#39;), alt.value(&#39;#8FB8BB&#39;)) ) . . (rect+bars).configure_view(stroke=None) .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/06/04/Above-Below-Normal.html",
            "relUrl": "/nyt/2020/06/04/Above-Below-Normal.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Tracking the Global Outbreak: Growth Rates",
            "content": "Today we will make the growth rate charts from the NYT article on Tracking the Global Outbreak for all the countries. . . We will use the JHU CSSE dataset since NYT does not provide its own global countries dataset . #hide_output import pandas as pd import altair as alt raw_data_url = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; raw_df = pd.read_csv(raw_data_url) alt.renderers.set_embed_options(actions=False) . A few important observations - . There are some countries that have data at a finer level like state/county. For those countires we will extract the extract the corresponding rows, sum them up into a single row and transpose it to convert to a dataframe. | Those exceptional countries are &#39;Australia&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Denmark&#39;, &#39;France&#39;, &#39;Netherlands&#39;, &#39;United Kingdom&#39; | . Let&#39;s convert the data into the desired long form from the existing wide form. . # collapse long_df = pd.DataFrame() exceptional_countries = [&#39;Australia&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Denmark&#39;, &#39;France&#39;, &#39;Netherlands&#39;, &#39;United Kingdom&#39;] country_df_list = [] def extract_country(s): if s[1].strip() not in exceptional_countries: # print(f&quot;{s[1]} - exceptional case&quot;) # else: temp_df = pd.DataFrame(s[4:]) temp_df.columns = [&#39;value&#39;] temp_df[&#39;country&#39;] = s[1] temp_df[&#39;growth&#39;] = temp_df[&#39;value&#39;].diff() temp_df[&#39;growth&#39;][0] = temp_df[&#39;value&#39;].iloc[0] temp_df[&#39;growth&#39;] = temp_df[&#39;growth&#39;].astype(int) temp_df = temp_df.rename_axis(&#39;date&#39;).reset_index() country_df_list.append(temp_df) for country in exceptional_countries: temp_df = pd.DataFrame(raw_df[raw_df[&#39;Country/Region&#39;] == country].iloc[:,4:].sum(axis=0).rename_axis(&#39;date&#39;).reset_index()) temp_df.columns = [&#39;date&#39;,&#39;value&#39;] temp_df[&#39;country&#39;] = country temp_df[&#39;growth&#39;] = temp_df[&#39;value&#39;].diff() temp_df.loc[0, &#39;growth&#39;] = temp_df[&#39;value&#39;].iloc[0] temp_df[&#39;growth&#39;] = temp_df[&#39;growth&#39;].astype(int) country_df_list.append(temp_df) raw_df.apply(extract_country, axis=1) long_df = pd.concat(country_df_list) . . Beacause this is a large dataset, Altair will - by default - refuse to display because of possible memory issues. So we will have to enable the json transformer so that the data is passed insternally as a url. We enable it using alt.data_transformers.enable(&#39;json&#39;). Do this if you are running it locally. To do the same on Fastpages, I have already uploaded the json file that is generated by Altair behind the scenes and and I will pass the url of the file to the chart so that the output visualization is seen on the website. . # collapse #alt.data_transformers.enable(&#39;json&#39;) # use this if running locally #alt.data_transformers.disable_max_rows() # avoid this as it can hang your system url = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/2020-06-02-Data.json&#39; #comment this when running locally otherwise you will have old data till 1st June only a = alt.Chart().mark_bar(size=2, opacity=0.2, color=&#39;gray&#39;).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None), y=alt.Y(&quot;growth:Q&quot;, title=None), ).properties(width=90, height=100) b = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.4).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&quot;rolling_mean:Q&quot;,title=&#39;cases&#39;) ) c = b.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 2}) alt.layer(a, c, data=url).facet(alt.Column(&#39;country:N&#39;, title=None, sort=alt.EncodingSortField(&#39;value&#39;, op=&#39;max&#39;, order=&#39;descending&#39;), header=alt.Header(labelFontSize=13, labelColor=&#39;gray&#39;, labelFontWeight=&#39;bolder&#39;, labelAlign=&#39;center&#39;, labelAnchor=&#39;middle&#39;, labelOrient=&#39;top&#39;, labelPadding=-15, labelAngle=0)), spacing=alt.RowColnumber(row=70, column=0), title=&quot;Countrywise Distribution of Growth, Averaged over 7 days&quot;, columns=7, ).configure_axis( grid=False, #domainWidth=0.1 ).configure_view(strokeWidth=0).configure_title( fontSize=25, font=&#39;Courier&#39;, anchor=&#39;middle&#39;, color=&#39;gray&#39;, dy=-30 ) . . There are a few issues with the above chart - . We are seeing negative values in growth rate, How can it be negative? The lowest it can go is 0. | The graphs for countries with very few cases don&#39;t look good. | The scales of the countries vary a lot. We need to adjust the scale like NYT does, to make it readable. | . Let&#39;s improve upon the issues above with the following solutions - . NYT does not show graphs for those with fewer than 100 cases. Like NYT we have filtered countries with less than 100 cases | The growth rates will be negative if there are discrepancies in the data - when the cumulative cases drop for any reason than the previous day. You will certainly notice thos in the dataset for some of the countries. | We will filter the negative values as based on my observation that&#39;s what NYT seems to be doing | Independednt Y axis | . Few points to keep in mind - . The bar chart shows the increment in cases per day | The line chart is the 7 day average of growth in cases per day | The facet is ordered in descending order by the maximum number of cases | We will forcefully align the countries one below the other because choosing independent axes often leads to misaligned facet items | . # collapse a = alt.Chart().mark_bar(size=2, opacity=0.05, color=&#39;red&#39;).transform_filter( alt.datum.growth &gt;= 0).transform_filter(alt.datum.value &gt; 100).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None), y=alt.Y(&quot;growth:Q&quot;, title=None), ).properties(width=90, height=100) b = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.04).transform_filter( alt.datum.growth &gt;= 0).transform_filter(alt.datum.value &gt; 100).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&quot;rolling_mean:Q&quot;,title=&#39;cases&#39;) ) c = b.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 2}) alt.layer(a, b, c, data=url).facet(alt.Column(&#39;country:N&#39;, title=None, sort=alt.EncodingSortField(&#39;value&#39;, op=&#39;max&#39;, order=&#39;descending&#39;), header=alt.Header(labelFontSize=13, labelColor=&#39;gray&#39;, labelFontWeight=&#39;bolder&#39;, labelAlign=&#39;center&#39;, labelAnchor=&#39;middle&#39;, labelOrient=&#39;top&#39;, labelPadding=-15, labelAngle=0)), spacing=alt.RowColnumber(row=70, column=5), title=&quot;Countrywise Distribution of Growth, Averaged over 7 days&quot;, columns=7, align=&#39;each&#39;, ).resolve_scale(y=&#39;independent&#39;, x=&#39;independent&#39;,).configure_axis( grid=False, #domainWidth=0.1 ).configure_view(strokeWidth=0).configure_title( fontSize=25, font=&#39;Courier&#39;, anchor=&#39;middle&#39;, color=&#39;gray&#39;, dy=-30 ) . . Feel free to comment below if you didn&#39;t understand anything and I will try my best to answer .",
            "url": "https://armsp.github.io/covidviz/nyt/facet/2020/06/02/World-Map-growth.html",
            "relUrl": "/nyt/facet/2020/06/02/World-Map-growth.html",
            "date": " • Jun 2, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Missing Deaths",
            "content": "Today we will make the following graph that shows the excess deaths as appeared in the article 87,000 Missing Deaths: Tracking the True Toll of the Coronavirus Outbreak . . Fortunately the NYT provides the dataset for this in their repository. . What does Excess Death mean and how do we calculate it? . Excess deaths are estimates that include deaths from Covid-19 and other causes. Reported Covid-19 deaths reflect official coronavirus deaths during the period when all-cause mortality data is available, including figures that were later revised. . According to the github repository - . Official Covid-19 death tolls offer a limited view of the impact of the outbreak because they often exclude people who have not been tested and those who died at home. All-cause mortality is widely used by demographers and other researchers to understand the full impact of deadly events, including epidemics, wars and natural disasters. The totals in this data include deaths from Covid-19 as well as those from other causes, likely including people who could not be treated or did not seek treatment for other conditions. . . Expected Deaths . We have calculated an average number of expected deaths for each area based on historical data for the same time of year. These expected deaths are the basis for our excess death calculations, which estimate how many more people have died this year than in an average year. . The number of years used in the historical averages changes depending on what data is available, whether it is reliable and underlying demographic changes. The baselines do not adjust for changes in age or other demographics, and they do not account for changes in total population. . The number of expected deaths are not adjusted for how non-Covid-19 deaths may change during the outbreak, which will take some time to figure out. As countries impose control measures, deaths from causes like road accidents and homicides may decline. And people who die from Covid-19 cannot die later from other causes, which may reduce other causes of death. Both of these factors, if they play a role, would lead these baselines to understate, rather than overstate, the number of excess deaths. . That is what we are going to do, average the results based on the baseline field to show the blue line for expected deaths. However it also looks like they are using some sort of linear model and smoothing as mentioned in the accompanying news article - . To estimate expected deaths, we fit a linear model to reported deaths in each country from 2015 to January 2020. The model has two components — a linear time trend to account for demographic changes and a smoothing spline to account for seasonal variation. For countries limited to monthly data, the model includes month as a fixed effect rather than using a smoothing spline. . Since there isn&#39;t much information on that we will ignore it for the time being. . What&#39;s the insight that this data gives? . These numbers undermine the notion that many people who have died from the virus may soon have died anyway. In Britain, which has recorded more Covid-19 deaths than any country except the United States, 59,000 more people than usual have died since mid-March — and about 14,000 more than have been captured by official death statistics. . import pandas as pd import altair as alt alt.renderers.set_embed_options(actions=False) url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/excess-deaths/deaths.csv&quot; raw = pd.read_csv(url) . Lets study Sweden, Switzerland, UK and France for our charts . sweden = raw[raw[&#39;country&#39;] == &quot;Sweden&quot;] switzerland = raw[raw[&#39;country&#39;] == &quot;Switzerland&quot;] uk = raw[raw[&#39;country&#39;] == &quot;United Kingdom&quot;] france = raw[raw[&#39;country&#39;] == &quot;France&quot;] . Let&#39;s start with a simple layered chart - area for year 2019 and line for 2020. We will not average anything right now nor will we use all the fields in our dataset. . base = alt.Chart(sweden).encode( x=alt.X(&#39;week&#39;) ) alt.layer( base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=500) . For Sweden they plot the gray lines for years 2015 to 2019. The blue line is the weekly average per year and the maroon line is the deaths in 2020. . # collapse base = alt.Chart(sweden).encode( x=&#39;week&#39;, ).properties(height=200) lines = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ).properties(width=500) avg + lines . . Looks like we capture the trend pretty well . Similarly for Switzerland, we will also turn off the grid and the view box - . # collapse base = alt.Chart(switzerland).encode( x=&#39;week&#39;, ).properties(height=300, width=500) lines = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) (avg+lines).configure_view(strokeWidth=0).configure_axis(grid=False) . . Trying the same for U.K - . # collapse base = alt.Chart(uk).encode( x=&#39;week&#39;, ).properties(height=300, width=550) l = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) rule = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) (rule+l).configure_view(strokeWidth=0).configure_axis(grid=False) . . Let&#39;s make use of loops to do the same but for France (based on observation it looks like the gray lines are from 2015 to 2019) - . # collapse base = alt.Chart(france).encode( x=&#39;week&#39;, ).properties(height=300, width=550) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) layer = [] for year in range(2015, 2021): l = base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) if year == 2020: l = base.mark_line(color=&#39;maroon&#39;).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) layer.append(l) alt.layer(avg,*layer).configure_view(strokeWidth=0).configure_axis(grid=False) . . The excess deaths articles and graphs update frequently and the graphics also changes quite a bit - . . In the latest versions of the charts they started using dashed lines, for that we will use strokeDash = alt.value([3,3]) . # collapse base = alt.Chart(france).encode( x=&#39;week&#39;, ).properties(height=300, width=550) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, strokeDash=[1,2], fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, strokeDash = alt.value([3,3]) ) layer = [] for year in range(2015, 2021): l = base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) if year == 2020: l = base.mark_line(color=&#39;maroon&#39;).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) layer.append(l) alt.layer(avg,*layer).configure_view(strokeWidth=0).configure_axis(grid=False) . .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/06/01/Excess-Deaths.html",
            "relUrl": "/nyt/2020/06/01/Excess-Deaths.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Covid Death Rates Graph",
            "content": "The graph that we will learn to make today is from the NYT article on Comparing Coronavirus Death Rates Across the U.S . . import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv&quot; usdf = pd.read_csv(url) nydf = usdf[(usdf[&#39;state&#39;] == &quot;New York&quot;) &amp; (usdf[&#39;date&#39;] &lt; &#39;2020-04-23&#39;)] nydf[&#39;deaths_perday&#39;] = nydf[&#39;deaths&#39;].diff() chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) chart . To highlight the region after the Stay at Home order date, we will use a Rectangle Chart - mark_react() We will also make a new source data that contains the starting date of the stay at home order and the perhaps the end date of stay at home or just the latest data in the dataset. NYT has not updated this graph and has data only till 21st or 22nd April. . # collapse chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700) . . This is the important piece of code on how to set up the Rectangle chart . source2 = [{ &quot;start&quot;: &quot;2020-03-23&quot;, &quot;end&quot;: nydf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . Similarly you can do the same for any other state. Lets try Michigan for a change - . michdf = usdf[(usdf[&#39;state&#39;] == &quot;Michigan&quot;) &amp; (usdf[&#39;date&#39;] &lt; &#39;2020-04-23&#39;)] michdf[&#39;deaths_perday&#39;] = michdf[&#39;deaths&#39;].diff() . # collapse chart = alt.Chart(michdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=150, width=700) source2 = [{ &quot;start&quot;: &quot;2020-03-25&quot;, &quot;end&quot;: michdf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . . The visualization till the latest date for NY would look like the following - . # collapse nydf = usdf[(usdf[&#39;state&#39;] == &quot;New York&quot;)] nydf[&#39;deaths_perday&#39;] = nydf[&#39;deaths&#39;].diff() nydf = nydf[nydf[&#39;date&#39;] &lt;= &#39;2020-05-31&#39;] chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700) source2 = [{ &quot;start&quot;: &quot;2020-03-23&quot;, &quot;end&quot;: nydf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/05/31/Covid-Death-Rates.html",
            "relUrl": "/nyt/2020/05/31/Covid-Death-Rates.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Covid Cases & Deaths Graph for U.S",
            "content": "The graph that we will learn to make today is from the NYT article on Coronavirus in the U.S . . import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) alt.renderers.set_embed_options(actions=False) usdf[&#39;new_deaths&#39;] = usdf[&#39;deaths&#39;].diff() usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart().mark_bar(size=5,opacity=0.2,color=&#39;gray&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_deaths:Q&#39;) ) # Area Chart area = alt.Chart().mark_area(fill=&#39;gray&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_deaths)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;black&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 3}) # Chart of deaths deaths = (bar+area+line).properties(width=800, title=&quot;Deaths&quot;) # Bar Chart bar2 = alt.Chart().mark_bar(size=5,opacity=0.2,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ) # Area Chart area2 = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line2 = area2.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 3}) cases = (bar2+area2+line2).properties(width=800, title=&quot;Cases&quot;) # Vertically concatenate the charts alt.vconcat(cases, deaths, data=usdf).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ) . Possible other ways to do this - . Repeat Chart | Facet Chart https://altair-viz.github.io/user_guide/compound_charts.html | .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/05/30/Covid-Cases-&-Deaths-in-US.html",
            "relUrl": "/nyt/2020/05/30/Covid-Cases-&-Deaths-in-US.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Covid Cases Graph",
            "content": "The graph that we will learn to make today is from the NYT article on Coronavirus Tracking in US which looks as follows - . . We will take the data from NYT&#39;s GitHub repo itself. Lets jump straight into the code - . import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.2,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line = area.mark_line(**{&quot;color&quot;: &#39;#c11111&#39;, &quot;opacity&quot;: 0.9, &quot;strokeWidth&quot;: 5}) chart = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=1000) chart . Now that we have replicated the chart effectively, lets filter the data so that we drop all the data before March as the chart from NYT shows - . That can be done in 2 ways : . using Pandas | using transform_filter on date | . You guessed it, we will use the latter. The code relevant to this would then be - . transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) . #collapse import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) #print(udf.columns) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.15,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.1).transform_window( #stroke=&#39;red&#39;, strokeWidth=2 rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;#c11111&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 5}) k = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=700)#width=alt.Step(500) k . . Now today or maybe a couple of days ago, NYT added interactivity to their charts. Lets do that too. The main concept here is using Altair Selections. For that we will use selection_single() on date field and change the opacity of the bars. So the important code pieces are - . single_bar = alt.selection_single(fields=[&#39;date&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39;) opacity = alt.condition(single_bar, alt.value(0.5), alt.value(0.15)) . #collapse import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) #print(udf.columns) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() single_bar = alt.selection_single(fields=[&#39;date&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39;) # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.15,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;), opacity= alt.condition(single_bar, alt.value(0.5), alt.value(0.15)) ).add_selection(single_bar).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.1).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;#c11111&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 5}) chart = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=700) chart . . Isn&#39;t that amazing . TODO . [ ] Adding tooltip/text to interactive bar chart | .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/05/29/Covid-cases.html",
            "relUrl": "/nyt/2020/05/29/Covid-cases.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "What is Altair?",
            "content": "To understand Altair we need to know what is it built on. The following hierarchy enables us to understand that - . D3 | Vega | Vega-Lite | Altair | . Lets start with Vega. . What is Vega? . Vega is a visualization grammar, a declarative language for creating, saving, and sharing interactive visualization designs. With Vega, you can describe the visual appearance and interactive behavior of a visualization in a JSON format, and generate web-based views using Canvas or SVG. . . Note: visualization grammar . Where does D3 fit in here? . To be clear, Vega is not intended as a “replacement” for D3. D3 is intentionally a lower-level library. During the early design of D3, we even referred to it as a “visualization kernel” rather than a “toolkit” or “framework”. In addition to custom design, D3 is intended as a supporting layer for higher-level visualization tools. Vega is one such tool, and Vega uses D3 heavily within its implementation. . Vega provides a higher-level visualization specification language on top of D3. By design, D3 will maintain an “expressivity advantage” and in some cases will be better suited for novel design ideas. On the other hand, we intend Vega to be convenient for a wide range of common yet customizable visualizations. . Now, that we know the top hierarchy, the rest is easy to follow - . Vega-Lite . Vega-Lite is a high-level grammar of interactive graphics. It provides a concise JSON syntax for rapidly generating visualizations to support analysis. Vega-Lite specifications can be compiled to Vega specifications. . . Note: high-level visualization grammar . Vega-Lite is used by some big players - . | | | | | | | | | | | | . Altair . In short, Altair exposes a Python API for building statistical visualizations that follows Vega-Lite syntax. . Altair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite. With Altair, you can spend more time understanding your data and its meaning. . Altair’s API is simple, friendly and consistent and built on top of the powerful Vega-Lite visualization grammar. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code. . . I do not intend to teach you how to use Altair or Vega Lite. You can do that on your own using the following excellent resources - . Conference Talk by Jake VanderPlas | . Exploratory Data Visualization with Altair | UW Data Viz Curriculum | .",
            "url": "https://armsp.github.io/covidviz/2020/05/28/Altair-Intro.html",
            "relUrl": "/2020/05/28/Altair-Intro.html",
            "date": " • May 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi :wave: . My name is Shantam. This blog will contain my attempts to replicate/emulate the visualizations at large media houses, related to COVID-19 pandemic, using open source data &amp; tools. . I am a driven python developer and data scientist. I have experience with Machine Learning and Big Data as well. Did I tell you I am a published poet too? . I am available for consulting, collaborations, part time or full time jobs. Let me know if you want to talk. . If you want me to try a specific visualization, do let me know by raising an issue in the GitHub Repository. . My website - www.shantamraj.com . I am working on a project that you should definitely check out - COVID-19 Stories .",
          "url": "https://armsp.github.io/covidviz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://armsp.github.io/covidviz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}