{
  
    
        "post0": {
            "title": "Latest epicenters of infection and death toll in the US",
            "content": "Today we will make a very special graph that looks like the article See How the Coronavirus Death Toll Grew Across the U.S. . . This will require us to use some special techniques using SVG Path elements. . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=True) # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf.head() . date county state fips cases deaths . 0 2020-01-21 | Snohomish | Washington | 53061.0 | 1 | 0 | . 1 2020-01-22 | Snohomish | Washington | 53061.0 | 1 | 0 | . 2 2020-01-23 | Snohomish | Washington | 53061.0 | 1 | 0 | . 3 2020-01-24 | Cook | Illinois | 17031.0 | 1 | 0 | . 4 2020-01-24 | Snohomish | Washington | 53061.0 | 1 | 0 | . # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in county data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . NYT publishes the data for New York City in a different way. So we will add custom FIPS for New York City and Puerto Rico too whose county level information is not present. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;].head() . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . cdf.loc[cdf[&#39;state&#39;] == &#39;Puerto Rico&#39;, &#39;fips&#39;] = 2 cdf[cdf[&#39;state&#39;] == &#39;Puerto Rico&#39;].head() . date county state fips cases deaths . 1858 2020-03-13 | Unknown | Puerto Rico | 2.0 | 3 | 0 | . 2220 2020-03-14 | Unknown | Puerto Rico | 2.0 | 4 | 0 | . 2642 2020-03-15 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . 3107 2020-03-16 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . 3637 2020-03-17 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . Extracting the latest cases and deaths - . aggregate = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;first&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) aggregate.head() . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-29 | New York | 219670 | 21941 | . 1 2.0 | Unknown | 2020-06-29 | Puerto Rico | 7250 | 153 | . 2 1001.0 | Autauga | 2020-06-29 | Alabama | 527 | 12 | . 3 1003.0 | Baldwin | 2020-06-29 | Alabama | 643 | 10 | . 4 1005.0 | Barbour | 2020-06-29 | Alabama | 322 | 1 | . Combining the 5 boroughs - New York, Kings, Queens, Bronx and Richmond - into one and adding that spatial area in the geodatatrame . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . county = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county[&#39;fips&#39;] = county[&#39;GEOID&#39;] county[&#39;fips&#39;] = county[&#39;fips&#39;].astype(&#39;int&#39;) county.head() . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 0 29 | 227 | 00758566 | 0500000US29227 | 29227 | Worth | 06 | 690564983 | 493903 | POLYGON ((-94.63203 40.57176, -94.53388 40.570... | -94.423288 | 40.479456 | 29227 | . 1 31 | 061 | 00835852 | 0500000US31061 | 31061 | Franklin | 06 | 1491355860 | 487899 | POLYGON ((-99.17940 40.35068, -98.72683 40.350... | -98.952991 | 40.176363 | 31061 | . 2 36 | 013 | 00974105 | 0500000US36013 | 36013 | Chautauqua | 06 | 2746047476 | 1139407865 | POLYGON ((-79.76195 42.26986, -79.62748 42.324... | -79.366918 | 42.227692 | 36013 | . 3 37 | 181 | 01008591 | 0500000US37181 | 37181 | Vance | 06 | 653713542 | 42178610 | POLYGON ((-78.49773 36.51467, -78.45728 36.541... | -78.406712 | 36.368814 | 37181 | . 4 47 | 183 | 01639799 | 0500000US47183 | 47183 | Weakley | 06 | 1503107848 | 3707114 | POLYGON ((-88.94916 36.41010, -88.81642 36.410... | -88.719909 | 36.298962 | 47183 | . We will actually work with Metropolitan Statistical Areas instead of counties, so we need more work to do. We have the MSA Shapefile as well as a dataset that has the counties that combine to form MSAs from the US Census . msa = pd.read_csv(&#39;core_msa_list.csv&#39;, sep=&quot;;&quot;) msa_shp = gpd.read_file(&#39;shapes/cb_2019_us_cbsa_500k/cb_2019_us_cbsa_500k.shp&#39;) #msa[msa[&#39;CBSA Title&#39;].str.startswith(&#39;New York&#39;)] . msa.head() . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County . 0 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 13 | Central | . 1 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 45 | Outlying | . 2 10140 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 27 | Central | . 3 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 59 | Outlying | . 4 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | . msa_shp.head() . CSAFP CBSAFP AFFGEOID GEOID NAME LSAD ALAND AWATER geometry . 0 425 | 37620 | 310M500US37620 | 37620 | Parkersburg-Vienna, WV | M1 | 1551452495 | 32408833 | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | . 1 None | 45980 | 310M500US45980 | 45980 | Troy, AL | M2 | 1740647520 | 2336975 | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | . 2 548 | 49020 | 310M500US49020 | 49020 | Winchester, VA-WV | M1 | 2752545068 | 16892497 | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | . 3 142 | 45180 | 310M500US45180 | 45180 | Talladega-Sylacauga, AL | M2 | 1908293036 | 60927931 | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | . 4 None | 25060 | 310M500US25060 | 25060 | Gulfport-Biloxi, MS | M1 | 5739122781 | 2105780374 | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | . msa[&#39;FIPS State Code&#39;] = msa[&#39;FIPS State Code&#39;].astype(str) msa[&#39;FIPS County Code&#39;] = msa[&#39;FIPS County Code&#39;].astype(str) . state_fips_max_length = msa[&#39;FIPS State Code&#39;].map(len).max() county_fips_max_length = msa[&#39;FIPS County Code&#39;].map(len).max() . msa[&#39;FIPS State Code&#39;] = msa[&#39;FIPS State Code&#39;].apply(lambda x: &#39;0&#39;*(state_fips_max_length - len(x))+x) msa[&#39;FIPS County Code&#39;] = msa[&#39;FIPS County Code&#39;].apply(lambda x: &#39;0&#39;*(county_fips_max_length - len(x))+x) . msa[&#39;fips&#39;] = msa[&#39;FIPS State Code&#39;]+msa[&#39;FIPS County Code&#39;] msa[&#39;fips&#39;] = msa[&#39;fips&#39;].astype(float) . Now we will add a row for New York . nyc_temp = pd.DataFrame({&#39;CBSA Code&#39;: 35620,&#39;Metropolitan Division Code&#39;: None, &#39;CSA Code&#39;: 408, &#39;CBSA Title&#39;: None,&#39;Metropolitan/Micropolitan Statistical Area&#39;: None, &#39;Metropolitan Division Title&#39;: None,&#39;CSA Title&#39;: None,&#39;County/County Equivalent&#39;: None, &#39;State Name&#39;: &#39;New York&#39;, &#39;FIPS State Code&#39;: 36, &#39;FIPS County Code&#39;: None, &#39;Central/Outlying County&#39;: None, &#39;fips&#39;: 1},index=[0]) msa = pd.concat([msa, nyc_temp], ignore_index=True) msa . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips . 0 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 013 | Central | 46013.0 | . 1 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 045 | Outlying | 46045.0 | . 2 10140 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 027 | Central | 53027.0 | . 3 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 059 | Outlying | 48059.0 | . 4 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | 48253.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1912 49700 | NaN | 472.0 | Yuba City, CA | Metropolitan Statistical Area | NaN | Sacramento-Roseville, CA | Yuba County | California | 06 | 115 | Central | 6115.0 | . 1913 49740 | NaN | NaN | Yuma, AZ | Metropolitan Statistical Area | NaN | NaN | Yuma County | Arizona | 04 | 027 | Central | 4027.0 | . 1914 49780 | NaN | 198.0 | Zanesville, OH | Micropolitan Statistical Area | NaN | Columbus-Marion-Zanesville, OH | Muskingum County | Ohio | 39 | 119 | Central | 39119.0 | . 1915 49820 | NaN | NaN | Zapata, TX | Micropolitan Statistical Area | NaN | NaN | Zapata County | Texas | 48 | 505 | Central | 48505.0 | . 1916 35620 | None | 408.0 | None | None | None | None | None | New York | 36 | None | None | 1.0 | . 1917 rows × 13 columns . msa[&#39;CBSA Code&#39;] = msa[&#39;CBSA Code&#39;].astype(float) . msa[msa[&#39;fips&#39;].isin(aggregate[&#39;fips&#39;]) == False] . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips . 8 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Aguada Municipio | Puerto Rico | 72 | 003 | Central | 72003.0 | . 9 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Aguadilla Municipio | Puerto Rico | 72 | 005 | Central | 72005.0 | . 10 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Añasco Municipio | Puerto Rico | 72 | 011 | Central | 72011.0 | . 11 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Isabela Municipio | Puerto Rico | 72 | 071 | Central | 72071.0 | . 12 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Lares Municipio | Puerto Rico | 72 | 081 | Central | 72081.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1591 42180.0 | NaN | 434.0 | Santa Isabel, PR | Micropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Santa Isabel Municipio | Puerto Rico | 72 | 133 | Central | 72133.0 | . 1903 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Guánica Municipio | Puerto Rico | 72 | 055 | Central | 72055.0 | . 1904 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Guayanilla Municipio | Puerto Rico | 72 | 059 | Central | 72059.0 | . 1905 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Peñuelas Municipio | Puerto Rico | 72 | 111 | Central | 72111.0 | . 1906 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Yauco Municipio | Puerto Rico | 72 | 153 | Central | 72153.0 | . 91 rows × 13 columns . # Puerto Rico does not provide data at county level. So we will have to do a similar exercise like NYC for PR and aggregate it statewise. # But since in albersUsa projection PR is filtered anyways, we won&#39;t be doing that exercise right away. Once I figure out how to use custom projection in the default # albersUsa projection that is used by Vega-Lite under the hood, we will include Puerto Rico aggregate[aggregate[&#39;state&#39;].str.startswith(&#39;Puerto&#39;)] . fips county date state cases deaths . 1 2.0 | Unknown | 2020-06-29 | Puerto Rico | 7250 | 153 | . aggregate[aggregate[&#39;fips&#39;].isin(msa[&#39;fips&#39;]) == False] . fips county date state cases deaths . 1 2.0 | Unknown | 2020-06-29 | Puerto Rico | 7250 | 153 | . 7 1011.0 | Bullock | 2020-06-29 | Alabama | 365 | 10 | . 8 1013.0 | Butler | 2020-06-29 | Alabama | 605 | 27 | . 11 1019.0 | Cherokee | 2020-06-29 | Alabama | 73 | 7 | . 13 1023.0 | Choctaw | 2020-06-29 | Alabama | 192 | 12 | . ... ... | ... | ... | ... | ... | ... | . 3036 56027.0 | Niobrara | 2020-06-29 | Wyoming | 2 | 0 | . 3037 56029.0 | Park | 2020-06-29 | Wyoming | 40 | 0 | . 3038 56031.0 | Platte | 2020-06-29 | Wyoming | 2 | 0 | . 3040 56035.0 | Sublette | 2020-06-29 | Wyoming | 6 | 0 | . 3044 56043.0 | Washakie | 2020-06-29 | Wyoming | 38 | 5 | . 1220 rows × 6 columns . Now we will merge the aggregated data with msa since msa is expanded on fips(CBSA repeats), which is the only unique column. . msa = msa.merge(aggregate, how=&#39;inner&#39;, on=&#39;fips&#39;) msa . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips county date state cases deaths . 0 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 013 | Central | 46013.0 | Brown | 2020-06-29 | South Dakota | 342 | 2 | . 1 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 045 | Outlying | 46045.0 | Edmunds | 2020-06-29 | South Dakota | 7 | 0 | . 2 10140.0 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 027 | Central | 53027.0 | Grays Harbor | 2020-06-29 | Washington | 26 | 0 | . 3 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 059 | Outlying | 48059.0 | Callahan | 2020-06-29 | Texas | 16 | 2 | . 4 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | 48253.0 | Jones | 2020-06-29 | Texas | 607 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1821 49700.0 | NaN | 472.0 | Yuba City, CA | Metropolitan Statistical Area | NaN | Sacramento-Roseville, CA | Yuba County | California | 06 | 115 | Central | 6115.0 | Yuba | 2020-06-29 | California | 71 | 1 | . 1822 49740.0 | NaN | NaN | Yuma, AZ | Metropolitan Statistical Area | NaN | NaN | Yuma County | Arizona | 04 | 027 | Central | 4027.0 | Yuma | 2020-06-29 | Arizona | 5895 | 90 | . 1823 49780.0 | NaN | 198.0 | Zanesville, OH | Micropolitan Statistical Area | NaN | Columbus-Marion-Zanesville, OH | Muskingum County | Ohio | 39 | 119 | Central | 39119.0 | Muskingum | 2020-06-29 | Ohio | 73 | 1 | . 1824 49820.0 | NaN | NaN | Zapata, TX | Micropolitan Statistical Area | NaN | NaN | Zapata County | Texas | 48 | 505 | Central | 48505.0 | Zapata | 2020-06-29 | Texas | 45 | 0 | . 1825 35620.0 | None | 408.0 | None | None | None | None | None | New York | 36 | None | None | 1.0 | New York City | 2020-06-29 | New York | 219670 | 21941 | . 1826 rows × 18 columns . msa.rename(columns={&#39;CBSA Code&#39;: &#39;CBSAFP&#39;}, inplace=True) . msa_shp[&#39;CBSAFP&#39;] = msa_shp[&#39;CBSAFP&#39;].astype(float) . msa_shp[&#39;lon&#39;] = msa_shp[&#39;geometry&#39;].centroid.x msa_shp[&#39;lat&#39;] = msa_shp[&#39;geometry&#39;].centroid.y . Now we will aggregate the msa data on CBSAFP so that it becomes similar to msa_shp geodataframe . msa_agg = msa.groupby(&#39;CBSAFP&#39;, as_index=False).agg({&#39;CBSA Title&#39;: &#39;first&#39;, &#39;State Name&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;sum&#39;, &#39;deaths&#39;: &#39;sum&#39;}) . msa_agg.head() . CBSAFP CBSA Title State Name date cases deaths . 0 10100.0 | Aberdeen, SD | South Dakota | 2020-06-29 | 349 | 2 | . 1 10140.0 | Aberdeen, WA | Washington | 2020-06-29 | 26 | 0 | . 2 10180.0 | Abilene, TX | Texas | 2020-06-29 | 1083 | 7 | . 3 10220.0 | Ada, OK | Oklahoma | 2020-06-29 | 38 | 2 | . 4 10300.0 | Adrian, MI | Michigan | 2020-06-29 | 943 | 10 | . Now we will merge msa_agg with msa and make the heights column that contains a custom SVG Path string per row, since right now Vega-Lite does not support &quot;scaleY&quot; as an encoding. In Vega, we don&#39;t have to do it this way, we can just provide a single svg path for an equilateral triangle and then stretch it(using scaleY) based on &quot;cases&quot; or &quot;deaths&quot;. . msa_shp = msa_shp.merge(msa_agg, how=&#39;inner&#39;, on=&#39;CBSAFP&#39;) . msa_shp[&#39;height&#39;] = msa_shp[&#39;deaths&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/50} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) msa_shp.head() . CSAFP CBSAFP AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat CBSA Title State Name date cases deaths height . 0 425 | 37620.0 | 310M500US37620 | 37620 | Parkersburg-Vienna, WV | M1 | 1551452495 | 32408833 | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-06-29 | 90 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | . 1 None | 45980.0 | 310M500US45980 | 45980 | Troy, AL | M2 | 1740647520 | 2336975 | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-06-29 | 401 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | . 2 548 | 49020.0 | 310M500US49020 | 49020 | Winchester, VA-WV | M1 | 2752545068 | 16892497 | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-06-29 | 846 | 9 | M -1.5 0 L0 -0.18 L1.5 0 | . 3 142 | 45180.0 | 310M500US45180 | 45180 | Talladega-Sylacauga, AL | M2 | 1908293036 | 60927931 | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-06-29 | 253 | 6 | M -1.5 0 L0 -0.12 L1.5 0 | . 4 None | 25060.0 | 310M500US25060 | 25060 | Gulfport-Biloxi, MS | M1 | 5739122781 | 2105780374 | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-06-29 | 1351 | 37 | M -1.5 0 L0 -0.74 L1.5 0 | . msa_shp.drop([&#39;CSAFP&#39;, &#39;AFFGEOID&#39;, &#39;GEOID&#39;, &#39;LSAD&#39;, &#39;ALAND&#39;, &#39;AWATER&#39;], axis=1, inplace=True) . msa_shp.head() . CBSAFP NAME geometry lon lat CBSA Title State Name date cases deaths height . 0 37620.0 | Parkersburg-Vienna, WV | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-06-29 | 90 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | . 1 45980.0 | Troy, AL | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-06-29 | 401 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | . 2 49020.0 | Winchester, VA-WV | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-06-29 | 846 | 9 | M -1.5 0 L0 -0.18 L1.5 0 | . 3 45180.0 | Talladega-Sylacauga, AL | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-06-29 | 253 | 6 | M -1.5 0 L0 -0.12 L1.5 0 | . 4 25060.0 | Gulfport-Biloxi, MS | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-06-29 | 1351 | 37 | M -1.5 0 L0 -0.74 L1.5 0 | . Let&#39;s finally plot this graph of deaths till now - . spikes = alt.Chart(msa_shp).transform_filter(alt.datum.deaths&gt;0).mark_point( fillOpacity=1, fill=alt.Gradient( gradient=&quot;linear&quot;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;red&#39;, offset=0.5)], x1=1, x2=1, y1=1, y2=0 ), #dx=10, #dy=-30, strokeOpacity=1, strokeWidth=1, stroke=&#39;red&#39; ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height:N&quot;, scale=None), #tooltip=[&#39;CBSA Title:N&#39;, &#39;deaths:Q&#39;], #color = alt.condition(selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1200, height=800 ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) (state+spikes).configure_view(strokeWidth=0) . Now we will study only last week&#39;s average cases per day to see where the indections are on the rise . #msa_shp[&#39;height_cases&#39;] = msa_shp[&#39;cases&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/1000} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) . cdf[&#39;cases_per_day&#39;] = cdf.groupby(&quot;fips&quot;)[&#39;cases&#39;].diff() . last_week_cases_avg = cdf.groupby(&quot;fips&quot;)[&quot;cases_per_day&quot;].apply(lambda x: x.iloc[-7:].mean()) . last_week_cases_avg = last_week_cases_avg.reset_index() . last_week_cases_avg.columns = [&#39;fips&#39;, &#39;avg_cases_last_week&#39;] . last_week_cases_avg.head() . fips avg_cases_last_week . 0 1.0 | 311.857143 | . 1 2.0 | 98.000000 | . 2 1001.0 | 12.142857 | . 3 1003.0 | 29.428571 | . 4 1005.0 | 6.428571 | . avg_last_week_cases = cdf.groupby(&quot;fips&quot;).agg({&#39;county&#39;: &#39;first&#39;, &#39;cases_per_day&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) . avg_last_week_cases = avg_last_week_cases.merge(last_week_cases_avg, how=&#39;inner&#39;, on=&#39;fips&#39;) . avg_last_week_cases.head() . fips county cases_per_day date state cases deaths avg_cases_last_week . 0 1.0 | New York City | 189.0 | 2020-06-29 | New York | 219670 | 21941 | 311.857143 | . 1 2.0 | Unknown | 61.0 | 2020-06-29 | Puerto Rico | 7250 | 153 | 98.000000 | . 2 1001.0 | Autauga | 24.0 | 2020-06-29 | Alabama | 527 | 12 | 12.142857 | . 3 1003.0 | Baldwin | 68.0 | 2020-06-29 | Alabama | 643 | 10 | 29.428571 | . 4 1005.0 | Barbour | 5.0 | 2020-06-29 | Alabama | 322 | 1 | 6.428571 | . msa_agg_lastweek = msa.merge(avg_last_week_cases, how=&#39;inner&#39;, on=&#39;fips&#39;) msa_agg_lastweek.head() . CBSAFP Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code ... state_x cases_x deaths_x county_y cases_per_day date_y state_y cases_y deaths_y avg_cases_last_week . 0 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | ... | South Dakota | 342 | 2 | Brown | 1.0 | 2020-06-29 | South Dakota | 342 | 2 | 2.285714 | . 1 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | ... | South Dakota | 7 | 0 | Edmunds | 0.0 | 2020-06-29 | South Dakota | 7 | 0 | 0.285714 | . 2 10140.0 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | ... | Washington | 26 | 0 | Grays Harbor | 0.0 | 2020-06-29 | Washington | 26 | 0 | 0.428571 | . 3 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | ... | Texas | 16 | 2 | Callahan | 0.0 | 2020-06-29 | Texas | 16 | 2 | 0.428571 | . 4 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | ... | Texas | 607 | 0 | Jones | 0.0 | 2020-06-29 | Texas | 607 | 0 | -6.142857 | . 5 rows × 25 columns . # Average of multiple series is same as sum of their averages so &#39;avg_cases_last_week&#39;: &#39;sum&#39; works well msa_agg_lastweek = msa_agg_lastweek.groupby(&#39;CBSAFP&#39;, as_index=False).agg({&#39;CBSA Title&#39;: &#39;first&#39;, &#39;State Name&#39;: &#39;last&#39;, &#39;avg_cases_last_week&#39;: &#39;sum&#39;}) . msa_agg_lastweek.head() . CBSAFP CBSA Title State Name avg_cases_last_week . 0 10100.0 | Aberdeen, SD | South Dakota | 2.571429 | . 1 10140.0 | Aberdeen, WA | Washington | 0.428571 | . 2 10180.0 | Abilene, TX | Texas | 4.000000 | . 3 10220.0 | Ada, OK | Oklahoma | 1.571429 | . 4 10300.0 | Adrian, MI | Michigan | 2.285714 | . msa_shp = msa_shp.merge(msa_agg_lastweek, how=&#39;inner&#39;, on=&#39;CBSAFP&#39;) msa_shp.head() . CBSAFP NAME geometry lon lat CBSA Title_x State Name_x date cases deaths height CBSA Title_y State Name_y avg_cases_last_week . 0 37620.0 | Parkersburg-Vienna, WV | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-06-29 | 90 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | Parkersburg-Vienna, WV | West Virginia | 4.142857 | . 1 45980.0 | Troy, AL | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-06-29 | 401 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | Troy, AL | Alabama | 5.714286 | . 2 49020.0 | Winchester, VA-WV | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-06-29 | 846 | 9 | M -1.5 0 L0 -0.18 L1.5 0 | Winchester, VA-WV | West Virginia | 7.714286 | . 3 45180.0 | Talladega-Sylacauga, AL | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-06-29 | 253 | 6 | M -1.5 0 L0 -0.12 L1.5 0 | Talladega-Sylacauga, AL | Alabama | 8.714286 | . 4 25060.0 | Gulfport-Biloxi, MS | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-06-29 | 1351 | 37 | M -1.5 0 L0 -0.74 L1.5 0 | Gulfport-Biloxi, MS | Mississippi | 48.571429 | . Adding a new column called height_last_week_avg_cases that has the custom SVG paths like we did earlier - . msa_shp[&#39;height_last_week_avg_cases&#39;] = msa_shp[&#39;avg_cases_last_week&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/10} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) . spikes = alt.Chart(msa_shp).mark_point( fillOpacity=1, fill=alt.Gradient( gradient=&quot;linear&quot;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;red&#39;, offset=0.5)], x1=1, x2=1, y1=1, y2=0 ), #dx=10, #dy=-30, stroke=&#39;red&#39;, strokeOpacity=1, strokeWidth=1 ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height_last_week_avg_cases:N&quot;, scale=None), tooltip=[&#39;NAME:N&#39;, &#39;avg_cases_last_week:Q&#39;] ).project( type=&#39;albersUsa&#39; ).properties( width=1200, height=800 ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) (state+spikes).configure_view(strokeWidth=0) . Its clear that now the cases are increasing much rapidly in the South especially in the states of - Texas, California, Florida and Arizona . There you have it! .",
            "url": "https://armsp.github.io/covidviz/geospatial/2020/07/01/MSA-cases-and-deaths.html",
            "relUrl": "/geospatial/2020/07/01/MSA-cases-and-deaths.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Per Capita COVID-19 cases worldwide",
            "content": "Today we will make the Per Capita covid cases worldwide from the article Coronavirus Map: Tracking the Global Outbreak that look like the following - . . import pandas as pd import geopandas as gpd import altair as alt #import numpy as np . We will use the JHU CSSE Dataset for the cases as well as the population. For the map we will use a geojson file that I made in the last world chloropleth case growth map. . population_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv&#39; population_data = pd.read_csv(population_uri) latest_cases_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/06-24-2020.csv&#39; latest_cases = pd.read_csv(latest_cases_uri) world_geojson = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/world.geojson&#39; world = gpd.read_file(world_geojson) . Dropping unnecessary columns and making necessary moniker changes to countries just like we did last time - . latest_cases = latest_cases.drop([&#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Last_Update&#39;, &#39;Lat&#39;, &#39;Long_&#39;, &#39;Combined_Key&#39;, &#39;Incidence_Rate&#39;, &#39;Case-Fatality_Ratio&#39;], axis=1) latest_cases = latest_cases.groupby(&#39;Country_Region&#39;).aggregate({&#39;Confirmed&#39;: &#39;sum&#39;, &#39;Recovered&#39;: &#39;sum&#39;, &#39;Deaths&#39;: &#39;sum&#39;, &#39;Active&#39;: &#39;sum&#39;, }) latest_cases = latest_cases.reset_index() . latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Taiwan*&#39;, &#39;Country_Region&#39;] = &#39;Taiwan&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;US&#39;, &#39;Country_Region&#39;] = &#39;United States&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Czech Republic&#39;, &#39;Country_Region&#39;] = &#39;Czechia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;West Bank and Gaza&#39;, &#39;Country_Region&#39;] = &#39;West Bank (disp)&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Western Sahara&#39;, &#39;Country_Region&#39;] = &#39;Western Sahara (disp)&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Trinidad and Tobago&#39;, &#39;Country_Region&#39;] = &#39;Trinidad &amp; Tobago&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Sao Tome and Principe&#39;, &#39;Country_Region&#39;] = &#39;Sao Tome &amp; Principe&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Saint Vincent and the Grenadines&#39;, &#39;Country_Region&#39;] = &#39;St Vincent &amp; the Grenadines&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Saint Lucia&#39;, &#39;Country_Region&#39;] = &#39;St Lucia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Saint Kitts and Nevis&#39;, &#39;Country_Region&#39;] = &#39;St Kitts &amp; Nevis&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;North Macedonia&#39;, &#39;Country_Region&#39;] = &#39;Macedonia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Bahamas&#39;, &#39;Country_Region&#39;] = &#39;Bahamas, The&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Bosnia and Herzegovina&#39;, &#39;Country_Region&#39;] = &#39;Bosnia &amp; Herzegovina&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Central African Republic&#39;, &#39;Country_Region&#39;] = &#39;Central African Rep&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Eswatini&#39;, &#39;Country_Region&#39;] = &#39;Swaziland&#39; #time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;South Korea&#39;, &#39;Country/Region&#39;] = &#39;Korea, South&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Dem Rep of the&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Rep of the&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Antigua and Barbuda&#39;, &#39;Country_Region&#39;] = &#39;Antigua &amp; Barbuda&#39; . latest_cases[latest_cases[&#39;Country_Region&#39;].isin(world[&#39;COUNTRY_NA&#39;]) == False] . Country_Region Confirmed Recovered Deaths Active . 48 Diamond Princess | 712 | 651 | 13 | 48 | . 64 Gambia | 42 | 26 | 2 | 14 | . 75 Holy See | 12 | 12 | 0 | 0 | . 104 MS Zaandam | 9 | 0 | 2 | 7 | . Extracting population data for countries - . population_data = population_data.drop([&#39;UID&#39;, &#39;code3&#39;, &#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Lat&#39;, &#39;Long_&#39;], axis=1) population_data = population_data[population_data[&#39;Country_Region&#39;] == population_data[&#39;Combined_Key&#39;]] population_data = population_data.reset_index(drop=True) population_data.head() . iso2 iso3 Country_Region Combined_Key Population . 0 AF | AFG | Afghanistan | Afghanistan | 38928341.0 | . 1 AL | ALB | Albania | Albania | 2877800.0 | . 2 DZ | DZA | Algeria | Algeria | 43851043.0 | . 3 AD | AND | Andorra | Andorra | 77265.0 | . 4 AO | AGO | Angola | Angola | 32866268.0 | . population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Taiwan*&#39;, &#39;Country_Region&#39;] = &#39;Taiwan&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;US&#39;, &#39;Country_Region&#39;] = &#39;United States&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Czech Republic&#39;, &#39;Country_Region&#39;] = &#39;Czechia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;West Bank and Gaza&#39;, &#39;Country_Region&#39;] = &#39;West Bank (disp)&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Western Sahara&#39;, &#39;Country_Region&#39;] = &#39;Western Sahara (disp)&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Trinidad and Tobago&#39;, &#39;Country_Region&#39;] = &#39;Trinidad &amp; Tobago&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Sao Tome and Principe&#39;, &#39;Country_Region&#39;] = &#39;Sao Tome &amp; Principe&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Saint Vincent and the Grenadines&#39;, &#39;Country_Region&#39;] = &#39;St Vincent &amp; the Grenadines&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Saint Lucia&#39;, &#39;Country_Region&#39;] = &#39;St Lucia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Saint Kitts and Nevis&#39;, &#39;Country_Region&#39;] = &#39;St Kitts &amp; Nevis&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;North Macedonia&#39;, &#39;Country_Region&#39;] = &#39;Macedonia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Bahamas&#39;, &#39;Country_Region&#39;] = &#39;Bahamas, The&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Bosnia and Herzegovina&#39;, &#39;Country_Region&#39;] = &#39;Bosnia &amp; Herzegovina&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Central African Republic&#39;, &#39;Country_Region&#39;] = &#39;Central African Rep&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Eswatini&#39;, &#39;Country_Region&#39;] = &#39;Swaziland&#39; #time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;South Korea&#39;, &#39;Country/Region&#39;] = &#39;Korea, South&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Dem Rep of the&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Rep of the&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Antigua and Barbuda&#39;, &#39;Country_Region&#39;] = &#39;Antigua &amp; Barbuda&#39; . world.columns = [&#39;Country_Region&#39;, &#39;geometry&#39;] world = world.merge(latest_cases, on=&#39;Country_Region&#39;, how=&#39;left&#39;) world = world.merge(population_data, on=&#39;Country_Region&#39;, how=&#39;left&#39;) world[&#39;per_capita&#39;] = world[&#39;Confirmed&#39;]/world[&#39;Population&#39;] . world[&#39;code&#39;] = world[&#39;per_capita&#39;].apply(lambda x: &#39;Less than 1 in 1000&#39; if x &lt;= (1/1000) else &#39;Less than 1 in 500&#39; if x&lt;= (1/500) else &#39;Less than 1 in 333&#39; if x&lt;= (1/333) else &#39;No Cases reported&#39; if pd.isnull(x) else &#39;Greater than 1 in 333&#39;) world[&#39;Share of Population&#39;] = world[&#39;Population&#39;]/world[&#39;Confirmed&#39;] world[&#39;Share of Population&#39;] = world[&#39;Share of Population&#39;].round() world[&#39;Share of Population&#39;] = world[&#39;Share of Population&#39;].apply(lambda x: f&quot;1 in {str(x).split(&#39;.&#39;)[0]}&quot;) . Making the chart - . alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).transform_filter(alt.datum.Country_Region != &#39;Antarctica&#39;).encode( color=alt.Color(&#39;code:N&#39;, scale=alt.Scale(domain=[&#39;No Cases reported&#39;, &#39;Less than 1 in 1000&#39;, &#39;Less than 1 in 500&#39;, &#39;Less than 1 in 333&#39;, &#39;Greater than 1 in 333&#39;], range=[&#39;lightgrey&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;]),legend=alt.Legend(title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200)), tooltip = [&#39;Country_Region&#39;, &#39;Confirmed&#39;, &#39;Share of Population&#39;] ).properties(width=1400, height=800).project(&#39;equalEarth&#39;).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/2020/06/28/World-Per-Capita-Cases.html",
            "relUrl": "/geospatial/2020/06/28/World-Per-Capita-Cases.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Temporary decline in CO₂ due to COVID-19",
            "content": "Today we will work on the following graph from the article Emissions Are Surging Back as Countries and States Reopen - . . I downloaded the dataset as an Excel file and saved data for individual countries as csv files. . import altair as alt import pandas as pd . #hide_output alt.renderers.set_embed_options(actions=False) ind = pd.read_csv(&#39;ind_co2_em.csv&#39;) ind = ind.iloc[1:] chn = pd.read_csv(&#39;china_co2_em.csv&#39;, sep=&#39;;&#39;) chn = chn.iloc[1:] us = pd.read_csv(&#39;us_co2_em.csv&#39;, sep=&#39;;&#39;) us = us.iloc[1:] euuk = pd.read_csv(&#39;euuk_co2_em.csv&#39;, sep=&#39;;&#39;) euuk = euuk.iloc[1:] globl = pd.read_csv(&#39;global_co2_em.csv&#39;, sep=&#39;;&#39;) globl = globl.iloc[1:] data = pd.concat([chn, ind, euuk, us, globl]) data[&#39;DATE&#39;] = pd.to_datetime(data[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) data[[&#39;PWR_CO2_MED&#39;,&#39;IND_CO2_MED&#39;,&#39;TRS_CO2_MED&#39;,&#39;PUB_CO2_MED&#39;,&#39;RES_CO2_MED&#39;,&#39;AVI_CO2_MED&#39;]] = data[[&#39;PWR_CO2_MED&#39;,&#39;IND_CO2_MED&#39;,&#39;TRS_CO2_MED&#39;,&#39;PUB_CO2_MED&#39;,&#39;RES_CO2_MED&#39;,&#39;AVI_CO2_MED&#39;]].apply(pd.to_numeric) . data.head() . REGION_ID REGION_CODE REGION_NAME TIME_POINT DATE TOTAL_CO2_MED PWR_CO2_MED IND_CO2_MED TRS_CO2_MED PUB_CO2_MED ... PUB_CO2_LOW RES_CO2_LOW AVI_CO2_LOW TOTAL_CO2_HIGH PWR_CO2_HIGH IND_CO2_HIGH TRS_CO2_HIGH PUB_CO2_HIGH RES_CO2_HIGH AVI_CO2_HIGH . 1 23 | CHN | China | 1 | 2020-01-01 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 23 | CHN | China | 2 | 2020-01-02 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 23 | CHN | China | 3 | 2020-01-03 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 23 | CHN | China | 4 | 2020-01-04 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 23 | CHN | China | 5 | 2020-01-05 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 26 columns . If you observe the chart closely you will realize that the graph is stacked, so that is what we will do right away using altair&#39;s area chart - . alt.Chart(data).mark_area().encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_NAME:N&#39;),#,scale=alt.Scale(scheme=&#39;reds&#39;)), ).properties(width=800, height=400) . This is close but not exactly like what we saw in the article. If you look closely you&#39;d realize that the order of countries is different. So we will try to follow the same order using the order encoding field. . alt.Chart(data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;),#,scale=alt.Scale(scheme=&#39;reds&#39;)), order=&#39;order:O&#39; ).properties(width=800, height=400) . This is exactly like it. Let&#39;s change the colors, I probably would have done it the following way - . alt.Chart(data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;GLOBAL&#39;], range=[&quot;#c9c9c9&quot;, &quot;#aaaaaa&quot;, &quot;#888888&quot;, &quot;#686868&quot;, &quot;#454545&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) . To make it just like the graph in the article, we will pick colors from here https://imagecolorpicker.com/en/ . alt.Chart(data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;GLOBAL&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) . If you look closely, you would notice that we are capturing the trend perfectly, however the area for &quot;REST of the world&quot; is much more than what it should be. That is because, its duplicating the data from US, EU, India, and China. So we need to subtract the contributions of these places from the global data and then stack them. . chn[&#39;DATE&#39;] = pd.to_datetime(chn[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) ind[&#39;DATE&#39;] = pd.to_datetime(ind[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) us[&#39;DATE&#39;] = pd.to_datetime(us[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) euuk[&#39;DATE&#39;] = pd.to_datetime(euuk[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) globl[&#39;DATE&#39;] = pd.to_datetime(globl[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) . ind[list(ind.columns)[5:]] = ind[list(ind.columns)[5:]].apply(pd.to_numeric) chn[list(chn.columns)[5:]] = chn[list(chn.columns)[5:]].apply(pd.to_numeric) us[list(us.columns)[5:]] = us[list(us.columns)[5:]].apply(pd.to_numeric) euuk[list(euuk.columns)[5:]] = euuk[list(euuk.columns)[5:]].apply(pd.to_numeric) globl[list(globl.columns)[5:]] = globl[list(globl.columns)[5:]].apply(pd.to_numeric) . countries_sum = ind[list(ind.columns)[5:]]+chn[list(chn.columns)[5:]]+us[list(us.columns)[5:]]+euuk[list(euuk.columns)[5:]] . rest = globl[list(globl.columns)[5:]] - countries_sum[list(countries_sum.columns)] . rest[&#39;REGION_ID&#39;] = 99 rest[&#39;REGION_CODE&#39;] = &#39;RST&#39; rest[&#39;REGION_NAME&#39;] = &#39;REST&#39; rest[&#39;TIME_POINT&#39;] = globl[&#39;TIME_POINT&#39;] rest[&#39;DATE&#39;] = globl[&#39;DATE&#39;] . data = pd.concat([chn, ind, euuk, us, rest]) . alt.Chart(data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;))), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400).configure_view(strokeWidth=0).configure_axis(grid=False) . This looks exactly like the chart in the article. Right now there is no way to properly add text in a stacked chart&#39;s corresponding area, but let&#39;s try it anyways so that once this option is available in Vega-Lite we will fix this code immediately later on. . base = alt.Chart(data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;))), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) t = alt.Chart(data).mark_text().encode( x=alt.X(&#39;DATE:T&#39;, aggregate=&#39;median&#39;, ), #y=alt.Y(&#39;variety:N&#39;), #detail=&#39;REGION_CODE:N&#39;, text=alt.Text(&#39;REGION_NAME:N&#39;), y=&#39;min(TOTAL_CO2_MED):Q&#39;, #text=&#39;REGION_NAME:N&#39; ) (base+t).configure_view(strokeWidth=0).configure_axis(grid=False) . While we are at it we can also make the following graph of global emissions by sector - . . The main idea behind these plots is layering an area plot on top of a line chart with the area shaded by the LOW and HIGH columns - . line = alt.Chart(globl).mark_line().encode( x=&#39;DATE:T&#39;, y=alt.Y(&#39;TRS_CO2_MED:Q&#39;), ) band = line.mark_area(opacity=0.3).encode( x=&#39;DATE:T&#39;, y=alt.Y(&#39;TRS_CO2_LOW:Q&#39;), y2=alt.Y2(&#39;TRS_CO2_HIGH:Q&#39;), ) line+band . Now we are going to change the data so that we can facet it properly like in the article&#39;s chart - . globl.drop([&#39;REGION_ID&#39;, &#39;REGION_CODE&#39;, &#39;REGION_NAME&#39;, &#39;TOTAL_CO2_MED&#39;, &#39;TOTAL_CO2_HIGH&#39;, &#39;TOTAL_CO2_LOW&#39;], axis=1, inplace=True) #globl.drop([&#39;TRS_CO2_MED&#39;, &#39;IND_CO2_MED&#39;, &#39;PWR_CO2_MED&#39;, &#39;PUB_CO2_MED&#39;, &#39;AVI_CO2_MED&#39;, &#39;RES_CO2_MED&#39;], axis=1, inplace=True) . globl.head() . TIME_POINT DATE PWR_CO2_MED IND_CO2_MED TRS_CO2_MED PUB_CO2_MED RES_CO2_MED AVI_CO2_MED PWR_CO2_LOW IND_CO2_LOW TRS_CO2_LOW PUB_CO2_LOW RES_CO2_LOW AVI_CO2_LOW PWR_CO2_HIGH IND_CO2_HIGH TRS_CO2_HIGH PUB_CO2_HIGH RES_CO2_HIGH AVI_CO2_HIGH . 1 1 | 2020-01-01 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 2 2 | 2020-01-02 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 3 3 | 2020-01-03 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 4 4 | 2020-01-04 | 0.0 | -0.014383 | -0.017373 | -0.001128 | 0.0 | -0.006994 | 0.0 | 0.0 | -0.000846 | 0.0 | 0.0 | 0.0 | 0.0 | -0.028767 | -0.033899 | -0.002256 | 0.0 | -0.017486 | . 5 5 | 2020-01-05 | 0.0 | -0.017260 | -0.020847 | -0.001354 | 0.0 | -0.008393 | 0.0 | 0.0 | -0.001015 | 0.0 | 0.0 | 0.0 | 0.0 | -0.034520 | -0.040679 | -0.002707 | 0.0 | -0.020983 | . data = pd.concat([pd.melt(globl.filter(regex=&#39;_MED|TIME_POINT|DATE&#39;), id_vars=[&#39;TIME_POINT&#39;, &#39;DATE&#39;], var_name=&#39;MED_KEY&#39;, value_name=&#39;MED_VALUES&#39;), pd.melt(globl.filter(regex=&#39;_HIGH|TIME_POINT|DATE&#39;), id_vars=[&#39;TIME_POINT&#39;, &#39;DATE&#39;], var_name=&#39;HIGH_KEY&#39;, value_name=&#39;HIGH_VALUES&#39;), pd.melt(globl.filter(regex=&#39;_LOW|TIME_POINT|DATE&#39;), id_vars=[&#39;TIME_POINT&#39;, &#39;DATE&#39;], var_name=&#39;LOW_KEY&#39;, value_name=&#39;LOW_VALUES&#39;)], axis=1).T.drop_duplicates().T . data . TIME_POINT DATE MED_KEY MED_VALUES HIGH_KEY HIGH_VALUES LOW_KEY LOW_VALUES . 0 1 | 2020-01-01 | PWR_CO2_MED | 0 | PWR_CO2_HIGH | 0 | PWR_CO2_LOW | 0 | . 1 2 | 2020-01-02 | PWR_CO2_MED | 0 | PWR_CO2_HIGH | 0 | PWR_CO2_LOW | 0 | . 2 3 | 2020-01-03 | PWR_CO2_MED | 0 | PWR_CO2_HIGH | 0 | PWR_CO2_LOW | 0 | . 3 4 | 2020-01-04 | PWR_CO2_MED | 0 | PWR_CO2_HIGH | 0 | PWR_CO2_LOW | 0 | . 4 5 | 2020-01-05 | PWR_CO2_MED | 0 | PWR_CO2_HIGH | 0 | PWR_CO2_LOW | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 973 159 | 2020-06-07 | AVI_CO2_MED | -0.777257 | AVI_CO2_HIGH | -1.34438 | AVI_CO2_LOW | -0.356262 | . 974 160 | 2020-06-08 | AVI_CO2_MED | -0.759206 | AVI_CO2_HIGH | -1.32958 | AVI_CO2_LOW | -0.33817 | . 975 161 | 2020-06-09 | AVI_CO2_MED | -0.721528 | AVI_CO2_HIGH | -1.28806 | AVI_CO2_LOW | -0.306776 | . 976 162 | 2020-06-10 | AVI_CO2_MED | -0.68384 | AVI_CO2_HIGH | -1.24645 | AVI_CO2_LOW | -0.275438 | . 977 163 | 2020-06-11 | AVI_CO2_MED | -0.640498 | AVI_CO2_HIGH | -1.19777 | AVI_CO2_LOW | -0.239945 | . 978 rows × 8 columns . a = alt.Chart().mark_area(opacity=0.5).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=&quot;%b&quot;)), y2=&#39;HIGH_VALUES:Q&#39;, y=&#39;LOW_VALUES:Q&#39; ) l = alt.Chart().mark_line().encode( x=&#39;DATE:T&#39;, y=&#39;MED_VALUES:Q&#39; ) alt.layer(a, l, data=data).facet(alt.Column(&#39;LOW_KEY&#39;, title=&quot;Change in global CO u2082 emissions by sector&quot;, sort=[&#39;TRS_CO2_LOW&#39;, &#39;IND_CO2_LOW&#39;, &#39;PWR_CO2_LOW&#39;, &#39;AVI_CO2_LOW&#39;, &#39;PUB_CO2_LOW&#39;, &#39;RES_CO2_LOW&#39;]), columns=3).configure_axis(grid=False, title=None).configure_axisX(orient=&#39;top&#39;, offset=-27).configure_view(strokeWidth=0).resolve_scale(x=&#39;independent&#39;).configure_header( titleFontSize=20, labelFontSize=14 )#.properties(title=None) #(a+l).facet(&#39;LOW_KEY:N&#39;, sort=alt.SortArray([&#39;TRS_CO2_LOW&#39;, &#39;IND_CO2_LOW&#39;, &#39;PWR_CO2_LOW&#39;, &#39;AVI_CO2_LOW&#39;, &#39;PUB_CO2_LOW&#39;, &#39;RES_CO2_LOW&#39;])) .",
            "url": "https://armsp.github.io/covidviz/climate/2020/06/26/CO2-Emissions.html",
            "relUrl": "/climate/2020/06/26/CO2-Emissions.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "World Geospatial chloropleth plot of cases",
            "content": "Today we will make a chloropleth of the countries in a world map like in the article Coronavirus Map: Tracking the Global Outbreak that looks like this - . For this we will use the JHU CSSE Dataset . #hide_output import pandas as pd import geopandas as gpd import altair as alt import numpy as np alt.renderers.set_embed_options(actions=False) . I made the following geojson file from the US State Department Global LSIB Polygons Detailes after simplifying it as it has too much details and is very large. Following is the code to do that. . Warning: Do NOT RUN THE FOLLOWING CELL. USe the geojson file I have provided - run the cell following the following cell. . #collapse us_st_world = gpd.read_file(&#39;shapes/Global_LSIB_Polygons_Detailed/Global_LSIB_Polygons_Detailed.dbf&#39;) us_st_world.drop([&#39;OBJECTID&#39;, &#39;Shape_Leng&#39;, &#39;Shape_Le_1&#39;, &#39;Shape_Area&#39;], axis=1, inplace=True) us_st_world[&quot;geometry&quot;] = us_st_world.geometry.simplify(tolerance=0.05) us_st_world.to_file(&quot;world.geojson&quot;, driver=&#39;GeoJSON&#39;) #alt.Chart(us_st_world_).mark_geoshape(strokeWidth=1, stroke=&#39;white&#39;).encode().properties(width=1000, height=500).project(&#39;equalEarth&#39;) . . world_geojson = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/world.geojson&#39; us_st_world = gpd.read_file(world_geojson) . uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; time_s_raw = pd.read_csv(uri) time_s = time_s_raw.groupby(&#39;Country/Region&#39;).agg(dict(zip(time_s_raw.columns[4:], [&#39;sum&#39;]*(len(time_s_raw.columns)-4)))) time_s = time_s.reset_index() #time_s . Let&#39;s first find out what countries in our dataset are not present in the shapefile . time_s[time_s[&#39;Country/Region&#39;].isin(us_st_world[&#39;COUNTRY_NA&#39;]) == False] . Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 ... 6/10/20 6/11/20 6/12/20 6/13/20 6/14/20 6/15/20 6/16/20 6/17/20 6/18/20 6/19/20 . 5 Antigua and Barbuda | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | . 11 Bahamas | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 103 | 103 | 103 | 103 | 103 | 103 | 104 | 104 | 104 | 104 | . 21 Bosnia and Herzegovina | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2775 | 2832 | 2893 | 2893 | 2893 | 3040 | 3085 | 3141 | 3174 | 3273 | . 33 Central African Republic | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1888 | 1952 | 2044 | 2057 | 2057 | 2222 | 2410 | 2564 | 2605 | 2605 | . 39 Congo (Brazzaville) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 728 | 728 | 728 | 728 | 728 | 883 | 883 | 883 | 883 | 883 | . 40 Congo (Kinshasa) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4390 | 4515 | 4637 | 4724 | 4778 | 4837 | 4974 | 5100 | 5283 | 5477 | . 48 Diamond Princess | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | . 58 Eswatini | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 398 | 449 | 472 | 486 | 490 | 506 | 520 | 563 | 586 | 623 | . 64 Gambia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 28 | 28 | 30 | 34 | 34 | 36 | 36 | . 75 Holy See | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | . 104 MS Zaandam | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | . 127 North Macedonia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3364 | 3538 | 3701 | 3895 | 4057 | 4157 | 4299 | 4482 | 4664 | 4820 | . 142 Saint Kitts and Nevis | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 15 | 15 | 15 | 15 | 15 | 15 | 15 | 15 | 15 | 15 | . 143 Saint Lucia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 19 | 19 | 19 | 19 | 19 | 19 | 19 | 19 | 19 | 19 | . 144 Saint Vincent and the Grenadines | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 27 | 27 | 27 | 27 | 27 | 27 | 29 | 29 | 29 | 29 | . 146 Sao Tome and Principe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 611 | 632 | 639 | 659 | 661 | 662 | 671 | 683 | 688 | 693 | . 165 Taiwan* | 1 | 1 | 3 | 3 | 4 | 5 | 8 | 8 | 9 | ... | 443 | 443 | 443 | 443 | 443 | 445 | 445 | 445 | 446 | 446 | . 171 Trinidad and Tobago | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 117 | 117 | 117 | 117 | 123 | 123 | 123 | 123 | 123 | 123 | . 174 US | 1 | 1 | 2 | 2 | 5 | 5 | 5 | 5 | 5 | ... | 2000702 | 2023590 | 2048986 | 2074526 | 2094058 | 2114026 | 2137731 | 2163290 | 2191052 | 2220961 | . 183 West Bank and Gaza | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 485 | 487 | 489 | 489 | 492 | 505 | 514 | 555 | 600 | 675 | . 184 Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | . 21 rows × 151 columns . Now we need to understand that the monikers of the countries can change and that we need to figure out how to unify them and then merge them. For that let&#39;s study each of the missing countries one by one like so - . #hide_output us_st_world[us_st_world[&#39;COUNTRY_NA&#39;].str.startswith(&#39;Antigua&#39;)] . Do the same technique for all the contries and you&#39;d end up with the following modifications - . time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Taiwan*&#39;, &#39;Country/Region&#39;] = &#39;Taiwan&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;US&#39;, &#39;Country/Region&#39;] = &#39;United States&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Czech Republic&#39;, &#39;Country/Region&#39;] = &#39;Czechia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;West Bank and Gaza&#39;, &#39;Country/Region&#39;] = &#39;West Bank (disp)&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Western Sahara&#39;, &#39;Country/Region&#39;] = &#39;Western Sahara (disp)&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Trinidad and Tobago&#39;, &#39;Country/Region&#39;] = &#39;Trinidad &amp; Tobago&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Sao Tome and Principe&#39;, &#39;Country/Region&#39;] = &#39;Sao Tome &amp; Principe&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Saint Vincent and the Grenadines&#39;, &#39;Country/Region&#39;] = &#39;St Vincent &amp; the Grenadines&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Saint Lucia&#39;, &#39;Country/Region&#39;] = &#39;St Lucia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Saint Kitts and Nevis&#39;, &#39;Country/Region&#39;] = &#39;St Kitts &amp; Nevis&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;North Macedonia&#39;, &#39;Country/Region&#39;] = &#39;Macedonia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Bahamas&#39;, &#39;Country/Region&#39;] = &#39;Bahamas, The&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Bosnia and Herzegovina&#39;, &#39;Country/Region&#39;] = &#39;Bosnia &amp; Herzegovina&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Central African Republic&#39;, &#39;Country/Region&#39;] = &#39;Central African Rep&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Eswatini&#39;, &#39;Country/Region&#39;] = &#39;Swaziland&#39; #time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;South Korea&#39;, &#39;Country/Region&#39;] = &#39;Korea, South&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country/Region&#39;] = &#39;Congo, Dem Rep of the&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country/Region&#39;] = &#39;Congo, Rep of the&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Antigua and Barbuda&#39;, &#39;Country/Region&#39;] = &#39;Antigua &amp; Barbuda&#39; . TODO . Can we do the above using code instead of manually? . We will ignore the following due to very few cases . # collapse time_s[time_s[&#39;Country/Region&#39;].isin(us_st_world[&#39;COUNTRY_NA&#39;]) == False] . . Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 ... 6/10/20 6/11/20 6/12/20 6/13/20 6/14/20 6/15/20 6/16/20 6/17/20 6/18/20 6/19/20 . 48 Diamond Princess | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | . 64 Gambia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 28 | 28 | 30 | 34 | 34 | 36 | 36 | . 75 Holy See | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | . 104 MS Zaandam | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | . 4 rows × 151 columns . Finding cases per day - . time_s_T = time_s.set_index(&#39;Country/Region&#39;).T time_s_T = time_s_T.apply(lambda x: x.diff(), axis=0) . Averageing the cases over a week - . # hide_output roll_case_avg_list = [] def roll_case_avg(row): #print(row) avgs = row[::-1].rolling(window=7).mean().apply(np.floor).shift(-6) roll_case_avg_list.append((row.name, avgs.iloc[0], avgs.iloc[14])) #print(avgs.iloc[1], avgs.iloc[8]) p = time_s_T.T p.apply(roll_case_avg, axis=1) . #roll_case_avg_list . I asked the NYT GitHub Team on how they are establishing the category colors and based on their input we will use the following classification - . The thresholds for that change are: . Blue: &lt; -15% | Yellow: &gt; -15% and &lt; +15% | Light orange: &gt;+15% and &lt;+100% | Mid orange: &gt;+100% and &lt;+200% | Dark red: &gt;+200% | . Let&#39;s define a function to do that for us - . def categorize(x): if x[&#39;now&#39;] == 0 or x[&#39;ago&#39;] == 0:#x[&#39;ago&#39;] &lt;= 5 or return &#39;Few or no cases&#39; delta = x[&#39;diff&#39;]/x[&#39;ago&#39;]*100 if delta &lt; -15: return &#39;Declining&#39; elif delta &gt; -15 and delta &lt; 15: return &#39;About the same&#39; elif delta &gt; 15 and delta &lt; 100: return &#39;Growth upto 2x&#39; elif delta &gt; 100 and delta &lt; 200: return &#39;Growth upto 3x&#39; elif delta &gt; 200: return &#39;Growth more than 3x&#39; . test2 = pd.DataFrame(roll_case_avg_list, columns=[&#39;country&#39;,&#39;now&#39;,&#39;ago&#39;]) test2[&#39;diff&#39;] = test2[&#39;now&#39;] - test2[&#39;ago&#39;] test2[&#39;category&#39;] = test2.apply(categorize, axis=1) test2.groupby(&#39;category&#39;).count() . country now ago diff . category . About the same 27 | 27 | 27 | 27 | . Declining 35 | 35 | 35 | 35 | . Few or no cases 48 | 48 | 48 | 48 | . Growth more than 3x 11 | 11 | 11 | 11 | . Growth upto 2x 51 | 51 | 51 | 51 | . Growth upto 3x 14 | 14 | 14 | 14 | . test2.columns = [&#39;COUNTRY_NA&#39;, &#39;now&#39;, &#39;ago&#39;, &#39;diff&#39;, &#39;category&#39;] plot2 = us_st_world.merge(test2, how=&#39;left&#39;, on=&#39;COUNTRY_NA&#39;) . plot2 . COUNTRY_NA geometry now ago diff category . 0 Abyei (disp) | POLYGON ((29.00000 9.67356, 28.78724 9.49406, ... | NaN | NaN | NaN | NaN | . 1 Afghanistan | POLYGON ((70.98955 38.49070, 71.37353 38.25597... | 618.0 | 758.0 | -140.0 | Declining | . 2 Akrotiri (UK) | POLYGON ((32.83539 34.70576, 32.98961 34.67999... | NaN | NaN | NaN | NaN | . 3 Aksai Chin (disp) | MULTIPOLYGON (((78.69853 34.09310, 78.69837 34... | NaN | NaN | NaN | NaN | . 4 Albania | POLYGON ((19.72764 42.66045, 19.79268 42.48135... | 60.0 | 16.0 | 44.0 | Growth more than 3x | . ... ... | ... | ... | ... | ... | ... | . 274 Burma | MULTIPOLYGON (((98.03206 9.83411, 98.06033 9.8... | 3.0 | 4.0 | -1.0 | Declining | . 275 India | MULTIPOLYGON (((93.84583 7.24456, 93.96289 7.0... | 12293.0 | 8956.0 | 3337.0 | Growth upto 2x | . 276 Benin | POLYGON ((2.84088 12.40599, 3.26927 12.01606, ... | 37.0 | 5.0 | 32.0 | Growth more than 3x | . 277 Niger | POLYGON ((12.02686 23.50849, 13.52600 23.15616... | 6.0 | 1.0 | 5.0 | Growth more than 3x | . 278 Nigeria | MULTIPOLYGON (((6.13707 4.37177, 6.08799 4.359... | 566.0 | 363.0 | 203.0 | Growth upto 2x | . 279 rows × 6 columns . Now we are ready to plot the chloropleth - . # collapse base=alt.Chart(plot2).mark_geoshape(stroke=&#39;white&#39;).transform_filter((alt.datum.COUNTRY_NA != &#39;Antarctica&#39;)).encode( color = alt.Color(&#39;category:N&#39;, scale=alt.Scale( domain=[&#39;Few or no cases&#39;, &#39;Declining&#39;, &#39;About the same&#39;, &#39;Growth upto 2x&#39;, &#39;Growth upto 3x&#39;, &#39;Growth more than 3x&#39;], range=[&#39;#f2f2f2&#39;, &#39;#badee8&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;] ), legend=alt.Legend(title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200) ), tooltip = [&#39;COUNTRY_NA&#39;, alt.Tooltip(&#39;now:Q&#39;, format=&#39;.0d&#39;), alt.Tooltip(&#39;ago:Q&#39;, format=&#39;.0d&#39;), &#39;category&#39;] ).properties(height=800, width=1500).project(&#39;equalEarth&#39;).configure_view(strokeWidth=0) . . base . We can do something even more interesting...we can make the chart interactive by highlighting the countries based on their category - . Falling | Almost the same | Rising 1 | Rising 2 | Rising 3. | . #collapse selector = alt.selection_single( fields=[&#39;category&#39;], empty=&#39;all&#39;, bind=&#39;legend&#39; ) interactive = base.encode( color = alt.Color( &#39;category:N&#39;, legend=alt.Legend(values=[&#39;Declining&#39;, &#39;About the same&#39;, &#39;Growth upto 2x&#39;, &#39;Growth upto 3x&#39;, &#39;Growth more than 3x&#39;], title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200), scale=alt.Scale( domain=[&#39;Few or no cases&#39;, &#39;Declining&#39;, &#39;About the same&#39;, &#39;Growth upto 2x&#39;, &#39;Growth upto 3x&#39;, &#39;Growth more than 3x&#39;], range=[&#39;#f2f2f2&#39;, &#39;#badee8&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;] ) ), opacity=alt.condition(selector, alt.value(1), alt.value(0.25)) ).add_selection( selector ) . . Now click on the legend to highlight the countries for that category. . interactive .",
            "url": "https://armsp.github.io/covidviz/geospatial/2020/06/15/World-Case-per-day-chloropleth.html",
            "relUrl": "/geospatial/2020/06/15/World-Case-per-day-chloropleth.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Geospatial plot of cases in US",
            "content": "Today we will make our first geospatial map from the article Coronavirus in the U.S.: Latest Map and Case Count which looks like the folowing - . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in state data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;]).sum() . fips cases deaths . county . Joplin 0.0 | 329 | 4 | . Kansas City 0.0 | 85094 | 1700 | . New York City 0.0 | 15615980 | 1528538 | . Unknown 0.0 | 885104 | 41064 | . #hide_output cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;, &#39;state&#39;]).sum() . NYT publishes the data for New York City in a different way by combining the results of the 5 boroughs that comprise it. So we will combine them too and add a new row in the dataset with a custom fips of 1. Let&#39;s start by making this change in the raw NYT dataset itself. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;] . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | . 262876 2020-06-23 | New York City | New York | 1.0 | 217803 | 21817 | . 265930 2020-06-24 | New York City | New York | 1.0 | 218089 | 21838 | . 268988 2020-06-25 | New York City | New York | 1.0 | 218429 | 21856 | . 272054 2020-06-26 | New York City | New York | 1.0 | 218799 | 21893 | . 275123 2020-06-27 | New York City | New York | 1.0 | 219157 | 21913 | . 119 rows × 6 columns . # collapse latest_cases = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) latest_cases . . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . 1 1001.0 | Autauga | 2020-06-27 | Alabama | 498 | 12 | . 2 1003.0 | Baldwin | 2020-06-27 | Alabama | 555 | 10 | . 3 1005.0 | Barbour | 2020-06-27 | Alabama | 317 | 1 | . 4 1007.0 | Bibb | 2020-06-27 | Alabama | 161 | 1 | . ... ... | ... | ... | ... | ... | ... | . 3038 56037.0 | Sweetwater | 2020-06-27 | Wyoming | 81 | 0 | . 3039 56039.0 | Teton | 2020-06-27 | Wyoming | 119 | 1 | . 3040 56041.0 | Uinta | 2020-06-27 | Wyoming | 167 | 0 | . 3041 56043.0 | Washakie | 2020-06-27 | Wyoming | 38 | 5 | . 3042 56045.0 | Weston | 2020-06-27 | Wyoming | 1 | 0 | . 3043 rows × 6 columns . Now we have to make the changes in our shapefile too. For that we need to **dissolve** the 5 buroughs into one single geospatial entity. . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . agg_nyc_data . STATEFP geometry COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER lon lat fips . 0 36 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | -73.927011 | 40.695278 | 1 | . # hide_output county_nyc = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county_nyc[&#39;fips&#39;] = county_nyc[&#39;GEOID&#39;] county_nyc[&#39;fips&#39;] = county_nyc[&#39;fips&#39;].astype(&#39;int&#39;) county_nyc # generate FIPS in the shapefile itself by combining STATEFP and COUNTYFP #county2[&#39;STATEFP&#39;] + county2[&#39;COUNTYFP&#39;] #latest_cases[&#39;fips&#39;] = latest_cases[&#39;fips&#39;].astype(&#39;int&#39;) . latest_cases[&#39;fips&#39;].isin(county_nyc[&#39;fips&#39;]).value_counts() . True 3043 Name: fips, dtype: int64 . latest_cases[latest_cases[&#39;county&#39;] == &#39;New York City&#39;] . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . county_nyc[county_nyc[&#39;fips&#39;] == 1] . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 3220 36 | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | -73.927011 | 40.695278 | 1 | . # collapse latest_cases_w_fips = county_nyc.merge(latest_cases, how=&#39;left&#39;, on=&#39;fips&#39;) circle_selection = alt.selection_single(on=&#39;mouseover&#39;, empty=&#39;none&#39;) circles = alt.Chart(latest_cases_w_fips).mark_point(fillOpacity=0.2, fill=&#39;red&#39;, strokeOpacity=1, color=&#39;red&#39;, strokeWidth=1).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, size=alt.Size(&#39;cases:Q&#39;, scale=alt.Scale(domain=[0, 7000],),legend=alt.Legend(title=&quot;Cases&quot;)), tooltip=[&#39;county:N&#39;, &#39;cases:Q&#39;, &#39;deaths:Q&#39;], color = alt.condition(circle_selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1000, height=700 ).add_selection( circle_selection ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) state_text = state.mark_text().transform_filter(alt.datum.NAME != &#39;Puerto Rico&#39;).encode( longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;, text=&#39;NAME&#39;, ).project( type=&#39;albersUsa&#39; ) . . (state+circles+state_text).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/2020/06/12/US-case-counts-geospatial.html",
            "relUrl": "/geospatial/2020/06/12/US-case-counts-geospatial.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Deaths above or below normal",
            "content": "We will make charts from the NYT article on What Is the Real Coronavirus Death Toll in Each State? . The charts look like the following - . Whats the purpose of this visualization? . Comparing recent totals of deaths from all causes can provide a more complete picture of the pandemic’s impact than tracking only deaths of people with confirmed diagnoses. Epidemiologists refer to fatalities in the gap between the observed and normal numbers of deaths as “excess deaths.” . Indeed, in nearly every state with an unusual number of deaths in recent weeks, that number is higher than the state’s reported number of deaths from Covid-19. On our charts, we have marked the number of official coronavirus deaths with red lines, so you can see how they match up with the total number of excess deaths. . Measuring excess deaths is crude because it does not capture all the details of how people died. But many epidemiologists believe it is the best way to measure the impact of the virus in real time. It shows how the virus is altering normal patterns of mortality where it strikes and undermines arguments that it is merely killing vulnerable people who would have died anyway. . Public health researchers use such methods to measure the impact of catastrophic events when official measures of mortality are flawed. . Measuring excess deaths does not tell us precisely how each person died. It is likely that most of the excess deaths in this period are because of the coronavirus itself, given the dangerousness of the virus and the well-documented problems with testing. But it is also possible that deaths from other causes have risen too, as hospitals have become stressed and people have been scared to seek care for ailments that are typically survivable. Some causes of death may be declining, as people stay inside more, drive less and limit their contact with others. . We will use 2 datasets to generate our chart - . The excess deaths dataset from NYT | The COVID-19 deaths dataset also from NYT | Luckily both are in the same GitHub repository - NYT Covid-19 Data However we need to do some significant preprocessing to arrive at the results. It took me a good amount of time to figure out the whole graph and once I had done it it just made so much sense :relieved: . The way these graphs are made is as follows - . First we chart the excess deaths. Excess deaths is calculated as the difference b/w all cause mortality data with expected deaths. These data are available from CDC. | NYT publishes the excess deaths data for NYC, so for starters we will use that before moving on to the other states. | Then we need to get the covid-19 related deaths from NYC which we can get from the NYT dataset or JHU CSSE dataset. | The challenge is actually combining the above. | How do we combine the above data? . The excess deaths data is weekly. With the starting and ending dates given (7 days duration), so what we are gonna do is we have to transform the COVID-19 deaths data in the same weekly format. . import pandas as pd import numpy as np import altair as alt excess_uri = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/excess-deaths/deaths.csv&#39; county_uri = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; ex_df = pd.read_csv(excess_uri) co_df = pd.read_csv(county_uri) . Extracting data for NYC for year 2020 - . # collapse_show ex_nyc = ex_df[(ex_df[&#39;placename&#39;] == &#39;New York City&#39;)] ex_nyc[&#39;year&#39;] = ex_nyc[&#39;year&#39;].astype(int) ex_nyc = ex_nyc[ex_nyc[&#39;year&#39;] == 2020] . . Testing the chart - . # collapse_show excess_deaths_chart = alt.Chart(ex_nyc).mark_bar(width=5).encode( x=&#39;week:N&#39;, y=&#39;excess_deaths:Q&#39;, color = alt.condition(alt.datum.excess_deaths&gt;0, alt.value(&#39;orange&#39;), alt.value(&#39;steelblue&#39;)) ).properties(height=500, width=alt.Step(10)) . . excess_deaths_chart . This was the easy part and we can see that we are following the trends properly by comparing it with the NYT chart - . There are some differences, which I think is due to the fact that the dataset provided by NYT is actually slightly different than the data they have used to plot. Also there is actually more data used in the article than there is in the GitHub repo. So to make the charts look completely similar we will have to dig in the CDC dataset - which we will ignore for now. . Now we will transform the COVID-19 deaths data to weekly deaths data . ny_co_df = co_df[co_df[&#39;county&#39;] == &#39;New York City&#39;] ny_co_df = ny_co_df[(ny_co_df[&#39;date&#39;] &gt; &#39;2020-03-06&#39;) &amp; (ny_co_df[&#39;date&#39;] &lt; &#39;2020-05-17&#39;)] ny_co_df[&#39;death_per_day&#39;] = ny_co_df[&#39;deaths&#39;].diff() ny_co_df = ny_co_df[1:] weekly_cov_death = ny_co_df.groupby(np.arange(len(ny_co_df))//7).sum() weekly_cov_death[&#39;week&#39;] = range(11,21) . Let&#39;s plot it to see if we get the trends right - . # collapse covid_tick_deaths = alt.Chart(weekly_cov_death).mark_tick(thickness=2, color=&#39;red&#39;).encode( x=&#39;week:N&#39;, y=&#39;death_per_day:Q&#39; ) . . excess_deaths_chart + covid_tick_deaths . This looks correct. Let&#39;s beautify a little - . # collapse excess_deaths_chart = alt.Chart(ex_nyc).mark_bar(width=9).encode( x=&#39;week:N&#39;, y=&#39;excess_deaths:Q&#39;, color = alt.condition(alt.datum.excess_deaths&gt;0, alt.value(&#39;#ffab00&#39;), alt.value(&#39;#8FB8BB&#39;)) ).properties(height=500, width=alt.Step(10)) . . (excess_deaths_chart + covid_tick_deaths).configure_view(strokeWidth=0).configure_axis(grid=False) . Lets add a rectangle from March to May to complete our chart . # collapse source = alt.pd.DataFrame([{&#39;start&#39;: 11, &#39;end&#39;: 20}]) rect = alt.Chart(source).mark_rect(opacity=1, fill=&#39;#eee&#39;, xOffset=5, x2Offset=5).encode( x=&#39;start:N&#39;, x2=&#39;end:N&#39; ) . . (rect + excess_deaths_chart + covid_tick_deaths).configure_view(strokeWidth=0).configure_axis(grid=False) . We can see that it captures the trend extremely well and the datapoints are almost exactly the same. . TODO . The rest of the states using the CDC Data .",
            "url": "https://armsp.github.io/covidviz/2020/06/04/Above-Below-Normal.html",
            "relUrl": "/2020/06/04/Above-Below-Normal.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Tracking the Global Outbreak: Growth Rates",
            "content": "Today we will make the growth rate charts from the NYT article on Tracking the Global Outbreak for all the countries. . . We will use the JHU CSSE dataset since NYT does not provide its own global countries dataset . #hide_output import pandas as pd import altair as alt raw_data_url = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; raw_df = pd.read_csv(raw_data_url) alt.renderers.set_embed_options(actions=False) . A few important observations - . There are some countries that have data at a finer level like state/county. For those countires we will extract the extract the corresponding rows, sum them up into a single row and transpose it to convert to a dataframe. | Those exceptional countries are &#39;Australia&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Denmark&#39;, &#39;France&#39;, &#39;Netherlands&#39;, &#39;United Kingdom&#39; | . Let&#39;s convert the data into the desired long form from the existing wide form. . # collapse long_df = pd.DataFrame() exceptional_countries = [&#39;Australia&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Denmark&#39;, &#39;France&#39;, &#39;Netherlands&#39;, &#39;United Kingdom&#39;] country_df_list = [] def extract_country(s): if s[1].strip() not in exceptional_countries: # print(f&quot;{s[1]} - exceptional case&quot;) # else: temp_df = pd.DataFrame(s[4:]) temp_df.columns = [&#39;value&#39;] temp_df[&#39;country&#39;] = s[1] temp_df[&#39;growth&#39;] = temp_df[&#39;value&#39;].diff() temp_df[&#39;growth&#39;][0] = temp_df[&#39;value&#39;].iloc[0] temp_df[&#39;growth&#39;] = temp_df[&#39;growth&#39;].astype(int) temp_df = temp_df.rename_axis(&#39;date&#39;).reset_index() country_df_list.append(temp_df) for country in exceptional_countries: temp_df = pd.DataFrame(raw_df[raw_df[&#39;Country/Region&#39;] == country].iloc[:,4:].sum(axis=0).rename_axis(&#39;date&#39;).reset_index()) temp_df.columns = [&#39;date&#39;,&#39;value&#39;] temp_df[&#39;country&#39;] = country temp_df[&#39;growth&#39;] = temp_df[&#39;value&#39;].diff() temp_df.loc[0, &#39;growth&#39;] = temp_df[&#39;value&#39;].iloc[0] temp_df[&#39;growth&#39;] = temp_df[&#39;growth&#39;].astype(int) country_df_list.append(temp_df) raw_df.apply(extract_country, axis=1) long_df = pd.concat(country_df_list) . . Beacause this is a large dataset, Altair will - by default - refuse to display because of possible memory issues. So we will have to enable the json transformer so that the data is passed insternally as a url. We enable it using alt.data_transformers.enable(&#39;json&#39;). Do this if you are running it locally. To do the same on Fastpages, I have already uploaded the json file that is generated by Altair behind the scenes and and I will pass the url of the file to the chart so that the output visualization is seen on the website. . # collapse #alt.data_transformers.enable(&#39;json&#39;) # use this if running locally #alt.data_transformers.disable_max_rows() # avoid this as it can hang your system url = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/2020-06-02-Data.json&#39; #comment this when running locally otherwise you will have old data till 1st June only a = alt.Chart().mark_bar(size=2, opacity=0.2, color=&#39;gray&#39;).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None), y=alt.Y(&quot;growth:Q&quot;, title=None), ).properties(width=90, height=100) b = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.4).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&quot;rolling_mean:Q&quot;,title=&#39;cases&#39;) ) c = b.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 2}) alt.layer(a, c, data=url).facet(alt.Column(&#39;country:N&#39;, title=None, sort=alt.EncodingSortField(&#39;value&#39;, op=&#39;max&#39;, order=&#39;descending&#39;), header=alt.Header(labelFontSize=13, labelColor=&#39;gray&#39;, labelFontWeight=&#39;bolder&#39;, labelAlign=&#39;center&#39;, labelAnchor=&#39;middle&#39;, labelOrient=&#39;top&#39;, labelPadding=-15, labelAngle=0)), spacing=alt.RowColnumber(row=70, column=0), title=&quot;Countrywise Distribution of Growth, Averaged over 7 days&quot;, columns=7, ).configure_axis( grid=False, #domainWidth=0.1 ).configure_view(strokeWidth=0).configure_title( fontSize=25, font=&#39;Courier&#39;, anchor=&#39;middle&#39;, color=&#39;gray&#39;, dy=-30 ) . . There are a few issues with the above chart - . We are seeing negative values in growth rate, How can it be negative? The lowest it can go is 0. | The graphs for countries with very few cases don&#39;t look good. | The scales of the countries vary a lot. We need to adjust the scale like NYT does, to make it readable. | . Let&#39;s improve upon the issues above with the following solutions - . NYT does not show graphs for those with fewer than 100 cases. Like NYT we have filtered countries with less than 100 cases | The growth rates will be negative if there are discrepancies in the data - when the cumulative cases drop for any reason than the previous day. You will certainly notice thos in the dataset for some of the countries. | We will filter the negative values as based on my observation that&#39;s what NYT seems to be doing | Independednt Y axis | . Few points to keep in mind - . The bar chart shows the increment in cases per day | The line chart is the 7 day average of growth in cases per day | The facet is ordered in descending order by the maximum number of cases | We will forcefully align the countries one below the other because choosing independent axes often leads to misaligned facet items | . # collapse a = alt.Chart().mark_bar(size=2, opacity=0.05, color=&#39;red&#39;).transform_filter( alt.datum.growth &gt;= 0).transform_filter(alt.datum.value &gt; 100).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None), y=alt.Y(&quot;growth:Q&quot;, title=None), ).properties(width=90, height=100) b = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.04).transform_filter( alt.datum.growth &gt;= 0).transform_filter(alt.datum.value &gt; 100).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&quot;rolling_mean:Q&quot;,title=&#39;cases&#39;) ) c = b.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 2}) alt.layer(a, b, c, data=url).facet(alt.Column(&#39;country:N&#39;, title=None, sort=alt.EncodingSortField(&#39;value&#39;, op=&#39;max&#39;, order=&#39;descending&#39;), header=alt.Header(labelFontSize=13, labelColor=&#39;gray&#39;, labelFontWeight=&#39;bolder&#39;, labelAlign=&#39;center&#39;, labelAnchor=&#39;middle&#39;, labelOrient=&#39;top&#39;, labelPadding=-15, labelAngle=0)), spacing=alt.RowColnumber(row=70, column=5), title=&quot;Countrywise Distribution of Growth, Averaged over 7 days&quot;, columns=7, align=&#39;each&#39;, ).resolve_scale(y=&#39;independent&#39;, x=&#39;independent&#39;,).configure_axis( grid=False, #domainWidth=0.1 ).configure_view(strokeWidth=0).configure_title( fontSize=25, font=&#39;Courier&#39;, anchor=&#39;middle&#39;, color=&#39;gray&#39;, dy=-30 ) . . Feel free to comment below if you didn&#39;t understand anything and I will try my best to answer .",
            "url": "https://armsp.github.io/covidviz/2020/06/02/World-Map-growth.html",
            "relUrl": "/2020/06/02/World-Map-growth.html",
            "date": " • Jun 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Missing Deaths",
            "content": "Today we will make the following graph that shows the excess deaths as appeared in the article 87,000 Missing Deaths: Tracking the True Toll of the Coronavirus Outbreak . . Fortunately the NYT provides the dataset for this in their repository. . What does Excess Death mean and how do we calculate it? . Excess deaths are estimates that include deaths from Covid-19 and other causes. Reported Covid-19 deaths reflect official coronavirus deaths during the period when all-cause mortality data is available, including figures that were later revised. . According to the github repository - . Official Covid-19 death tolls offer a limited view of the impact of the outbreak because they often exclude people who have not been tested and those who died at home. All-cause mortality is widely used by demographers and other researchers to understand the full impact of deadly events, including epidemics, wars and natural disasters. The totals in this data include deaths from Covid-19 as well as those from other causes, likely including people who could not be treated or did not seek treatment for other conditions. . . Expected Deaths . We have calculated an average number of expected deaths for each area based on historical data for the same time of year. These expected deaths are the basis for our excess death calculations, which estimate how many more people have died this year than in an average year. . The number of years used in the historical averages changes depending on what data is available, whether it is reliable and underlying demographic changes. The baselines do not adjust for changes in age or other demographics, and they do not account for changes in total population. . The number of expected deaths are not adjusted for how non-Covid-19 deaths may change during the outbreak, which will take some time to figure out. As countries impose control measures, deaths from causes like road accidents and homicides may decline. And people who die from Covid-19 cannot die later from other causes, which may reduce other causes of death. Both of these factors, if they play a role, would lead these baselines to understate, rather than overstate, the number of excess deaths. . That is what we are going to do, average the results based on the baseline field to show the blue line for expected deaths. However it also looks like they are using some sort of linear model and smoothing as mentioned in the accompanying news article - . To estimate expected deaths, we fit a linear model to reported deaths in each country from 2015 to January 2020. The model has two components — a linear time trend to account for demographic changes and a smoothing spline to account for seasonal variation. For countries limited to monthly data, the model includes month as a fixed effect rather than using a smoothing spline. . Since there isn&#39;t much information on that we will ignore it for the time being. . What&#39;s the insight that this data gives? . These numbers undermine the notion that many people who have died from the virus may soon have died anyway. In Britain, which has recorded more Covid-19 deaths than any country except the United States, 59,000 more people than usual have died since mid-March — and about 14,000 more than have been captured by official death statistics. . import pandas as pd import altair as alt url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/excess-deaths/deaths.csv&quot; raw = pd.read_csv(url) . Lets study Sweden, Switzerland, UK and France for our charts . sweden = raw[raw[&#39;country&#39;] == &quot;Sweden&quot;] switzerland = raw[raw[&#39;country&#39;] == &quot;Switzerland&quot;] uk = raw[raw[&#39;country&#39;] == &quot;United Kingdom&quot;] france = raw[raw[&#39;country&#39;] == &quot;France&quot;] . Let&#39;s start with a simple layered chart - area for year 2019 and line for 2020. We will not average anything right now nor will we use all the fields in our dataset. . base = alt.Chart(sweden).encode( x=alt.X(&#39;week&#39;) ) alt.layer( base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=500) . For Sweden they plot the gray lines for years 2015 to 2019. The blue line is the weekly average per year and the maroon line is the deaths in 2020. . # collapse base = alt.Chart(sweden).encode( x=&#39;week&#39;, ).properties(height=200) lines = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ).properties(width=500) avg + lines . . Looks like we capture the trend pretty well . Similarly for Switzerland, we will also turn off the grid and the view box - . # collapse base = alt.Chart(switzerland).encode( x=&#39;week&#39;, ).properties(height=300, width=500) lines = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) (avg+lines).configure_view(strokeWidth=0).configure_axis(grid=False) . . Trying the same for U.K - . # collapse base = alt.Chart(uk).encode( x=&#39;week&#39;, ).properties(height=300, width=550) l = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) rule = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) (rule+l).configure_view(strokeWidth=0).configure_axis(grid=False) . . Let&#39;s make use of loops to do the same but for France (based on observation it looks like the gray lines are from 2015 to 2019) - . # collapse base = alt.Chart(france).encode( x=&#39;week&#39;, ).properties(height=300, width=550) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) layer = [] for year in range(2015, 2021): l = base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) if year == 2020: l = base.mark_line(color=&#39;maroon&#39;).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) layer.append(l) alt.layer(avg,*layer).configure_view(strokeWidth=0).configure_axis(grid=False) . . The excess deaths articles and graphs update frequently and the graphics also changes quite a bit - . . In the latest versions of the charts they started using dashed lines, for that we will use strokeDash = alt.value([3,3]) . # collapse base = alt.Chart(france).encode( x=&#39;week&#39;, ).properties(height=300, width=550) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, strokeDash=[1,2], fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, strokeDash = alt.value([3,3]) ) layer = [] for year in range(2015, 2021): l = base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) if year == 2020: l = base.mark_line(color=&#39;maroon&#39;).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) layer.append(l) alt.layer(avg,*layer).configure_view(strokeWidth=0).configure_axis(grid=False) . .",
            "url": "https://armsp.github.io/covidviz/2020/06/01/Excess-Deaths.html",
            "relUrl": "/2020/06/01/Excess-Deaths.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Covid Death Rates Graph",
            "content": "The graph that we will learn to make today is from the NYT article on Comparing Coronavirus Death Rates Across the U.S . . import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv&quot; usdf = pd.read_csv(url) nydf = usdf[(usdf[&#39;state&#39;] == &quot;New York&quot;) &amp; (usdf[&#39;date&#39;] &lt; &#39;2020-04-23&#39;)] nydf[&#39;deaths_perday&#39;] = nydf[&#39;deaths&#39;].diff() chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) chart . To highlight the region after the Stay at Home order date, we will use a Rectangle Chart - mark_react() We will also make a new source data that contains the starting date of the stay at home order and the perhaps the end date of stay at home or just the latest data in the dataset. NYT has not updated this graph and has data only till 21st or 22nd April. . # collapse chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700) . . This is the important piece of code on how to set up the Rectangle chart . source2 = [{ &quot;start&quot;: &quot;2020-03-23&quot;, &quot;end&quot;: nydf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . Similarly you can do the same for any other state. Lets try Michigan for a change - . michdf = usdf[(usdf[&#39;state&#39;] == &quot;Michigan&quot;) &amp; (usdf[&#39;date&#39;] &lt; &#39;2020-04-23&#39;)] michdf[&#39;deaths_perday&#39;] = michdf[&#39;deaths&#39;].diff() . # collapse chart = alt.Chart(michdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=150, width=700) source2 = [{ &quot;start&quot;: &quot;2020-03-25&quot;, &quot;end&quot;: michdf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . . The visualization till the latest date for NY would look like the following - . # collapse nydf = usdf[(usdf[&#39;state&#39;] == &quot;New York&quot;)] nydf[&#39;deaths_perday&#39;] = nydf[&#39;deaths&#39;].diff() chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700) source2 = [{ &quot;start&quot;: &quot;2020-03-23&quot;, &quot;end&quot;: nydf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . . TODO . Facet for all the states .",
            "url": "https://armsp.github.io/covidviz/2020/05/31/Covid-Death-Rates.html",
            "relUrl": "/2020/05/31/Covid-Death-Rates.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Covid Cases & Deaths Graph for U.S",
            "content": "The graph that we will learn to make today is from the NYT article on Coronavirus in the U.S . . import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) alt.renderers.set_embed_options(actions=False) usdf[&#39;new_deaths&#39;] = usdf[&#39;deaths&#39;].diff() usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart().mark_bar(size=5,opacity=0.2,color=&#39;gray&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_deaths:Q&#39;) ) # Area Chart area = alt.Chart().mark_area(fill=&#39;gray&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_deaths)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;black&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 3}) # Chart of deaths deaths = (bar+area+line).properties(width=800, title=&quot;Deaths&quot;) # Bar Chart bar2 = alt.Chart().mark_bar(size=5,opacity=0.2,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ) # Area Chart area2 = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line2 = area2.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 3}) cases = (bar2+area2+line2).properties(width=800, title=&quot;Cases&quot;) # Vertically concatenate the charts alt.vconcat(cases, deaths, data=usdf).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ) . Possible other ways to do this - . Repeat Chart | Facet Chart https://altair-viz.github.io/user_guide/compound_charts.html | .",
            "url": "https://armsp.github.io/covidviz/2020/05/30/Covid-Cases-&-Deaths-in-US.html",
            "relUrl": "/2020/05/30/Covid-Cases-&-Deaths-in-US.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Covid Cases Graph",
            "content": "The graph that we will learn to make today is from the NYT article on Coronavirus Tracking in US which looks as follows - . . We will take the data from NYT&#39;s GitHub repo itself. Lets jump straight into the code - . import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.2,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line = area.mark_line(**{&quot;color&quot;: &#39;#c11111&#39;, &quot;opacity&quot;: 0.9, &quot;strokeWidth&quot;: 5}) chart = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=1000) chart . Now that we have replicated the chart effectively, lets filter the data so that we drop all the data before March as the chart from NYT shows - . That can be done in 2 ways : . using Pandas | using transform_filter on date | . You guessed it, we will use the latter. The code relevant to this would then be - . transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) . #collapse import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) #print(udf.columns) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.15,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.1).transform_window( #stroke=&#39;red&#39;, strokeWidth=2 rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;#c11111&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 5}) k = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=700)#width=alt.Step(500) k . . Now today or maybe a couple of days ago, NYT added interactivity to their charts. Lets do that too. The main concept here is using Altair Selections. For that we will use selection_single() on date field and change the opacity of the bars. So the important code pieces are - . single_bar = alt.selection_single(fields=[&#39;date&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39;) opacity = alt.condition(single_bar, alt.value(0.5), alt.value(0.15)) . #collapse import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) #print(udf.columns) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() single_bar = alt.selection_single(fields=[&#39;date&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39;) # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.15,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;), opacity= alt.condition(single_bar, alt.value(0.5), alt.value(0.15)) ).add_selection(single_bar).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.1).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;#c11111&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 5}) chart = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=700) chart . . Isn&#39;t that amazing . TODO . [ ] Adding tooltip/text to interactive bar chart | .",
            "url": "https://armsp.github.io/covidviz/2020/05/29/Covid-cases.html",
            "relUrl": "/2020/05/29/Covid-cases.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "What is Altair?",
            "content": "To understand Altair we need to know what is it built on. The following hierarchy enables us to understand that - . D3 | Vega | Vega-Lite | Altair | . Lets start with Vega. . What is Vega? . Vega is a visualization grammar, a declarative language for creating, saving, and sharing interactive visualization designs. With Vega, you can describe the visual appearance and interactive behavior of a visualization in a JSON format, and generate web-based views using Canvas or SVG. . . Note: visualization grammar . Where does D3 fit in here? . To be clear, Vega is not intended as a “replacement” for D3. D3 is intentionally a lower-level library. During the early design of D3, we even referred to it as a “visualization kernel” rather than a “toolkit” or “framework”. In addition to custom design, D3 is intended as a supporting layer for higher-level visualization tools. Vega is one such tool, and Vega uses D3 heavily within its implementation. . Vega provides a higher-level visualization specification language on top of D3. By design, D3 will maintain an “expressivity advantage” and in some cases will be better suited for novel design ideas. On the other hand, we intend Vega to be convenient for a wide range of common yet customizable visualizations. . Now, that we know the top hierarchy, the rest is easy to follow - . Vega-Lite . Vega-Lite is a high-level grammar of interactive graphics. It provides a concise JSON syntax for rapidly generating visualizations to support analysis. Vega-Lite specifications can be compiled to Vega specifications. . . Note: high-level visualization grammar . Vega-Lite is used by some big players - . | | | | | | | | | | | | . Altair . In short, Altair exposes a Python API for building statistical visualizations that follows Vega-Lite syntax. . Altair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite. With Altair, you can spend more time understanding your data and its meaning. . Altair’s API is simple, friendly and consistent and built on top of the powerful Vega-Lite visualization grammar. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code. . . I do not intend to teach you how to use Altair or Vega Lite. You can do that on your own using the following excellent resources - . Conference Talk by Jake VanderPlas | . Exploratory Data Visualization with Altair | UW Data Viz Curriculum | .",
            "url": "https://armsp.github.io/covidviz/2020/05/28/Altair-Intro.html",
            "relUrl": "/2020/05/28/Altair-Intro.html",
            "date": " • May 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi :wave: . My name is Shantam. This blog will contain my attempts to replicate/emulate the visualizations at large media houses, related to COVID-19 pandemic, using open source data &amp; tools. . I am a driven python developer and data scientist. I have worked with Machine Learning and Big Data too. Did I tell you I am a published poet too? . I am available for consulting, collaborations, part time or full time jobs. Let me know if you want to talk. . If you want me to try a specific visualization, do let me know by raising an issue in the GitHub Repository. . My website - www.shantamraj.com . I am working on a project that you should definitely check out - COVID-19 Stories .",
          "url": "https://armsp.github.io/covidviz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://armsp.github.io/covidviz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}